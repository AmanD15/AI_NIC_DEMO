{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIr5qOBm6e9I"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "I am starting a series of post in medium covering most of the CNN architectures implemented so far, in pytorch and tensorflow. I believe after getting your hands on with the standard architectures, we will be ready to build our own custom CNN architectures for any task.\n",
        "\n",
        "So I am starting with the oldest CNN architecture LeNet(1998). It was primarily developed for recognition of handwritten and other characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0OFJp296Ebs"
      },
      "source": [
        "<img src=\"https://miro.medium.com/max/700/1*lvvWF48t7cyRWqct13eU0w.jpeg\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHrc6zHo79qw"
      },
      "source": [
        "The above picture summarizes the LeNet's architecture, let's break down each of them layer by layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ldrx3_0V8ZaZ"
      },
      "source": [
        "## LeNet Architecture\n",
        "S.No | Layers | Output Shape (Height, Width, Channels)\n",
        "--- | --- | ---\n",
        "1 | Input Layer | 32 x 32 x 1\n",
        "2 | Conv2d [6 Filters of size = 5x5, stride = 1, padding = 0 ] | 28 x 28 x 6\n",
        "3 | Average Pooling [stride = 2, padding = 0] | 14 x 14 x 6\n",
        "4 | Conv2d [16 Filters of size = 5x5, stride = 1, padding = 0 ] | 10 x 10 x 16\n",
        "5 | Average Pooling [stride = 2, padding = 0] | 5 x 5 x 16\n",
        "6 | Conv2d [120 Filters of size = 5x5, stride = 1, padding = 0 ] | 1 x 1 x 120\n",
        "7 | Linear1 Layer | 120\n",
        "8 | Linear2 Layer | 84\n",
        "9 | Final Linear Layer | 10\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6h1Bup0z3Lu"
      },
      "source": [
        "<img src=\"https://miro.medium.com/max/330/1*D47ER7IArwPv69k3O_1nqQ.png\">\n",
        "\n",
        "## Number of Learning Parameters = [i x (f x f) x b] + b\n",
        "i = Number of input channels in conv2d\n",
        "\n",
        "f = Filter Size\n",
        "\n",
        "b = Number of Bias\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK1Yaofcxg9H"
      },
      "source": [
        "## Output size calculation after applying convolution\n",
        "Stride and Padding are kept constants across the network, so S = 1, P = 0\n",
        "\n",
        "1. Input Layer shape = 32 x 32 x 1\n",
        "2. After applying conv2d with 6 filters of (5x5),\n",
        "  * Output shape = ((32 + 0 - 5) / 1) + 1 = 28\n",
        "  * No of Learning Parameters = ([ 1 x (5 * 5) x 1] + 1) * 6 filters = 156\n",
        "3. After applying Average Pooling (2x2),\n",
        "  * Output shape = ((28 + 0 - 2) / 2) + 1 = 14\n",
        "  * No of Learning Parameters = None (0)\n",
        "4. After applying conv2d with 16 filters of (5x5),\n",
        "  * Output shape = ((14 + 0 - 5) / 1) + 1 = 10\n",
        "  * No of Learning Parameters = ([ 6 x (5 * 5) x 1] + 1) * 16 filters = 2416\n",
        "5. After applying Average Pooling (2x2),\n",
        "  * Output shape = ((10 + 0 - 2) / 2) + 1 = 5\n",
        "  * No of Learning Parameters = None (0)\n",
        "6. After applying conv2d with 150 filters of (5x5),\n",
        "  * Output shape = ((5 + 0 - 5) / 1) + 1 = 1\n",
        "  * No of Learning Parameters = ([ 16 x (5 * 5) x 1] + 1) * 120 filters = 48120\n",
        "7. Apply Linear Layer of 84 neurons,\n",
        "  * No of Learning Parameters = (120 * 84 + 84) = 10164\n",
        "8. Apply Linear Layer of 10 neurons,\n",
        "  * No of Learning Parameters = (84 * 10 + 10) = 850\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6rxu7NFIlpx"
      },
      "outputs": [],
      "source": [
        "# Importing necessary modules\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "!pip install torchsummaryX --quiet\n",
        "from torchsummaryX import summary as summaryX\n",
        "from torchsummary import summary\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2xgkq3Y6Svj",
        "outputId": "c443139a-ee82-4cc2-d3ce-023eb923cc5e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LeNet(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
              "  (relu1): ReLU()\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
              "  (relu2): ReLU()\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=256, out_features=120, bias=False)\n",
              "  (relu3): ReLU()\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=False)\n",
              "  (relu4): ReLU()\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class LeNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LeNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 6, 5, bias=False)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.pool1 = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5, bias=False)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.pool2 = nn.MaxPool2d(2, 2)\n",
        "    self.fc1 = nn.Linear(256, 120, bias=False)\n",
        "    self.relu3 = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(120, 84, bias=False)\n",
        "    self.relu4 = nn.ReLU()\n",
        "    self.fc3 = nn.Linear(84, 10, bias=False)\n",
        "    # self.q = q\n",
        "    # if q:\n",
        "    #   self.quant = QuantStub()\n",
        "    #   self.dequant = DeQuantStub()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.relu1(x)\n",
        "    x = self.pool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.relu2(x)\n",
        "    x = self.pool2(x)\n",
        "    # Be careful to use reshape here instead of view\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu3(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.relu4(x)\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "model = LeNet()\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JegPc_r6h7cU",
        "outputId": "adb12c65-bc6e-4359-fabd-9580a2df8a80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10])\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 6, 24, 24]             150\n",
            "              ReLU-2            [-1, 6, 24, 24]               0\n",
            "         MaxPool2d-3            [-1, 6, 12, 12]               0\n",
            "            Conv2d-4             [-1, 16, 8, 8]           2,400\n",
            "              ReLU-5             [-1, 16, 8, 8]               0\n",
            "         MaxPool2d-6             [-1, 16, 4, 4]               0\n",
            "            Linear-7                  [-1, 120]          30,720\n",
            "              ReLU-8                  [-1, 120]               0\n",
            "            Linear-9                   [-1, 84]          10,080\n",
            "             ReLU-10                   [-1, 84]               0\n",
            "           Linear-11                   [-1, 10]             840\n",
            "================================================================\n",
            "Total params: 44,190\n",
            "Trainable params: 44,190\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.08\n",
            "Params size (MB): 0.17\n",
            "Estimated Total Size (MB): 0.25\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(64,1,28,28)\n",
        "output = model(x)\n",
        "print(output.shape)\n",
        "summary(model, (1,28,28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcql3rGfMPiz"
      },
      "source": [
        "# Loading MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NC71TWVL-qb",
        "outputId": "06875bd9-94d7-489f-f95f-6d1ed9428aaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 11673392.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 343213.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3173168.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3599873.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "learning_rate = 0.01\n",
        "num_epochs = 10\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "trainset = datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "                                          shuffle=True, num_workers=16, pin_memory=True)\n",
        "\n",
        "testset = datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=16, pin_memory=True)\n",
        "dataset_sizes = {'train':len(trainset), 'test':len(testset)}\n",
        "\n",
        "model = LeNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "yj9gtF1eUmXq",
        "outputId": "8b68215f-c286-46f1-b635-9de0f347a3b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "        <p>Loss: 0.0000   0 / 60000</p>\n",
              "        <progress value='0' max='60000', style='width: 100%'>0</progress>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 1, 28, 28])\n",
            "Training complete in 0m 1s\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import HTML, display\n",
        "class ProgressMonitor(object):\n",
        "    \"\"\"\n",
        "    Custom IPython progress bar for training\n",
        "    \"\"\"\n",
        "\n",
        "    tmpl = \"\"\"\n",
        "        <p>Loss: {loss:0.4f}   {value} / {length}</p>\n",
        "        <progress value='{value}' max='{length}', style='width: 100%'>{value}</progress>\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, length):\n",
        "        self.length = length\n",
        "        self.count = 0\n",
        "        self.display = display(self.html(0, 0), display_id=True)\n",
        "\n",
        "    def html(self, count, loss):\n",
        "        return HTML(self.tmpl.format(length=self.length, value=count, loss=loss))\n",
        "\n",
        "    def update(self, count, loss):\n",
        "        self.count += count\n",
        "        self.display.update(self.html(self.count, loss))\n",
        "\n",
        "def train_new(model,criterion,optimizer,num_epochs,dataloaders,dataset_sizes,first_epoch=1):\n",
        "  since = time.time()\n",
        "  best_loss = 999999\n",
        "  best_epoch = -1\n",
        "  last_train_loss = -1\n",
        "  plot_train_loss = []\n",
        "  plot_valid_loss = []\n",
        "\n",
        "\n",
        "  for epoch in range(first_epoch, first_epoch + num_epochs):\n",
        "      print()\n",
        "      print('Epoch', epoch)\n",
        "      running_loss = 0.0\n",
        "      valid_loss = 0.0\n",
        "\n",
        "      # train phase\n",
        "      model.train()\n",
        "\n",
        "      # create a progress bar\n",
        "      progress = ProgressMonitor(length=dataset_sizes[\"train\"])\n",
        "\n",
        "      for data in dataloaders[0]:\n",
        "          # Move the training data to the GPU\n",
        "          inputs, labels  = data\n",
        "          batch_size = inputs.shape[0]\n",
        "          print(inputs.shape)\n",
        "          # break\n",
        "          inputs = Variable(inputs.to(device))\n",
        "          labels = Variable(labels.to(device))\n",
        "\n",
        "          # clear previous gradient computation\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.data * batch_size\n",
        "          # update progress bar\n",
        "          progress.update(batch_size, running_loss)\n",
        "      # break\n",
        "      epoch_loss = running_loss / dataset_sizes[\"train\"]\n",
        "      print('Training loss:', epoch_loss.item())\n",
        "      writer.add_scalar('Training Loss', epoch_loss, epoch)\n",
        "      plot_train_loss.append(epoch_loss)\n",
        "\n",
        "      # validation phase\n",
        "      model.eval()\n",
        "      # We don't need gradients for validation, so wrap in\n",
        "      # no_grad to save memory\n",
        "      with torch.no_grad():\n",
        "        for data in dataloaders[-1]:\n",
        "            inputs, labels  = data\n",
        "            batch_size = inputs.shape[0]\n",
        "\n",
        "            inputs = Variable(inputs.to(device))\n",
        "            labels = Variable(labels.to(device))\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # calculate the loss\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # update running loss value\n",
        "            valid_loss += loss.data * batch_size\n",
        "\n",
        "      epoch_valid_loss = valid_loss / dataset_sizes[\"test\"]\n",
        "      print('Validation loss:', epoch_valid_loss.item())\n",
        "      plot_valid_loss.append(epoch_valid_loss)\n",
        "      writer.add_scalar('Validation Loss', epoch_valid_loss, epoch)\n",
        "\n",
        "  time_elapsed = time.time() - since\n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "  return plot_train_loss, plot_valid_loss, model\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  train_losses, valid_losses, model = train_new(model = model ,criterion = criterion,optimizer = optimizer,\n",
        "                                              num_epochs=10,dataloaders = [train_loader, test_loader],dataset_sizes = dataset_sizes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlGGwg81bQ-5",
        "outputId": "da177a18-4d1f-424a-f882-c7f67e4df40c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Predicted 59161 correctly out of 60000 from training dataset, Acuracy : 98.60\n",
            "Model Predicted 9827 correctly out of 10000 from testing dataset, Acuracy : 98.27\n"
          ]
        }
      ],
      "source": [
        "def accuracy(loader, model, train=True):\n",
        "    num_correct = num_samples = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for data in loader:\n",
        "        inputs, labels  = data\n",
        "        batch_size = inputs.shape[0]\n",
        "\n",
        "        inputs = Variable(inputs.to(device))\n",
        "        labels = Variable(labels.to(device))\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = outputs.max(1)\n",
        "        num_correct += (preds == labels).sum()\n",
        "        num_samples += preds.size(0)\n",
        "    accuracy = (num_correct.item()/num_samples)*100\n",
        "    if train:\n",
        "      print(\"Model Predicted {} correctly out of {} from training dataset, Acuracy : {:.2f}\".format(num_correct.item(), num_samples, accuracy))\n",
        "    else:\n",
        "      print(\"Model Predicted {} correctly out of {} from testing dataset, Acuracy : {:.2f}\".format(num_correct.item(), num_samples, accuracy))\n",
        "    model.train()\n",
        "\n",
        "accuracy(train_loader, model)\n",
        "accuracy(test_loader, model, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KO3zXz3bP7Lj"
      },
      "outputs": [],
      "source": [
        "import torch.quantization\n",
        "from torch.quantization import QuantStub, DeQuantStub\n",
        "\n",
        "class QuantLeNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(QuantLeNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 6, 5, bias=False)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.pool1 = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5, bias=False)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.pool2 = nn.MaxPool2d(2, 2)\n",
        "    self.fc1 = nn.Linear(256, 120, bias=False)\n",
        "    self.relu3 = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(120, 84, bias=False)\n",
        "    self.relu4 = nn.ReLU()\n",
        "    self.fc3 = nn.Linear(84, 10, bias=False)\n",
        "\n",
        "    self.quant = QuantStub()\n",
        "    self.dequant = DeQuantStub()\n",
        "\n",
        "  def forward(self, x):\n",
        "  \n",
        "    x = self.quant(x)\n",
        "    \n",
        "    # print(x,\" \",type(x))\n",
        "    x = self.conv1(x)\n",
        "    # print(x)\n",
        "\n",
        "    x = self.relu1(x)\n",
        "    x = self.pool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.relu2(x)\n",
        "    x = self.pool2(x)\n",
        "\n",
        "    # Be careful to use reshape here instead of view\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu3(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.relu4(x)\n",
        "    x = self.fc3(x)\n",
        "    x = self.dequant(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_sa3QsGU5gI",
        "outputId": "bcae2540-cda2-4304-a552-1e1bcf370e81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QuantLeNet(\n",
              "  (conv1): ConvReLU2d(\n",
              "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
              "    (1): ReLU()\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (relu1): Identity()\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): ConvReLU2d(\n",
              "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
              "    (1): ReLU()\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (relu2): Identity()\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): LinearReLU(\n",
              "    (0): Linear(in_features=256, out_features=120, bias=False)\n",
              "    (1): ReLU()\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (relu3): Identity()\n",
              "  (fc2): LinearReLU(\n",
              "    (0): Linear(in_features=120, out_features=84, bias=False)\n",
              "    (1): ReLU()\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (relu4): Identity()\n",
              "  (fc3): Linear(\n",
              "    in_features=84, out_features=10, bias=False\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net_quantized = QuantLeNet().to(device)\n",
        "\n",
        "\n",
        "# Copy weights from unquantized model\n",
        "net_quantized.load_state_dict(model.state_dict())\n",
        "\n",
        "net_quantized = torch.quantization.fuse_modules(net_quantized, [['conv1', 'relu1'],\n",
        "                                            ['conv2', 'relu2'],\n",
        "                                            ['fc1', 'relu3'],\n",
        "                                            ['fc2', 'relu4']], inplace=True)\n",
        "\n",
        "net_quantized.eval()\n",
        "\n",
        "net_quantized.qconfig = torch.ao.quantization.default_qconfig\n",
        "net_quantized = torch.ao.quantization.prepare(net_quantized) # Insert observers\n",
        "net_quantized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UlkyFDaW_u-",
        "outputId": "27953b11-fd35-4122-b465-adea6deea16a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Predicted 9827 correctly out of 10000 from testing dataset, Acuracy : 98.27\n"
          ]
        }
      ],
      "source": [
        "accuracy(test_loader, net_quantized, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCdJ_sVqXJTq",
        "outputId": "0e32c8a8-f709-4756-c975-fdfe502c174d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QuantLeNet(\n",
              "  (conv1): ConvReLU2d(\n",
              "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
              "    (1): ReLU()\n",
              "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=6.905801773071289)\n",
              "  )\n",
              "  (relu1): Identity()\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): ConvReLU2d(\n",
              "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
              "    (1): ReLU()\n",
              "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=36.02035140991211)\n",
              "  )\n",
              "  (relu2): Identity()\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): LinearReLU(\n",
              "    (0): Linear(in_features=256, out_features=120, bias=False)\n",
              "    (1): ReLU()\n",
              "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=208.48196411132812)\n",
              "  )\n",
              "  (relu3): Identity()\n",
              "  (fc2): LinearReLU(\n",
              "    (0): Linear(in_features=120, out_features=84, bias=False)\n",
              "    (1): ReLU()\n",
              "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=309.3049011230469)\n",
              "  )\n",
              "  (relu4): Identity()\n",
              "  (fc3): Linear(\n",
              "    in_features=84, out_features=10, bias=False\n",
              "    (activation_post_process): MinMaxObserver(min_val=-181.9198455810547, max_val=73.59796142578125)\n",
              "  )\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): MinMaxObserver(min_val=-1.0, max_val=1.0)\n",
              "  )\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net_quantized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqZCgaYcW-xx",
        "outputId": "f8b87b8c-f2a3-4316-9185-45ce95b8d33b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QuantLeNet(\n",
              "  (conv1): QuantizedConvReLU2d(1, 6, kernel_size=(5, 5), stride=(1, 1), scale=0.05437639355659485, zero_point=0, bias=False)\n",
              "  (relu1): Identity()\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): QuantizedConvReLU2d(6, 16, kernel_size=(5, 5), stride=(1, 1), scale=0.2836248278617859, zero_point=0, bias=False)\n",
              "  (relu2): Identity()\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): QuantizedLinearReLU(in_features=256, out_features=120, scale=1.6415902376174927, zero_point=0, qscheme=torch.per_tensor_affine)\n",
              "  (relu3): Identity()\n",
              "  (fc2): QuantizedLinearReLU(in_features=120, out_features=84, scale=2.435471773147583, zero_point=0, qscheme=torch.per_tensor_affine)\n",
              "  (relu4): Identity()\n",
              "  (fc3): QuantizedLinear(in_features=84, out_features=10, scale=2.011951208114624, zero_point=90, qscheme=torch.per_tensor_affine)\n",
              "  (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
              "  (dequant): DeQuantize()\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net_quantized = torch.ao.quantization.convert(net_quantized)\n",
        "net_quantized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr7ZWCnGXn23",
        "outputId": "9767eee4-e094-4be0-9430-1d991b0f17cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Predicted 9801 correctly out of 10000 from testing dataset, Acuracy : 98.01\n"
          ]
        }
      ],
      "source": [
        "accuracy(test_loader, net_quantized, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "op6ZNPq0RP6r"
      },
      "outputs": [],
      "source": [
        "torch.save(net_quantized.state_dict(), \"./model_weights.pt\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
