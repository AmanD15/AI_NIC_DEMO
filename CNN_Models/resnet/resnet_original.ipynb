{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from utils.meters import AverageMeter, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 40\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Image preprocessing modules\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "# CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./cifar10',\n",
    "                                             train=True, \n",
    "                                             transform=transform,\n",
    "                                             download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./cifar10',\n",
    "                                            train=False, \n",
    "                                            transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=100,shuffle=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                                          \n",
    "\n",
    "# 3x3 convolution\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                     stride=stride, padding=1, bias=False)\n",
    "\n",
    "# Residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        #TODO\n",
    "        # 1) convert x(in float) to posit\n",
    "        # 2) convert back posit to float and store in x\n",
    "        # 3) repeat the same for all the inputs of conv functions\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        #TODO\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            #TODO - apply conversion from float to posit and then to float of x in below line\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# ResNet\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(3, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "         #TODO\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        #TODO\n",
    "        out = self.layer1(out)\n",
    "\n",
    "        #TODO\n",
    "        out = self.layer2(out)\n",
    "\n",
    "        #TODO\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "model = ResNet(ResidualBlock, [2, 2, 2]).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195738\n",
      "Epoch [1/40], Step [100/500] Loss: 1.5788 Top 1: 44.0 Top 5: 89.0\n",
      "Epoch [1/40], Step [200/500] Loss: 1.3178 Top 1: 53.0 Top 5: 94.0\n",
      "Epoch [1/40], Step [300/500] Loss: 1.3418 Top 1: 52.0 Top 5: 91.0\n",
      "Epoch [1/40], Step [400/500] Loss: 1.2328 Top 1: 55.0 Top 5: 96.0\n",
      "Epoch [1/40], Step [500/500] Loss: 1.1940 Top 1: 59.0 Top 5: 98.0\n",
      "Epoch [2/40], Step [100/500] Loss: 1.2657 Top 1: 58.0 Top 5: 95.0\n",
      "Epoch [2/40], Step [200/500] Loss: 1.0963 Top 1: 58.0 Top 5: 97.0\n",
      "Epoch [2/40], Step [300/500] Loss: 1.0623 Top 1: 61.0 Top 5: 96.0\n",
      "Epoch [2/40], Step [400/500] Loss: 0.8725 Top 1: 66.0 Top 5: 98.0\n",
      "Epoch [2/40], Step [500/500] Loss: 0.8634 Top 1: 65.0 Top 5: 99.0\n",
      "Epoch [3/40], Step [100/500] Loss: 1.0047 Top 1: 61.0 Top 5: 96.0\n",
      "Epoch [3/40], Step [200/500] Loss: 0.6753 Top 1: 76.0 Top 5: 98.0\n",
      "Epoch [3/40], Step [300/500] Loss: 0.7319 Top 1: 73.0 Top 5: 99.0\n",
      "Epoch [3/40], Step [400/500] Loss: 0.6651 Top 1: 78.0 Top 5: 100.0\n",
      "Epoch [3/40], Step [500/500] Loss: 0.7196 Top 1: 79.0 Top 5: 99.0\n",
      "Epoch [4/40], Step [100/500] Loss: 0.6404 Top 1: 77.0 Top 5: 99.0\n",
      "Epoch [4/40], Step [200/500] Loss: 0.8934 Top 1: 68.0 Top 5: 96.0\n",
      "Epoch [4/40], Step [300/500] Loss: 0.8069 Top 1: 72.0 Top 5: 99.0\n",
      "Epoch [4/40], Step [400/500] Loss: 0.6463 Top 1: 76.0 Top 5: 99.0\n",
      "Epoch [4/40], Step [500/500] Loss: 0.6285 Top 1: 75.0 Top 5: 99.0\n",
      "Epoch [5/40], Step [100/500] Loss: 0.8435 Top 1: 69.0 Top 5: 97.0\n",
      "Epoch [5/40], Step [200/500] Loss: 0.9568 Top 1: 71.0 Top 5: 95.0\n",
      "Epoch [5/40], Step [300/500] Loss: 0.7366 Top 1: 71.0 Top 5: 98.0\n",
      "Epoch [5/40], Step [400/500] Loss: 0.6394 Top 1: 81.0 Top 5: 99.0\n",
      "Epoch [5/40], Step [500/500] Loss: 0.6287 Top 1: 76.0 Top 5: 100.0\n",
      "Epoch [6/40], Step [100/500] Loss: 0.8168 Top 1: 70.0 Top 5: 99.0\n",
      "Epoch [6/40], Step [200/500] Loss: 0.4694 Top 1: 85.0 Top 5: 100.0\n",
      "Epoch [6/40], Step [300/500] Loss: 0.8439 Top 1: 70.0 Top 5: 96.0\n",
      "Epoch [6/40], Step [400/500] Loss: 0.7761 Top 1: 72.0 Top 5: 99.0\n",
      "Epoch [6/40], Step [500/500] Loss: 0.4465 Top 1: 82.0 Top 5: 99.0\n",
      "Epoch [7/40], Step [100/500] Loss: 0.5247 Top 1: 80.0 Top 5: 100.0\n",
      "Epoch [7/40], Step [200/500] Loss: 0.6355 Top 1: 80.0 Top 5: 99.0\n",
      "Epoch [7/40], Step [300/500] Loss: 0.6583 Top 1: 81.0 Top 5: 98.0\n",
      "Epoch [7/40], Step [400/500] Loss: 0.5154 Top 1: 81.0 Top 5: 100.0\n",
      "Epoch [7/40], Step [500/500] Loss: 0.5402 Top 1: 80.0 Top 5: 99.0\n",
      "Epoch [8/40], Step [100/500] Loss: 0.3336 Top 1: 91.0 Top 5: 100.0\n",
      "Epoch [8/40], Step [200/500] Loss: 0.4997 Top 1: 85.0 Top 5: 100.0\n",
      "Epoch [8/40], Step [300/500] Loss: 0.5758 Top 1: 81.0 Top 5: 97.0\n",
      "Epoch [8/40], Step [400/500] Loss: 0.5450 Top 1: 80.0 Top 5: 100.0\n",
      "Epoch [8/40], Step [500/500] Loss: 0.5625 Top 1: 81.0 Top 5: 98.0\n",
      "Epoch [9/40], Step [100/500] Loss: 0.5299 Top 1: 80.0 Top 5: 100.0\n",
      "Epoch [9/40], Step [200/500] Loss: 0.5024 Top 1: 79.0 Top 5: 100.0\n",
      "Epoch [9/40], Step [300/500] Loss: 0.6313 Top 1: 83.0 Top 5: 98.0\n",
      "Epoch [9/40], Step [400/500] Loss: 0.4340 Top 1: 90.0 Top 5: 99.0\n",
      "Epoch [9/40], Step [500/500] Loss: 0.6392 Top 1: 74.0 Top 5: 97.0\n",
      "Epoch [10/40], Step [100/500] Loss: 0.3190 Top 1: 88.0 Top 5: 100.0\n",
      "Epoch [10/40], Step [200/500] Loss: 0.4785 Top 1: 80.0 Top 5: 100.0\n",
      "Epoch [10/40], Step [300/500] Loss: 0.6348 Top 1: 81.0 Top 5: 98.0\n",
      "Epoch [10/40], Step [400/500] Loss: 0.5746 Top 1: 78.0 Top 5: 100.0\n",
      "Epoch [10/40], Step [500/500] Loss: 0.5808 Top 1: 79.0 Top 5: 99.0\n",
      "Epoch [11/40], Step [100/500] Loss: 0.3799 Top 1: 86.0 Top 5: 100.0\n",
      "Epoch [11/40], Step [200/500] Loss: 0.3853 Top 1: 85.0 Top 5: 100.0\n",
      "Epoch [11/40], Step [300/500] Loss: 0.5649 Top 1: 82.0 Top 5: 99.0\n",
      "Epoch [11/40], Step [400/500] Loss: 0.4451 Top 1: 83.0 Top 5: 100.0\n",
      "Epoch [11/40], Step [500/500] Loss: 0.4274 Top 1: 86.0 Top 5: 99.0\n",
      "Epoch [12/40], Step [100/500] Loss: 0.4686 Top 1: 85.0 Top 5: 99.0\n",
      "Epoch [12/40], Step [200/500] Loss: 0.8296 Top 1: 73.0 Top 5: 97.0\n",
      "Epoch [12/40], Step [300/500] Loss: 0.6172 Top 1: 84.0 Top 5: 99.0\n",
      "Epoch [12/40], Step [400/500] Loss: 0.3746 Top 1: 86.0 Top 5: 100.0\n",
      "Epoch [12/40], Step [500/500] Loss: 0.4119 Top 1: 89.0 Top 5: 99.0\n",
      "Epoch [13/40], Step [100/500] Loss: 0.3456 Top 1: 91.0 Top 5: 99.0\n",
      "Epoch [13/40], Step [200/500] Loss: 0.4551 Top 1: 84.0 Top 5: 100.0\n",
      "Epoch [13/40], Step [300/500] Loss: 0.6108 Top 1: 78.0 Top 5: 98.0\n",
      "Epoch [13/40], Step [400/500] Loss: 0.3226 Top 1: 92.0 Top 5: 98.0\n",
      "Epoch [13/40], Step [500/500] Loss: 0.4958 Top 1: 87.0 Top 5: 98.0\n",
      "Epoch [14/40], Step [100/500] Loss: 0.3550 Top 1: 87.0 Top 5: 100.0\n",
      "Epoch [14/40], Step [200/500] Loss: 0.4036 Top 1: 81.0 Top 5: 100.0\n",
      "Epoch [14/40], Step [300/500] Loss: 0.4607 Top 1: 85.0 Top 5: 99.0\n",
      "Epoch [14/40], Step [400/500] Loss: 0.5339 Top 1: 80.0 Top 5: 100.0\n",
      "Epoch [14/40], Step [500/500] Loss: 0.3469 Top 1: 90.0 Top 5: 100.0\n",
      "Epoch [15/40], Step [100/500] Loss: 0.4349 Top 1: 88.0 Top 5: 99.0\n",
      "Epoch [15/40], Step [200/500] Loss: 0.4390 Top 1: 82.0 Top 5: 100.0\n",
      "Epoch [15/40], Step [300/500] Loss: 0.3675 Top 1: 88.0 Top 5: 100.0\n",
      "Epoch [15/40], Step [400/500] Loss: 0.6671 Top 1: 80.0 Top 5: 97.0\n",
      "Epoch [15/40], Step [500/500] Loss: 0.3399 Top 1: 91.0 Top 5: 99.0\n",
      "Epoch [16/40], Step [100/500] Loss: 0.3846 Top 1: 85.0 Top 5: 100.0\n",
      "Epoch [16/40], Step [200/500] Loss: 0.4436 Top 1: 83.0 Top 5: 100.0\n",
      "Epoch [16/40], Step [300/500] Loss: 0.2573 Top 1: 91.0 Top 5: 100.0\n",
      "Epoch [16/40], Step [400/500] Loss: 0.4509 Top 1: 85.0 Top 5: 99.0\n",
      "Epoch [16/40], Step [500/500] Loss: 0.4779 Top 1: 85.0 Top 5: 99.0\n",
      "Epoch [17/40], Step [100/500] Loss: 0.5580 Top 1: 81.0 Top 5: 100.0\n",
      "Epoch [17/40], Step [200/500] Loss: 0.3931 Top 1: 89.0 Top 5: 100.0\n",
      "Epoch [17/40], Step [300/500] Loss: 0.2109 Top 1: 92.0 Top 5: 100.0\n",
      "Epoch [17/40], Step [400/500] Loss: 0.3904 Top 1: 87.0 Top 5: 100.0\n",
      "Epoch [17/40], Step [500/500] Loss: 0.5845 Top 1: 79.0 Top 5: 100.0\n",
      "Epoch [18/40], Step [100/500] Loss: 0.3543 Top 1: 88.0 Top 5: 100.0\n",
      "Epoch [18/40], Step [200/500] Loss: 0.3847 Top 1: 89.0 Top 5: 99.0\n",
      "Epoch [18/40], Step [300/500] Loss: 0.4514 Top 1: 85.0 Top 5: 100.0\n",
      "Epoch [18/40], Step [400/500] Loss: 0.4035 Top 1: 91.0 Top 5: 100.0\n",
      "Epoch [18/40], Step [500/500] Loss: 0.3953 Top 1: 88.0 Top 5: 100.0\n",
      "Epoch [19/40], Step [100/500] Loss: 0.3579 Top 1: 84.0 Top 5: 100.0\n",
      "Epoch [19/40], Step [200/500] Loss: 0.3114 Top 1: 90.0 Top 5: 100.0\n",
      "Epoch [19/40], Step [300/500] Loss: 0.4182 Top 1: 87.0 Top 5: 99.0\n",
      "Epoch [19/40], Step [400/500] Loss: 0.3087 Top 1: 85.0 Top 5: 100.0\n",
      "Epoch [19/40], Step [500/500] Loss: 0.3002 Top 1: 91.0 Top 5: 99.0\n",
      "Epoch [20/40], Step [100/500] Loss: 0.3593 Top 1: 83.0 Top 5: 100.0\n",
      "Epoch [20/40], Step [200/500] Loss: 0.4046 Top 1: 88.0 Top 5: 99.0\n",
      "Epoch [20/40], Step [300/500] Loss: 0.3719 Top 1: 87.0 Top 5: 99.0\n",
      "Epoch [20/40], Step [400/500] Loss: 0.5361 Top 1: 83.0 Top 5: 100.0\n",
      "Epoch [20/40], Step [500/500] Loss: 0.3276 Top 1: 89.0 Top 5: 100.0\n",
      "Epoch [21/40], Step [100/500] Loss: 0.4604 Top 1: 87.0 Top 5: 100.0\n",
      "Epoch [21/40], Step [200/500] Loss: 0.4724 Top 1: 85.0 Top 5: 99.0\n",
      "Epoch [21/40], Step [300/500] Loss: 0.3221 Top 1: 89.0 Top 5: 100.0\n",
      "Epoch [21/40], Step [400/500] Loss: 0.3021 Top 1: 86.0 Top 5: 100.0\n",
      "Epoch [21/40], Step [500/500] Loss: 0.2784 Top 1: 90.0 Top 5: 99.0\n",
      "Epoch [22/40], Step [100/500] Loss: 0.3505 Top 1: 91.0 Top 5: 100.0\n",
      "Epoch [22/40], Step [200/500] Loss: 0.2159 Top 1: 91.0 Top 5: 100.0\n",
      "Epoch [22/40], Step [300/500] Loss: 0.3760 Top 1: 84.0 Top 5: 99.0\n",
      "Epoch [22/40], Step [400/500] Loss: 0.3991 Top 1: 92.0 Top 5: 100.0\n",
      "Epoch [22/40], Step [500/500] Loss: 0.2922 Top 1: 92.0 Top 5: 100.0\n",
      "Epoch [23/40], Step [100/500] Loss: 0.3265 Top 1: 88.0 Top 5: 100.0\n",
      "Epoch [23/40], Step [200/500] Loss: 0.3161 Top 1: 87.0 Top 5: 100.0\n",
      "Epoch [23/40], Step [300/500] Loss: 0.2597 Top 1: 88.0 Top 5: 100.0\n",
      "Epoch [23/40], Step [400/500] Loss: 0.3129 Top 1: 89.0 Top 5: 98.0\n",
      "Epoch [23/40], Step [500/500] Loss: 0.1855 Top 1: 97.0 Top 5: 100.0\n",
      "Epoch [24/40], Step [100/500] Loss: 0.4259 Top 1: 86.0 Top 5: 100.0\n",
      "Epoch [24/40], Step [200/500] Loss: 0.2400 Top 1: 91.0 Top 5: 100.0\n",
      "Epoch [24/40], Step [300/500] Loss: 0.1890 Top 1: 95.0 Top 5: 100.0\n",
      "Epoch [24/40], Step [400/500] Loss: 0.2199 Top 1: 93.0 Top 5: 100.0\n",
      "Epoch [24/40], Step [500/500] Loss: 0.2514 Top 1: 95.0 Top 5: 100.0\n",
      "Epoch [25/40], Step [100/500] Loss: 0.1491 Top 1: 95.0 Top 5: 100.0\n",
      "Epoch [25/40], Step [200/500] Loss: 0.3421 Top 1: 92.0 Top 5: 100.0\n",
      "Epoch [25/40], Step [300/500] Loss: 0.3964 Top 1: 86.0 Top 5: 98.0\n",
      "Epoch [25/40], Step [400/500] Loss: 0.2086 Top 1: 91.0 Top 5: 100.0\n",
      "Epoch [25/40], Step [500/500] Loss: 0.2388 Top 1: 90.0 Top 5: 100.0\n",
      "Epoch [26/40], Step [100/500] Loss: 0.2434 Top 1: 95.0 Top 5: 100.0\n",
      "Epoch [26/40], Step [200/500] Loss: 0.3959 Top 1: 88.0 Top 5: 100.0\n",
      "Epoch [26/40], Step [300/500] Loss: 0.2137 Top 1: 93.0 Top 5: 100.0\n",
      "Epoch [26/40], Step [400/500] Loss: 0.2756 Top 1: 89.0 Top 5: 100.0\n",
      "Epoch [26/40], Step [500/500] Loss: 0.3043 Top 1: 88.0 Top 5: 100.0\n",
      "Epoch [27/40], Step [100/500] Loss: 0.2110 Top 1: 93.0 Top 5: 100.0\n",
      "Epoch [27/40], Step [200/500] Loss: 0.2579 Top 1: 91.0 Top 5: 100.0\n",
      "Epoch [27/40], Step [300/500] Loss: 0.3423 Top 1: 88.0 Top 5: 100.0\n",
      "Epoch [27/40], Step [400/500] Loss: 0.2394 Top 1: 91.0 Top 5: 100.0\n",
      "Epoch [27/40], Step [500/500] Loss: 0.4003 Top 1: 87.0 Top 5: 100.0\n",
      "Epoch [28/40], Step [100/500] Loss: 0.1794 Top 1: 93.0 Top 5: 100.0\n",
      "Epoch [28/40], Step [200/500] Loss: 0.2103 Top 1: 93.0 Top 5: 100.0\n",
      "Epoch [28/40], Step [300/500] Loss: 0.2434 Top 1: 93.0 Top 5: 100.0\n",
      "Epoch [28/40], Step [400/500] Loss: 0.2723 Top 1: 89.0 Top 5: 100.0\n",
      "Epoch [28/40], Step [500/500] Loss: 0.2817 Top 1: 86.0 Top 5: 100.0\n",
      "Epoch [29/40], Step [100/500] Loss: 0.2595 Top 1: 92.0 Top 5: 100.0\n",
      "Epoch [29/40], Step [200/500] Loss: 0.2116 Top 1: 93.0 Top 5: 100.0\n",
      "Epoch [29/40], Step [300/500] Loss: 0.2676 Top 1: 90.0 Top 5: 100.0\n",
      "Epoch [29/40], Step [400/500] Loss: 0.3055 Top 1: 90.0 Top 5: 98.0\n",
      "Epoch [29/40], Step [500/500] Loss: 0.2624 Top 1: 90.0 Top 5: 100.0\n",
      "Epoch [30/40], Step [100/500] Loss: 0.2321 Top 1: 90.0 Top 5: 100.0\n",
      "Epoch [30/40], Step [200/500] Loss: 0.2623 Top 1: 90.0 Top 5: 100.0\n",
      "Epoch [30/40], Step [300/500] Loss: 0.2944 Top 1: 93.0 Top 5: 100.0\n",
      "Epoch [30/40], Step [400/500] Loss: 0.2987 Top 1: 88.0 Top 5: 100.0\n",
      "Epoch [30/40], Step [500/500] Loss: 0.0809 Top 1: 99.0 Top 5: 100.0\n",
      "Epoch [31/40], Step [100/500] Loss: 0.2155 Top 1: 92.0 Top 5: 100.0\n",
      "Epoch [31/40], Step [200/500] Loss: 0.2047 Top 1: 91.0 Top 5: 100.0\n",
      "Epoch [31/40], Step [300/500] Loss: 0.1494 Top 1: 93.0 Top 5: 100.0\n",
      "Epoch [31/40], Step [400/500] Loss: 0.3465 Top 1: 90.0 Top 5: 100.0\n",
      "Epoch [31/40], Step [500/500] Loss: 0.2207 Top 1: 93.0 Top 5: 99.0\n",
      "Epoch [32/40], Step [100/500] Loss: 0.2057 Top 1: 91.0 Top 5: 100.0\n",
      "Epoch [32/40], Step [200/500] Loss: 0.3335 Top 1: 87.0 Top 5: 99.0\n",
      "Epoch [32/40], Step [300/500] Loss: 0.1523 Top 1: 94.0 Top 5: 100.0\n",
      "Epoch [32/40], Step [400/500] Loss: 0.1729 Top 1: 94.0 Top 5: 100.0\n",
      "Epoch [32/40], Step [500/500] Loss: 0.2045 Top 1: 92.0 Top 5: 100.0\n",
      "Epoch [33/40], Step [100/500] Loss: 0.1960 Top 1: 95.0 Top 5: 100.0\n",
      "Epoch [33/40], Step [200/500] Loss: 0.2743 Top 1: 91.0 Top 5: 100.0\n",
      "Epoch [33/40], Step [300/500] Loss: 0.2827 Top 1: 91.0 Top 5: 100.0\n",
      "Epoch [33/40], Step [400/500] Loss: 0.1710 Top 1: 92.0 Top 5: 100.0\n",
      "Epoch [33/40], Step [500/500] Loss: 0.2173 Top 1: 92.0 Top 5: 100.0\n",
      "Epoch [34/40], Step [100/500] Loss: 0.1858 Top 1: 94.0 Top 5: 100.0\n",
      "Epoch [34/40], Step [200/500] Loss: 0.1712 Top 1: 95.0 Top 5: 100.0\n",
      "Epoch [34/40], Step [300/500] Loss: 0.1743 Top 1: 94.0 Top 5: 100.0\n",
      "Epoch [34/40], Step [400/500] Loss: 0.2633 Top 1: 88.0 Top 5: 100.0\n",
      "Epoch [34/40], Step [500/500] Loss: 0.2467 Top 1: 91.0 Top 5: 100.0\n",
      "Epoch [35/40], Step [100/500] Loss: 0.1558 Top 1: 93.0 Top 5: 100.0\n",
      "Epoch [35/40], Step [200/500] Loss: 0.2292 Top 1: 94.0 Top 5: 98.0\n",
      "Epoch [35/40], Step [300/500] Loss: 0.1544 Top 1: 95.0 Top 5: 100.0\n",
      "Epoch [35/40], Step [400/500] Loss: 0.2165 Top 1: 93.0 Top 5: 100.0\n",
      "Epoch [35/40], Step [500/500] Loss: 0.1996 Top 1: 94.0 Top 5: 100.0\n",
      "Epoch [36/40], Step [100/500] Loss: 0.1614 Top 1: 96.0 Top 5: 100.0\n",
      "Epoch [36/40], Step [200/500] Loss: 0.3323 Top 1: 88.0 Top 5: 99.0\n",
      "Epoch [36/40], Step [300/500] Loss: 0.2503 Top 1: 91.0 Top 5: 100.0\n",
      "Epoch [36/40], Step [400/500] Loss: 0.2208 Top 1: 94.0 Top 5: 100.0\n",
      "Epoch [36/40], Step [500/500] Loss: 0.1613 Top 1: 93.0 Top 5: 100.0\n",
      "Epoch [37/40], Step [100/500] Loss: 0.2312 Top 1: 91.0 Top 5: 100.0\n",
      "Epoch [37/40], Step [200/500] Loss: 0.2325 Top 1: 93.0 Top 5: 100.0\n",
      "Epoch [37/40], Step [300/500] Loss: 0.2831 Top 1: 90.0 Top 5: 100.0\n",
      "Epoch [37/40], Step [400/500] Loss: 0.4314 Top 1: 88.0 Top 5: 100.0\n",
      "Epoch [37/40], Step [500/500] Loss: 0.3657 Top 1: 87.0 Top 5: 99.0\n",
      "Epoch [38/40], Step [100/500] Loss: 0.1369 Top 1: 95.0 Top 5: 100.0\n",
      "Epoch [38/40], Step [200/500] Loss: 0.3821 Top 1: 88.0 Top 5: 98.0\n",
      "Epoch [38/40], Step [300/500] Loss: 0.1628 Top 1: 96.0 Top 5: 100.0\n",
      "Epoch [38/40], Step [400/500] Loss: 0.1919 Top 1: 90.0 Top 5: 100.0\n",
      "Epoch [38/40], Step [500/500] Loss: 0.2452 Top 1: 94.0 Top 5: 100.0\n",
      "Epoch [39/40], Step [100/500] Loss: 0.2121 Top 1: 94.0 Top 5: 100.0\n",
      "Epoch [39/40], Step [200/500] Loss: 0.1281 Top 1: 94.0 Top 5: 100.0\n",
      "Epoch [39/40], Step [300/500] Loss: 0.1024 Top 1: 98.0 Top 5: 100.0\n",
      "Epoch [39/40], Step [400/500] Loss: 0.2115 Top 1: 91.0 Top 5: 100.0\n",
      "Epoch [39/40], Step [500/500] Loss: 0.1115 Top 1: 97.0 Top 5: 100.0\n",
      "Epoch [40/40], Step [100/500] Loss: 0.1381 Top 1: 96.0 Top 5: 100.0\n",
      "Epoch [40/40], Step [200/500] Loss: 0.1789 Top 1: 94.0 Top 5: 100.0\n",
      "Epoch [40/40], Step [300/500] Loss: 0.2329 Top 1: 93.0 Top 5: 99.0\n",
      "Epoch [40/40], Step [400/500] Loss: 0.2246 Top 1: 91.0 Top 5: 100.0\n",
      "Epoch [40/40], Step [500/500] Loss: 0.2936 Top 1: 92.0 Top 5: 100.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_parameters = sum([l.nelement() for l in model.parameters()])\n",
    "print(num_parameters)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "curr_lr = learning_rate\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        prec1, prec5 = accuracy(outputs.detach(), labels, topk=(1, 5))\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f} Top 1: {} Top 5: {}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item(),prec1,prec5))\n",
    "\n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 86.64 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "# torch.save(model.state_dict(), 'resnet.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "ResNet                                   --\n",
       "├─Conv2d: 1-1                            432\n",
       "├─BatchNorm2d: 1-2                       32\n",
       "├─ReLU: 1-3                              --\n",
       "├─Sequential: 1-4                        --\n",
       "│    └─ResidualBlock: 2-1                --\n",
       "│    │    └─Conv2d: 3-1                  2,304\n",
       "│    │    └─BatchNorm2d: 3-2             32\n",
       "│    │    └─ReLU: 3-3                    --\n",
       "│    │    └─Conv2d: 3-4                  2,304\n",
       "│    │    └─BatchNorm2d: 3-5             32\n",
       "│    └─ResidualBlock: 2-2                --\n",
       "│    │    └─Conv2d: 3-6                  2,304\n",
       "│    │    └─BatchNorm2d: 3-7             32\n",
       "│    │    └─ReLU: 3-8                    --\n",
       "│    │    └─Conv2d: 3-9                  2,304\n",
       "│    │    └─BatchNorm2d: 3-10            32\n",
       "├─Sequential: 1-5                        --\n",
       "│    └─ResidualBlock: 2-3                --\n",
       "│    │    └─Conv2d: 3-11                 4,608\n",
       "│    │    └─BatchNorm2d: 3-12            64\n",
       "│    │    └─ReLU: 3-13                   --\n",
       "│    │    └─Conv2d: 3-14                 9,216\n",
       "│    │    └─BatchNorm2d: 3-15            64\n",
       "│    │    └─Sequential: 3-16             4,672\n",
       "│    └─ResidualBlock: 2-4                --\n",
       "│    │    └─Conv2d: 3-17                 9,216\n",
       "│    │    └─BatchNorm2d: 3-18            64\n",
       "│    │    └─ReLU: 3-19                   --\n",
       "│    │    └─Conv2d: 3-20                 9,216\n",
       "│    │    └─BatchNorm2d: 3-21            64\n",
       "├─Sequential: 1-6                        --\n",
       "│    └─ResidualBlock: 2-5                --\n",
       "│    │    └─Conv2d: 3-22                 18,432\n",
       "│    │    └─BatchNorm2d: 3-23            128\n",
       "│    │    └─ReLU: 3-24                   --\n",
       "│    │    └─Conv2d: 3-25                 36,864\n",
       "│    │    └─BatchNorm2d: 3-26            128\n",
       "│    │    └─Sequential: 3-27             18,560\n",
       "│    └─ResidualBlock: 2-6                --\n",
       "│    │    └─Conv2d: 3-28                 36,864\n",
       "│    │    └─BatchNorm2d: 3-29            128\n",
       "│    │    └─ReLU: 3-30                   --\n",
       "│    │    └─Conv2d: 3-31                 36,864\n",
       "│    │    └─BatchNorm2d: 3-32            128\n",
       "├─AvgPool2d: 1-7                         --\n",
       "├─Linear: 1-8                            650\n",
       "=================================================================\n",
       "Total params: 195,738\n",
       "Trainable params: 195,738\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]             432\n",
      "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
      "              ReLU-3           [-1, 16, 32, 32]               0\n",
      "            Conv2d-4           [-1, 16, 32, 32]           2,304\n",
      "       BatchNorm2d-5           [-1, 16, 32, 32]              32\n",
      "              ReLU-6           [-1, 16, 32, 32]               0\n",
      "            Conv2d-7           [-1, 16, 32, 32]           2,304\n",
      "       BatchNorm2d-8           [-1, 16, 32, 32]              32\n",
      "              ReLU-9           [-1, 16, 32, 32]               0\n",
      "    ResidualBlock-10           [-1, 16, 32, 32]               0\n",
      "           Conv2d-11           [-1, 16, 32, 32]           2,304\n",
      "      BatchNorm2d-12           [-1, 16, 32, 32]              32\n",
      "             ReLU-13           [-1, 16, 32, 32]               0\n",
      "           Conv2d-14           [-1, 16, 32, 32]           2,304\n",
      "      BatchNorm2d-15           [-1, 16, 32, 32]              32\n",
      "             ReLU-16           [-1, 16, 32, 32]               0\n",
      "    ResidualBlock-17           [-1, 16, 32, 32]               0\n",
      "           Conv2d-18           [-1, 32, 16, 16]           4,608\n",
      "      BatchNorm2d-19           [-1, 32, 16, 16]              64\n",
      "             ReLU-20           [-1, 32, 16, 16]               0\n",
      "           Conv2d-21           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-22           [-1, 32, 16, 16]              64\n",
      "           Conv2d-23           [-1, 32, 16, 16]           4,608\n",
      "      BatchNorm2d-24           [-1, 32, 16, 16]              64\n",
      "             ReLU-25           [-1, 32, 16, 16]               0\n",
      "    ResidualBlock-26           [-1, 32, 16, 16]               0\n",
      "           Conv2d-27           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-28           [-1, 32, 16, 16]              64\n",
      "             ReLU-29           [-1, 32, 16, 16]               0\n",
      "           Conv2d-30           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-31           [-1, 32, 16, 16]              64\n",
      "             ReLU-32           [-1, 32, 16, 16]               0\n",
      "    ResidualBlock-33           [-1, 32, 16, 16]               0\n",
      "           Conv2d-34             [-1, 64, 8, 8]          18,432\n",
      "      BatchNorm2d-35             [-1, 64, 8, 8]             128\n",
      "             ReLU-36             [-1, 64, 8, 8]               0\n",
      "           Conv2d-37             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-38             [-1, 64, 8, 8]             128\n",
      "           Conv2d-39             [-1, 64, 8, 8]          18,432\n",
      "      BatchNorm2d-40             [-1, 64, 8, 8]             128\n",
      "             ReLU-41             [-1, 64, 8, 8]               0\n",
      "    ResidualBlock-42             [-1, 64, 8, 8]               0\n",
      "           Conv2d-43             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-44             [-1, 64, 8, 8]             128\n",
      "             ReLU-45             [-1, 64, 8, 8]               0\n",
      "           Conv2d-46             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-47             [-1, 64, 8, 8]             128\n",
      "             ReLU-48             [-1, 64, 8, 8]               0\n",
      "    ResidualBlock-49             [-1, 64, 8, 8]               0\n",
      "        AvgPool2d-50             [-1, 64, 1, 1]               0\n",
      "           Linear-51                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 195,738\n",
      "Trainable params: 195,738\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.63\n",
      "Params size (MB): 0.75\n",
      "Estimated Total Size (MB): 4.38\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, 32, 32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
