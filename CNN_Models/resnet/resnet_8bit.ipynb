{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "from models.qresnet import ResNet_cifar10, BasicBlock\n",
    "from preprocess import get_transform\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from utils.log import setup_logging, ResultsLog, save_checkpoint\n",
    "from utils.meters import AverageMeter, accuracy\n",
    "from utils.optim import OptimRegime\n",
    "from utils.misc import torch_dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 128\n",
    "dataset = \"cifar10\"\n",
    "input_size = 32\n",
    "classes = 10\n",
    "depth = 18\n",
    "device = \"cpu\"\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(data_loader, model, criterion, epoch=0, training=True, optimizer=None):\n",
    "    regularizer = getattr(model, 'regularization', None)\n",
    "    # if args.device_ids and len(args.device_ids) > 1:\n",
    "    #     model = torch.nn.DataParallel(model, args.device_ids)\n",
    "        \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (inputs, target) in enumerate(data_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        target = target.to(\"cpu\")\n",
    "        inputs = inputs.to(\"cpu\", dtype=dtype)\n",
    "\n",
    "        # compute output\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, target)\n",
    "        if regularizer is not None:\n",
    "            loss += regularizer(model)\n",
    "\n",
    "        if type(output) is list:\n",
    "            output = output[0]\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.detach(), target, topk=(1, 5))\n",
    "        losses.update(float(loss), inputs.size(0))\n",
    "        top1.update(float(prec1), inputs.size(0))\n",
    "        top5.update(float(prec5), inputs.size(0))\n",
    "\n",
    "        if training:\n",
    "            # optimizer.update(epoch, epoch * len(data_loader) + i)\n",
    "            # compute gradient and do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        print_freq = 10\n",
    "        if i % print_freq == 0:\n",
    "            logging.info('{phase} - Epoch: [{0}][{1}/{2}]\\t'\n",
    "                         'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                         'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                         'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                         'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                         'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                             epoch, i, len(data_loader),\n",
    "                             phase='TRAINING' if training else 'EVALUATING',\n",
    "                             batch_time=batch_time,\n",
    "                             data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "    return losses.avg, top1.avg, top5.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, criterion, epoch, optimizer):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    return forward(data_loader, model, criterion, epoch,\n",
    "                   training=True, optimizer=optimizer)\n",
    "\n",
    "\n",
    "def validate(data_loader, model, criterion, epoch):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        return forward(data_loader, model, criterion, epoch,\n",
    "                       training=False, optimizer=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "saving to ./results/2024-02-22_02-44-33\n",
      "number of parameters: 175274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet_cifar10(\n",
       "  (conv1): QConv2d(\n",
       "    3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "    (quantize_input): QuantMeasure()\n",
       "  )\n",
       "  (bn1): RangeBN(\n",
       "    (quantize_input): QuantMeasure()\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): QConv2d(\n",
       "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (bn1): RangeBN(\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (conv2): QConv2d(\n",
       "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (bn2): RangeBN(\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (add_relu): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): QConv2d(\n",
       "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (bn1): RangeBN(\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (conv2): QConv2d(\n",
       "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (bn2): RangeBN(\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (add_relu): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): QConv2d(\n",
       "        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (bn1): RangeBN(\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (conv2): QConv2d(\n",
       "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (bn2): RangeBN(\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): QConv2d(\n",
       "          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "          (quantize_input): QuantMeasure()\n",
       "        )\n",
       "        (1): RangeBN(\n",
       "          (quantize_input): QuantMeasure()\n",
       "        )\n",
       "      )\n",
       "      (add_relu): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): QConv2d(\n",
       "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (bn1): RangeBN(\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (conv2): QConv2d(\n",
       "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (bn2): RangeBN(\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (add_relu): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): QConv2d(\n",
       "        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (bn1): RangeBN(\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (conv2): QConv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (bn2): RangeBN(\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): QConv2d(\n",
       "          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "          (quantize_input): QuantMeasure()\n",
       "        )\n",
       "        (1): RangeBN(\n",
       "          (quantize_input): QuantMeasure()\n",
       "        )\n",
       "      )\n",
       "      (add_relu): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): QConv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (bn1): RangeBN(\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (conv2): QConv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (bn2): RangeBN(\n",
       "        (quantize_input): QuantMeasure()\n",
       "      )\n",
       "      (add_relu): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "  (fc): QLinear(\n",
       "    in_features=64, out_features=10, bias=True\n",
       "    (quantize_input): QuantMeasure()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global best_prec1, dtype\n",
    "best_prec1 = 0\n",
    "# args = parser.parse_args()\n",
    "dtype = torch.float\n",
    "torch.manual_seed(seed)\n",
    "time_stamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "# if args.evaluate:\n",
    "#     args.results_dir = '/tmp'\n",
    "# if args.save is '':\n",
    "#     args.save = time_stamp\n",
    "save_path = os.path.join(\"./results\", time_stamp)\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "setup_logging(os.path.join(save_path, 'log.txt'))\n",
    "results_path = os.path.join(save_path, 'results')\n",
    "results = ResultsLog(\n",
    "    results_path, title='Training Results - %s' % time_stamp)\n",
    "\n",
    "logging.info(\"saving to %s\", save_path)\n",
    "# logging.debug(\"run arguments: %s\", args)\n",
    "\n",
    "# To use cuda, uncomment below\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed_all(seed)\n",
    "#     torch.cuda.set_device(0)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "\n",
    "# create model\n",
    "# logging.info(\"creating model %s\", args.model)\n",
    "# model = models.__dict__[args.model]\n",
    "# model_config = {'input_size': args.input_size, 'dataset': args.dataset}\n",
    "\n",
    "# if args.model_config is not '':\n",
    "#     model_config = dict(model_config, **literal_eval(args.model_config))\n",
    "\n",
    "model = ResNet_cifar10(num_classes=classes,block=BasicBlock, depth=depth)\n",
    "# logging.info(\"created model with configuration: %s\", model_config)\n",
    "\n",
    "# TODO: implement resume training and only validation features\n",
    "# if evaluate:\n",
    "#     if not os.path.isfile(args.evaluate):\n",
    "#         parser.error('invalid checkpoint: {}'.format(args.evaluate))\n",
    "#     checkpoint = torch.load(args.evaluate)\n",
    "#     model.load_state_dict(checkpoint['state_dict'])\n",
    "#     logging.info(\"loaded checkpoint '%s' (epoch %s)\",\n",
    "#                     args.evaluate, checkpoint['epoch'])\n",
    "# elif resume:\n",
    "#     checkpoint_file = args.resume\n",
    "#     if os.path.isdir(checkpoint_file):\n",
    "#         results.load(os.path.join(checkpoint_file, 'results.csv'))\n",
    "#         checkpoint_file = os.path.join(\n",
    "#             checkpoint_file, 'model_best.pth.tar')\n",
    "#     if os.path.isfile(checkpoint_file):\n",
    "#         logging.info(\"loading checkpoint '%s'\", args.resume)\n",
    "#         checkpoint = torch.load(checkpoint_file)\n",
    "#         args.start_epoch = checkpoint['epoch'] - 1\n",
    "#         best_prec1 = checkpoint['best_prec1']\n",
    "#         model.load_state_dict(checkpoint['state_dict'])\n",
    "#         logging.info(\"loaded checkpoint '%s' (epoch %s)\",\n",
    "#                         checkpoint_file, checkpoint['epoch'])\n",
    "#     else:\n",
    "#         logging.error(\"no checkpoint found at '%s'\", args.resume)\n",
    "\n",
    "num_parameters = sum([l.nelement() for l in model.parameters()])\n",
    "logging.info(\"number of parameters: %d\", num_parameters)\n",
    "\n",
    "# Data loading code\n",
    "default_transform = {\n",
    "    'train': get_transform(dataset,\n",
    "                            input_size=input_size, augment=True),\n",
    "    'eval': get_transform(dataset,\n",
    "                            input_size=input_size, augment=False)\n",
    "}\n",
    "transform = getattr(model, 'input_transform', default_transform)\n",
    "regime = getattr(model, 'regime')\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = getattr(model, 'criterion', nn.CrossEntropyLoss)()\n",
    "criterion.to(device, dtype)\n",
    "model.to(device, dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_data = datasets.CIFAR10(root=\"./cifar10\",\n",
    "                train=False,\n",
    "                transform=transform['eval'],\n",
    "                download=False)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "                val_data,\n",
    "                batch_size=batch_size, shuffle=False,\n",
    "                num_workers=8, pin_memory=True)\n",
    "\n",
    "# if args.evaluate:\n",
    "#     validate(val_loader, model, criterion, 0)\n",
    "#     exit\n",
    "\n",
    "train_data = datasets.CIFAR10(root=\"./cifar10\",\n",
    "                train=True,\n",
    "                transform=transform['train'],\n",
    "                download=False)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                train_data,\n",
    "                batch_size=batch_size, shuffle=True,\n",
    "                num_workers=8, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training regime: [{'epoch': 0, 'optimizer': 'SGD', 'lr': 0.1, 'weight_decay': 0.0001, 'momentum': 0.9}, {'epoch': 81, 'lr': 0.01}, {'epoch': 122, 'lr': 0.001, 'weight_decay': 0}, {'epoch': 164, 'lr': 0.0001}]\n",
      "TRAINING - Epoch: [0][0/391]\tTime 1.460 (1.460)\tData 0.249 (0.249)\tLoss 2.3503 (2.3503)\tPrec@1 8.594 (8.594)\tPrec@5 46.875 (46.875)\n",
      "TRAINING - Epoch: [0][10/391]\tTime 0.805 (0.861)\tData 0.001 (0.025)\tLoss 2.2445 (2.3216)\tPrec@1 22.656 (10.938)\tPrec@5 70.312 (61.364)\n",
      "TRAINING - Epoch: [0][20/391]\tTime 0.788 (0.832)\tData 0.002 (0.014)\tLoss 2.0746 (2.2564)\tPrec@1 21.875 (14.286)\tPrec@5 76.562 (66.369)\n",
      "TRAINING - Epoch: [0][30/391]\tTime 0.765 (0.816)\tData 0.002 (0.010)\tLoss 2.1097 (2.2098)\tPrec@1 25.781 (17.036)\tPrec@5 74.219 (69.178)\n",
      "TRAINING - Epoch: [0][40/391]\tTime 0.775 (0.819)\tData 0.002 (0.008)\tLoss 2.0262 (2.1816)\tPrec@1 20.312 (18.293)\tPrec@5 83.594 (71.189)\n",
      "TRAINING - Epoch: [0][50/391]\tTime 0.750 (0.811)\tData 0.002 (0.007)\tLoss 2.0238 (2.1520)\tPrec@1 26.562 (19.332)\tPrec@5 79.688 (72.993)\n",
      "TRAINING - Epoch: [0][60/391]\tTime 0.808 (0.807)\tData 0.002 (0.006)\tLoss 2.1221 (2.1346)\tPrec@1 20.312 (20.223)\tPrec@5 75.781 (74.103)\n",
      "TRAINING - Epoch: [0][70/391]\tTime 0.790 (0.804)\tData 0.002 (0.005)\tLoss 2.0247 (2.1215)\tPrec@1 25.000 (20.863)\tPrec@5 81.250 (74.791)\n",
      "TRAINING - Epoch: [0][80/391]\tTime 0.758 (0.800)\tData 0.002 (0.005)\tLoss 1.9637 (2.1040)\tPrec@1 26.562 (21.576)\tPrec@5 86.719 (75.617)\n",
      "TRAINING - Epoch: [0][90/391]\tTime 0.764 (0.799)\tData 0.002 (0.005)\tLoss 1.9228 (2.0908)\tPrec@1 22.656 (21.969)\tPrec@5 80.469 (76.193)\n",
      "TRAINING - Epoch: [0][100/391]\tTime 0.771 (0.798)\tData 0.002 (0.004)\tLoss 1.9996 (2.0794)\tPrec@1 17.969 (22.208)\tPrec@5 80.469 (76.686)\n",
      "TRAINING - Epoch: [0][110/391]\tTime 0.792 (0.797)\tData 0.002 (0.004)\tLoss 1.9703 (2.0665)\tPrec@1 25.000 (22.663)\tPrec@5 82.812 (77.358)\n",
      "TRAINING - Epoch: [0][120/391]\tTime 0.757 (0.795)\tData 0.002 (0.004)\tLoss 2.0274 (2.0552)\tPrec@1 28.125 (23.153)\tPrec@5 85.156 (77.931)\n",
      "TRAINING - Epoch: [0][130/391]\tTime 0.750 (0.795)\tData 0.002 (0.004)\tLoss 1.8271 (2.0424)\tPrec@1 32.031 (23.640)\tPrec@5 84.375 (78.507)\n",
      "TRAINING - Epoch: [0][140/391]\tTime 0.944 (0.795)\tData 0.002 (0.004)\tLoss 1.8267 (2.0321)\tPrec@1 39.844 (24.080)\tPrec@5 85.156 (79.023)\n",
      "TRAINING - Epoch: [0][150/391]\tTime 0.843 (0.794)\tData 0.002 (0.004)\tLoss 1.9544 (2.0167)\tPrec@1 27.344 (24.550)\tPrec@5 87.500 (79.574)\n",
      "TRAINING - Epoch: [0][160/391]\tTime 0.741 (0.793)\tData 0.002 (0.004)\tLoss 1.9103 (2.0059)\tPrec@1 29.688 (25.015)\tPrec@5 85.938 (80.027)\n",
      "TRAINING - Epoch: [0][170/391]\tTime 0.769 (0.792)\tData 0.002 (0.003)\tLoss 1.7321 (1.9928)\tPrec@1 35.156 (25.434)\tPrec@5 89.062 (80.473)\n",
      "TRAINING - Epoch: [0][180/391]\tTime 0.753 (0.792)\tData 0.002 (0.003)\tLoss 1.7435 (1.9817)\tPrec@1 35.938 (25.833)\tPrec@5 85.156 (80.879)\n",
      "TRAINING - Epoch: [0][190/391]\tTime 0.754 (0.791)\tData 0.003 (0.003)\tLoss 1.8591 (1.9704)\tPrec@1 28.125 (26.133)\tPrec@5 86.719 (81.266)\n",
      "TRAINING - Epoch: [0][200/391]\tTime 0.780 (0.790)\tData 0.002 (0.003)\tLoss 2.0643 (1.9639)\tPrec@1 25.000 (26.423)\tPrec@5 80.469 (81.573)\n",
      "TRAINING - Epoch: [0][210/391]\tTime 0.757 (0.789)\tData 0.002 (0.003)\tLoss 1.8120 (1.9549)\tPrec@1 27.344 (26.677)\tPrec@5 88.281 (81.931)\n",
      "TRAINING - Epoch: [0][220/391]\tTime 0.830 (0.789)\tData 0.002 (0.003)\tLoss 1.7424 (1.9455)\tPrec@1 33.594 (26.994)\tPrec@5 88.281 (82.236)\n",
      "TRAINING - Epoch: [0][230/391]\tTime 0.761 (0.789)\tData 0.002 (0.003)\tLoss 1.5387 (1.9353)\tPrec@1 42.188 (27.364)\tPrec@5 89.062 (82.481)\n",
      "TRAINING - Epoch: [0][240/391]\tTime 0.797 (0.789)\tData 0.002 (0.003)\tLoss 1.8237 (1.9280)\tPrec@1 30.469 (27.661)\tPrec@5 85.938 (82.715)\n",
      "TRAINING - Epoch: [0][250/391]\tTime 0.756 (0.789)\tData 0.002 (0.003)\tLoss 1.7349 (1.9188)\tPrec@1 30.469 (27.969)\tPrec@5 87.500 (82.915)\n",
      "TRAINING - Epoch: [0][260/391]\tTime 0.747 (0.789)\tData 0.002 (0.003)\tLoss 1.8688 (1.9113)\tPrec@1 30.469 (28.242)\tPrec@5 82.812 (83.124)\n",
      "TRAINING - Epoch: [0][270/391]\tTime 0.811 (0.789)\tData 0.002 (0.003)\tLoss 1.7205 (1.9028)\tPrec@1 40.625 (28.615)\tPrec@5 86.719 (83.401)\n",
      "TRAINING - Epoch: [0][280/391]\tTime 0.757 (0.788)\tData 0.002 (0.003)\tLoss 1.6372 (1.8948)\tPrec@1 37.500 (28.923)\tPrec@5 91.406 (83.658)\n",
      "TRAINING - Epoch: [0][290/391]\tTime 0.774 (0.788)\tData 0.002 (0.003)\tLoss 1.7906 (1.8872)\tPrec@1 35.156 (29.242)\tPrec@5 84.375 (83.870)\n",
      "TRAINING - Epoch: [0][300/391]\tTime 0.735 (0.788)\tData 0.002 (0.003)\tLoss 1.6836 (1.8796)\tPrec@1 39.062 (29.620)\tPrec@5 89.844 (84.051)\n",
      "TRAINING - Epoch: [0][310/391]\tTime 0.814 (0.787)\tData 0.002 (0.003)\tLoss 1.6844 (1.8722)\tPrec@1 40.625 (29.899)\tPrec@5 89.844 (84.242)\n",
      "TRAINING - Epoch: [0][320/391]\tTime 0.743 (0.787)\tData 0.002 (0.003)\tLoss 1.6281 (1.8632)\tPrec@1 43.750 (30.259)\tPrec@5 92.188 (84.450)\n",
      "TRAINING - Epoch: [0][330/391]\tTime 0.743 (0.786)\tData 0.002 (0.003)\tLoss 1.6081 (1.8537)\tPrec@1 44.531 (30.643)\tPrec@5 90.625 (84.654)\n",
      "TRAINING - Epoch: [0][340/391]\tTime 0.778 (0.786)\tData 0.002 (0.003)\tLoss 1.5710 (1.8473)\tPrec@1 37.500 (30.863)\tPrec@5 94.531 (84.824)\n",
      "TRAINING - Epoch: [0][350/391]\tTime 0.792 (0.786)\tData 0.002 (0.003)\tLoss 1.6192 (1.8389)\tPrec@1 32.812 (31.205)\tPrec@5 90.625 (84.987)\n",
      "TRAINING - Epoch: [0][360/391]\tTime 0.810 (0.786)\tData 0.002 (0.003)\tLoss 1.5002 (1.8322)\tPrec@1 42.969 (31.477)\tPrec@5 95.312 (85.171)\n",
      "TRAINING - Epoch: [0][370/391]\tTime 0.829 (0.785)\tData 0.002 (0.003)\tLoss 1.4969 (1.8243)\tPrec@1 48.438 (31.816)\tPrec@5 90.625 (85.339)\n",
      "TRAINING - Epoch: [0][380/391]\tTime 0.725 (0.785)\tData 0.002 (0.003)\tLoss 1.4057 (1.8160)\tPrec@1 45.312 (32.146)\tPrec@5 93.750 (85.538)\n",
      "TRAINING - Epoch: [0][390/391]\tTime 0.513 (0.783)\tData 0.002 (0.003)\tLoss 1.5223 (1.8091)\tPrec@1 45.000 (32.424)\tPrec@5 96.250 (85.704)\n",
      "EVALUATING - Epoch: [0][0/79]\tTime 0.776 (0.776)\tData 0.210 (0.210)\tLoss 1.4552 (1.4552)\tPrec@1 46.094 (46.094)\tPrec@5 94.531 (94.531)\n",
      "EVALUATING - Epoch: [0][10/79]\tTime 0.343 (0.388)\tData 0.001 (0.021)\tLoss 1.4694 (1.4819)\tPrec@1 49.219 (46.236)\tPrec@5 92.969 (93.395)\n",
      "EVALUATING - Epoch: [0][20/79]\tTime 0.338 (0.366)\tData 0.002 (0.012)\tLoss 1.5157 (1.4870)\tPrec@1 48.438 (46.577)\tPrec@5 90.625 (92.857)\n",
      "EVALUATING - Epoch: [0][30/79]\tTime 0.348 (0.360)\tData 0.002 (0.009)\tLoss 1.3425 (1.4827)\tPrec@1 52.344 (46.774)\tPrec@5 94.531 (92.792)\n",
      "EVALUATING - Epoch: [0][40/79]\tTime 0.353 (0.355)\tData 0.002 (0.007)\tLoss 1.4554 (1.4814)\tPrec@1 41.406 (46.665)\tPrec@5 90.625 (92.912)\n",
      "EVALUATING - Epoch: [0][50/79]\tTime 0.323 (0.352)\tData 0.002 (0.006)\tLoss 1.6079 (1.4858)\tPrec@1 41.406 (46.783)\tPrec@5 92.969 (92.969)\n",
      "EVALUATING - Epoch: [0][60/79]\tTime 0.328 (0.353)\tData 0.002 (0.005)\tLoss 1.3475 (1.4819)\tPrec@1 58.594 (46.311)\tPrec@5 94.531 (93.161)\n",
      "EVALUATING - Epoch: [0][70/79]\tTime 0.324 (0.350)\tData 0.002 (0.005)\tLoss 1.3190 (1.4869)\tPrec@1 52.344 (46.171)\tPrec@5 96.875 (93.123)\n",
      "\n",
      " Epoch: 1\tTraining Loss 1.8091 \tTraining Prec@1 32.424 \tTraining Prec@5 85.704 \tValidation Loss 1.4852 \tValidation Prec@1 46.150 \tValidation Prec@5 93.120 \n",
      "\n",
      "Plot file saved at: /workspaces/codespaces-jupyter/results/2024-02-22_02-44-33/results.html\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "TRAINING - Epoch: [1][0/391]\tTime 1.505 (1.505)\tData 0.289 (0.289)\tLoss 1.4930 (1.4930)\tPrec@1 47.656 (47.656)\tPrec@5 90.625 (90.625)\n",
      "TRAINING - Epoch: [1][10/391]\tTime 0.791 (0.879)\tData 0.001 (0.028)\tLoss 1.4619 (1.4808)\tPrec@1 48.438 (46.378)\tPrec@5 92.188 (92.756)\n",
      "TRAINING - Epoch: [1][20/391]\tTime 0.750 (0.837)\tData 0.002 (0.015)\tLoss 1.3737 (1.5001)\tPrec@1 50.000 (45.647)\tPrec@5 96.094 (91.927)\n",
      "TRAINING - Epoch: [1][30/391]\tTime 0.827 (0.823)\tData 0.002 (0.011)\tLoss 1.4906 (1.5011)\tPrec@1 46.094 (45.413)\tPrec@5 96.875 (92.389)\n",
      "TRAINING - Epoch: [1][40/391]\tTime 0.832 (0.818)\tData 0.002 (0.009)\tLoss 1.5674 (1.5087)\tPrec@1 46.875 (45.808)\tPrec@5 93.750 (92.378)\n",
      "TRAINING - Epoch: [1][50/391]\tTime 0.751 (0.810)\tData 0.002 (0.007)\tLoss 1.4427 (1.5063)\tPrec@1 50.781 (45.741)\tPrec@5 92.969 (92.463)\n",
      "TRAINING - Epoch: [1][60/391]\tTime 0.759 (0.810)\tData 0.002 (0.007)\tLoss 1.4546 (1.4930)\tPrec@1 47.656 (46.235)\tPrec@5 93.750 (92.636)\n",
      "TRAINING - Epoch: [1][70/391]\tTime 0.755 (0.806)\tData 0.002 (0.006)\tLoss 1.3416 (1.4809)\tPrec@1 54.688 (46.699)\tPrec@5 90.625 (92.815)\n",
      "TRAINING - Epoch: [1][80/391]\tTime 0.778 (0.806)\tData 0.002 (0.005)\tLoss 1.2724 (1.4668)\tPrec@1 55.469 (47.164)\tPrec@5 94.531 (92.959)\n",
      "TRAINING - Epoch: [1][90/391]\tTime 0.901 (0.805)\tData 0.002 (0.005)\tLoss 1.2621 (1.4639)\tPrec@1 55.469 (47.450)\tPrec@5 93.750 (92.866)\n",
      "TRAINING - Epoch: [1][100/391]\tTime 0.774 (0.805)\tData 0.002 (0.005)\tLoss 1.2457 (1.4547)\tPrec@1 57.031 (47.826)\tPrec@5 92.188 (92.984)\n",
      "TRAINING - Epoch: [1][110/391]\tTime 0.785 (0.803)\tData 0.002 (0.005)\tLoss 1.3638 (1.4481)\tPrec@1 56.250 (48.205)\tPrec@5 94.531 (93.039)\n",
      "TRAINING - Epoch: [1][120/391]\tTime 0.781 (0.803)\tData 0.002 (0.004)\tLoss 1.3332 (1.4403)\tPrec@1 53.125 (48.534)\tPrec@5 93.750 (93.085)\n",
      "TRAINING - Epoch: [1][130/391]\tTime 0.846 (0.803)\tData 0.002 (0.004)\tLoss 1.5579 (1.4347)\tPrec@1 47.656 (48.825)\tPrec@5 90.625 (93.124)\n",
      "TRAINING - Epoch: [1][140/391]\tTime 0.788 (0.802)\tData 0.002 (0.004)\tLoss 1.4998 (1.4297)\tPrec@1 50.000 (49.113)\tPrec@5 91.406 (93.201)\n",
      "TRAINING - Epoch: [1][150/391]\tTime 0.753 (0.802)\tData 0.002 (0.004)\tLoss 1.5074 (1.4258)\tPrec@1 48.438 (49.208)\tPrec@5 92.188 (93.269)\n",
      "TRAINING - Epoch: [1][160/391]\tTime 0.817 (0.801)\tData 0.002 (0.004)\tLoss 1.3780 (1.4188)\tPrec@1 50.781 (49.398)\tPrec@5 95.312 (93.415)\n",
      "TRAINING - Epoch: [1][170/391]\tTime 0.768 (0.801)\tData 0.002 (0.004)\tLoss 1.2289 (1.4107)\tPrec@1 56.250 (49.744)\tPrec@5 96.094 (93.494)\n",
      "TRAINING - Epoch: [1][180/391]\tTime 0.787 (0.799)\tData 0.002 (0.004)\tLoss 1.3977 (1.4039)\tPrec@1 53.906 (49.983)\tPrec@5 96.875 (93.590)\n",
      "TRAINING - Epoch: [1][190/391]\tTime 0.758 (0.799)\tData 0.002 (0.003)\tLoss 1.2423 (1.3987)\tPrec@1 57.812 (50.262)\tPrec@5 96.094 (93.623)\n",
      "TRAINING - Epoch: [1][200/391]\tTime 0.826 (0.799)\tData 0.002 (0.003)\tLoss 1.2865 (1.3936)\tPrec@1 57.812 (50.447)\tPrec@5 94.531 (93.696)\n",
      "TRAINING - Epoch: [1][210/391]\tTime 0.773 (0.798)\tData 0.002 (0.003)\tLoss 1.3454 (1.3905)\tPrec@1 48.438 (50.552)\tPrec@5 94.531 (93.724)\n",
      "TRAINING - Epoch: [1][220/391]\tTime 0.808 (0.798)\tData 0.002 (0.003)\tLoss 1.2342 (1.3822)\tPrec@1 55.469 (50.884)\tPrec@5 93.750 (93.803)\n",
      "TRAINING - Epoch: [1][230/391]\tTime 0.769 (0.797)\tData 0.002 (0.003)\tLoss 1.2340 (1.3763)\tPrec@1 54.688 (51.092)\tPrec@5 99.219 (93.895)\n",
      "TRAINING - Epoch: [1][240/391]\tTime 0.807 (0.797)\tData 0.002 (0.003)\tLoss 1.0959 (1.3705)\tPrec@1 66.406 (51.378)\tPrec@5 96.875 (93.980)\n",
      "TRAINING - Epoch: [1][250/391]\tTime 0.834 (0.798)\tData 0.002 (0.003)\tLoss 1.3565 (1.3662)\tPrec@1 50.000 (51.578)\tPrec@5 94.531 (94.021)\n",
      "TRAINING - Epoch: [1][260/391]\tTime 1.584 (0.810)\tData 0.002 (0.003)\tLoss 1.1501 (1.3630)\tPrec@1 62.500 (51.703)\tPrec@5 93.750 (94.034)\n",
      "TRAINING - Epoch: [1][270/391]\tTime 1.317 (0.832)\tData 0.002 (0.003)\tLoss 1.2900 (1.3594)\tPrec@1 56.250 (51.828)\tPrec@5 96.875 (94.082)\n",
      "TRAINING - Epoch: [1][280/391]\tTime 1.422 (0.854)\tData 0.002 (0.003)\tLoss 1.3343 (1.3557)\tPrec@1 53.906 (52.046)\tPrec@5 92.969 (94.109)\n",
      "TRAINING - Epoch: [1][290/391]\tTime 1.445 (0.874)\tData 0.009 (0.003)\tLoss 1.2835 (1.3537)\tPrec@1 54.688 (52.156)\tPrec@5 96.094 (94.120)\n",
      "TRAINING - Epoch: [1][300/391]\tTime 1.376 (0.893)\tData 0.002 (0.003)\tLoss 1.4242 (1.3487)\tPrec@1 50.000 (52.349)\tPrec@5 92.188 (94.178)\n",
      "TRAINING - Epoch: [1][310/391]\tTime 1.469 (0.911)\tData 0.002 (0.003)\tLoss 1.3691 (1.3463)\tPrec@1 49.219 (52.459)\tPrec@5 95.312 (94.220)\n",
      "TRAINING - Epoch: [1][320/391]\tTime 1.376 (0.926)\tData 0.002 (0.003)\tLoss 1.0449 (1.3431)\tPrec@1 64.062 (52.585)\tPrec@5 97.656 (94.242)\n",
      "TRAINING - Epoch: [1][330/391]\tTime 1.348 (0.942)\tData 0.009 (0.003)\tLoss 1.1656 (1.3396)\tPrec@1 63.281 (52.736)\tPrec@5 95.312 (94.265)\n",
      "TRAINING - Epoch: [1][340/391]\tTime 1.347 (0.957)\tData 0.002 (0.003)\tLoss 1.2490 (1.3356)\tPrec@1 59.375 (52.930)\tPrec@5 93.750 (94.288)\n",
      "TRAINING - Epoch: [1][350/391]\tTime 1.453 (0.970)\tData 0.003 (0.003)\tLoss 1.3007 (1.3319)\tPrec@1 52.344 (53.065)\tPrec@5 94.531 (94.322)\n",
      "TRAINING - Epoch: [1][360/391]\tTime 1.353 (0.982)\tData 0.002 (0.004)\tLoss 1.1866 (1.3286)\tPrec@1 55.469 (53.164)\tPrec@5 95.312 (94.356)\n",
      "TRAINING - Epoch: [1][370/391]\tTime 1.335 (0.993)\tData 0.002 (0.003)\tLoss 1.0569 (1.3235)\tPrec@1 64.844 (53.392)\tPrec@5 97.656 (94.396)\n",
      "TRAINING - Epoch: [1][380/391]\tTime 1.439 (1.004)\tData 0.002 (0.003)\tLoss 1.2077 (1.3194)\tPrec@1 56.250 (53.578)\tPrec@5 98.438 (94.449)\n",
      "TRAINING - Epoch: [1][390/391]\tTime 1.070 (1.014)\tData 0.002 (0.003)\tLoss 1.2777 (1.3159)\tPrec@1 53.750 (53.712)\tPrec@5 96.250 (94.496)\n",
      "EVALUATING - Epoch: [1][0/79]\tTime 1.154 (1.154)\tData 0.409 (0.409)\tLoss 1.0864 (1.0864)\tPrec@1 57.812 (57.812)\tPrec@5 96.875 (96.875)\n",
      "EVALUATING - Epoch: [1][10/79]\tTime 0.597 (0.690)\tData 0.003 (0.041)\tLoss 1.1649 (1.1696)\tPrec@1 57.031 (58.310)\tPrec@5 96.875 (96.023)\n",
      "EVALUATING - Epoch: [1][20/79]\tTime 0.671 (0.672)\tData 0.002 (0.023)\tLoss 1.2608 (1.2212)\tPrec@1 59.375 (58.557)\tPrec@5 92.969 (95.833)\n",
      "EVALUATING - Epoch: [1][30/79]\tTime 0.651 (0.664)\tData 0.002 (0.017)\tLoss 1.1197 (1.2094)\tPrec@1 60.156 (58.795)\tPrec@5 99.219 (95.691)\n",
      "EVALUATING - Epoch: [1][40/79]\tTime 0.605 (0.652)\tData 0.002 (0.014)\tLoss 1.1782 (1.1879)\tPrec@1 58.594 (59.470)\tPrec@5 99.219 (96.151)\n",
      "EVALUATING - Epoch: [1][50/79]\tTime 0.602 (0.656)\tData 0.004 (0.012)\tLoss 1.2392 (1.1939)\tPrec@1 60.156 (59.544)\tPrec@5 95.312 (96.186)\n",
      "EVALUATING - Epoch: [1][60/79]\tTime 0.594 (0.655)\tData 0.002 (0.010)\tLoss 1.2035 (1.1898)\tPrec@1 60.156 (59.734)\tPrec@5 96.094 (96.145)\n",
      "EVALUATING - Epoch: [1][70/79]\tTime 0.603 (0.654)\tData 0.002 (0.009)\tLoss 1.0930 (1.1995)\tPrec@1 67.969 (59.254)\tPrec@5 97.656 (96.072)\n",
      "\n",
      " Epoch: 2\tTraining Loss 1.3159 \tTraining Prec@1 53.712 \tTraining Prec@5 94.496 \tValidation Loss 1.1979 \tValidation Prec@1 59.290 \tValidation Prec@5 95.960 \n",
      "\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "TRAINING - Epoch: [2][0/391]\tTime 2.412 (2.412)\tData 0.390 (0.390)\tLoss 1.0321 (1.0321)\tPrec@1 61.719 (61.719)\tPrec@5 98.438 (98.438)\n",
      "TRAINING - Epoch: [2][10/391]\tTime 1.401 (1.531)\tData 0.009 (0.039)\tLoss 0.9915 (1.1110)\tPrec@1 66.406 (62.429)\tPrec@5 97.656 (96.094)\n",
      "TRAINING - Epoch: [2][20/391]\tTime 1.616 (1.508)\tData 0.009 (0.022)\tLoss 1.0965 (1.1147)\tPrec@1 63.281 (61.830)\tPrec@5 93.750 (96.057)\n",
      "TRAINING - Epoch: [2][30/391]\tTime 1.452 (1.488)\tData 0.002 (0.017)\tLoss 1.1039 (1.1221)\tPrec@1 61.719 (62.122)\tPrec@5 95.312 (95.817)\n",
      "TRAINING - Epoch: [2][40/391]\tTime 1.356 (1.477)\tData 0.002 (0.014)\tLoss 1.1441 (1.1336)\tPrec@1 63.281 (61.928)\tPrec@5 96.875 (95.732)\n",
      "TRAINING - Epoch: [2][50/391]\tTime 1.546 (1.470)\tData 0.002 (0.012)\tLoss 0.9059 (1.1324)\tPrec@1 70.312 (61.657)\tPrec@5 99.219 (95.925)\n",
      "TRAINING - Epoch: [2][60/391]\tTime 1.291 (1.454)\tData 0.002 (0.010)\tLoss 1.1673 (1.1326)\tPrec@1 57.812 (61.770)\tPrec@5 94.531 (95.902)\n",
      "TRAINING - Epoch: [2][70/391]\tTime 1.542 (1.453)\tData 0.002 (0.009)\tLoss 1.1157 (1.1397)\tPrec@1 67.188 (61.257)\tPrec@5 95.312 (96.006)\n",
      "TRAINING - Epoch: [2][80/391]\tTime 1.354 (1.447)\tData 0.002 (0.009)\tLoss 1.0189 (1.1353)\tPrec@1 62.500 (61.372)\tPrec@5 96.875 (96.123)\n",
      "TRAINING - Epoch: [2][90/391]\tTime 1.388 (1.447)\tData 0.015 (0.008)\tLoss 1.1684 (1.1367)\tPrec@1 61.719 (61.324)\tPrec@5 96.094 (96.111)\n",
      "TRAINING - Epoch: [2][100/391]\tTime 1.465 (1.447)\tData 0.002 (0.008)\tLoss 1.1734 (1.1393)\tPrec@1 57.812 (61.200)\tPrec@5 96.875 (96.101)\n",
      "TRAINING - Epoch: [2][110/391]\tTime 1.356 (1.447)\tData 0.002 (0.007)\tLoss 1.1168 (1.1375)\tPrec@1 64.062 (61.254)\tPrec@5 95.312 (96.122)\n",
      "TRAINING - Epoch: [2][120/391]\tTime 1.630 (1.444)\tData 0.011 (0.007)\tLoss 1.1096 (1.1345)\tPrec@1 64.844 (61.422)\tPrec@5 96.094 (96.107)\n",
      "TRAINING - Epoch: [2][130/391]\tTime 1.260 (1.440)\tData 0.002 (0.007)\tLoss 1.0704 (1.1339)\tPrec@1 60.156 (61.438)\tPrec@5 99.219 (96.118)\n",
      "TRAINING - Epoch: [2][140/391]\tTime 1.479 (1.437)\tData 0.002 (0.007)\tLoss 1.1877 (1.1346)\tPrec@1 62.500 (61.480)\tPrec@5 97.656 (96.094)\n",
      "TRAINING - Epoch: [2][150/391]\tTime 1.444 (1.435)\tData 0.009 (0.007)\tLoss 1.2091 (1.1347)\tPrec@1 53.125 (61.501)\tPrec@5 96.875 (96.114)\n",
      "TRAINING - Epoch: [2][160/391]\tTime 1.421 (1.436)\tData 0.002 (0.007)\tLoss 0.9854 (1.1285)\tPrec@1 69.531 (61.714)\tPrec@5 99.219 (96.176)\n",
      "TRAINING - Epoch: [2][170/391]\tTime 1.331 (1.434)\tData 0.002 (0.007)\tLoss 1.0773 (1.1252)\tPrec@1 62.500 (61.824)\tPrec@5 98.438 (96.249)\n",
      "TRAINING - Epoch: [2][180/391]\tTime 1.311 (1.436)\tData 0.002 (0.006)\tLoss 1.0457 (1.1236)\tPrec@1 64.062 (61.857)\tPrec@5 96.875 (96.279)\n",
      "TRAINING - Epoch: [2][190/391]\tTime 1.369 (1.434)\tData 0.002 (0.006)\tLoss 1.0524 (1.1201)\tPrec@1 63.281 (61.968)\tPrec@5 96.875 (96.282)\n",
      "TRAINING - Epoch: [2][200/391]\tTime 1.706 (1.435)\tData 0.002 (0.006)\tLoss 1.2126 (1.1231)\tPrec@1 60.938 (61.866)\tPrec@5 93.750 (96.269)\n",
      "TRAINING - Epoch: [2][210/391]\tTime 1.449 (1.435)\tData 0.002 (0.006)\tLoss 1.1498 (1.1225)\tPrec@1 60.156 (61.926)\tPrec@5 97.656 (96.294)\n",
      "TRAINING - Epoch: [2][220/391]\tTime 1.293 (1.433)\tData 0.009 (0.006)\tLoss 1.1007 (1.1225)\tPrec@1 63.281 (61.892)\tPrec@5 97.656 (96.256)\n",
      "TRAINING - Epoch: [2][230/391]\tTime 1.565 (1.432)\tData 0.015 (0.006)\tLoss 1.1740 (1.1218)\tPrec@1 64.844 (61.922)\tPrec@5 96.875 (96.276)\n",
      "TRAINING - Epoch: [2][240/391]\tTime 1.293 (1.431)\tData 0.010 (0.006)\tLoss 0.9056 (1.1195)\tPrec@1 70.312 (61.913)\tPrec@5 99.219 (96.311)\n",
      "TRAINING - Epoch: [2][250/391]\tTime 1.539 (1.432)\tData 0.002 (0.006)\tLoss 1.0264 (1.1162)\tPrec@1 66.406 (62.008)\tPrec@5 97.656 (96.318)\n",
      "TRAINING - Epoch: [2][260/391]\tTime 1.523 (1.430)\tData 0.009 (0.006)\tLoss 1.0618 (1.1133)\tPrec@1 62.500 (62.153)\tPrec@5 98.438 (96.369)\n",
      "TRAINING - Epoch: [2][270/391]\tTime 1.347 (1.430)\tData 0.002 (0.006)\tLoss 1.1141 (1.1123)\tPrec@1 63.281 (62.180)\tPrec@5 94.531 (96.365)\n",
      "TRAINING - Epoch: [2][280/391]\tTime 1.434 (1.428)\tData 0.002 (0.006)\tLoss 1.0804 (1.1121)\tPrec@1 64.844 (62.191)\tPrec@5 94.531 (96.377)\n",
      "TRAINING - Epoch: [2][290/391]\tTime 1.417 (1.428)\tData 0.002 (0.006)\tLoss 0.9642 (1.1099)\tPrec@1 68.750 (62.274)\tPrec@5 97.656 (96.381)\n",
      "TRAINING - Epoch: [2][300/391]\tTime 1.479 (1.428)\tData 0.011 (0.006)\tLoss 1.0483 (1.1088)\tPrec@1 65.625 (62.336)\tPrec@5 97.656 (96.397)\n",
      "TRAINING - Epoch: [2][310/391]\tTime 1.329 (1.430)\tData 0.009 (0.006)\tLoss 1.0597 (1.1067)\tPrec@1 57.031 (62.420)\tPrec@5 98.438 (96.413)\n",
      "TRAINING - Epoch: [2][320/391]\tTime 1.366 (1.429)\tData 0.009 (0.005)\tLoss 0.8777 (1.1032)\tPrec@1 71.094 (62.527)\tPrec@5 97.656 (96.444)\n",
      "TRAINING - Epoch: [2][330/391]\tTime 1.308 (1.429)\tData 0.002 (0.005)\tLoss 1.0530 (1.1019)\tPrec@1 63.281 (62.547)\tPrec@5 96.094 (96.438)\n",
      "TRAINING - Epoch: [2][340/391]\tTime 1.470 (1.429)\tData 0.009 (0.005)\tLoss 1.0382 (1.0996)\tPrec@1 65.625 (62.654)\tPrec@5 97.656 (96.479)\n",
      "TRAINING - Epoch: [2][350/391]\tTime 1.327 (1.427)\tData 0.002 (0.005)\tLoss 1.0724 (1.0975)\tPrec@1 60.156 (62.745)\tPrec@5 98.438 (96.492)\n",
      "TRAINING - Epoch: [2][360/391]\tTime 1.323 (1.426)\tData 0.002 (0.005)\tLoss 0.9471 (1.0953)\tPrec@1 70.312 (62.859)\tPrec@5 98.438 (96.511)\n",
      "TRAINING - Epoch: [2][370/391]\tTime 1.415 (1.428)\tData 0.009 (0.005)\tLoss 1.1090 (1.0954)\tPrec@1 58.594 (62.829)\tPrec@5 97.656 (96.528)\n",
      "TRAINING - Epoch: [2][380/391]\tTime 1.325 (1.427)\tData 0.004 (0.005)\tLoss 1.0539 (1.0934)\tPrec@1 60.156 (62.865)\tPrec@5 98.438 (96.547)\n",
      "TRAINING - Epoch: [2][390/391]\tTime 0.997 (1.425)\tData 0.002 (0.005)\tLoss 0.9444 (1.0922)\tPrec@1 72.500 (62.940)\tPrec@5 98.750 (96.548)\n",
      "EVALUATING - Epoch: [2][0/79]\tTime 1.420 (1.420)\tData 0.390 (0.390)\tLoss 1.0303 (1.0303)\tPrec@1 57.812 (57.812)\tPrec@5 98.438 (98.438)\n",
      "EVALUATING - Epoch: [2][10/79]\tTime 0.571 (0.707)\tData 0.009 (0.041)\tLoss 0.9089 (0.9858)\tPrec@1 67.969 (64.631)\tPrec@5 96.875 (97.940)\n",
      "EVALUATING - Epoch: [2][20/79]\tTime 0.553 (0.675)\tData 0.002 (0.024)\tLoss 1.1058 (1.0210)\tPrec@1 60.156 (64.472)\tPrec@5 94.531 (97.247)\n",
      "EVALUATING - Epoch: [2][30/79]\tTime 0.661 (0.663)\tData 0.009 (0.018)\tLoss 0.8721 (1.0153)\tPrec@1 71.094 (65.474)\tPrec@5 100.000 (97.177)\n",
      "EVALUATING - Epoch: [2][40/79]\tTime 0.694 (0.660)\tData 0.002 (0.014)\tLoss 0.9745 (0.9973)\tPrec@1 65.625 (66.216)\tPrec@5 98.438 (97.370)\n",
      "EVALUATING - Epoch: [2][50/79]\tTime 0.610 (0.658)\tData 0.002 (0.012)\tLoss 1.0866 (1.0049)\tPrec@1 60.156 (65.916)\tPrec@5 96.875 (97.335)\n",
      "EVALUATING - Epoch: [2][60/79]\tTime 0.571 (0.655)\tData 0.002 (0.011)\tLoss 0.9468 (1.0041)\tPrec@1 69.531 (65.920)\tPrec@5 96.875 (97.298)\n",
      "EVALUATING - Epoch: [2][70/79]\tTime 0.610 (0.656)\tData 0.002 (0.010)\tLoss 0.8953 (1.0132)\tPrec@1 76.562 (65.713)\tPrec@5 98.438 (97.304)\n",
      "\n",
      " Epoch: 3\tTraining Loss 1.0922 \tTraining Prec@1 62.940 \tTraining Prec@5 96.548 \tValidation Loss 1.0086 \tValidation Prec@1 65.780 \tValidation Prec@5 97.340 \n",
      "\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "TRAINING - Epoch: [3][0/391]\tTime 2.498 (2.498)\tData 0.434 (0.434)\tLoss 0.9485 (0.9485)\tPrec@1 68.750 (68.750)\tPrec@5 97.656 (97.656)\n",
      "TRAINING - Epoch: [3][10/391]\tTime 1.440 (1.536)\tData 0.009 (0.044)\tLoss 1.0161 (0.9840)\tPrec@1 61.719 (66.690)\tPrec@5 97.656 (97.372)\n",
      "TRAINING - Epoch: [3][20/391]\tTime 1.387 (1.466)\tData 0.002 (0.025)\tLoss 1.0747 (0.9935)\tPrec@1 62.500 (66.927)\tPrec@5 99.219 (97.247)\n",
      "TRAINING - Epoch: [3][30/391]\tTime 1.381 (1.459)\tData 0.002 (0.018)\tLoss 0.9534 (0.9883)\tPrec@1 67.969 (67.339)\tPrec@5 96.875 (97.278)\n",
      "TRAINING - Epoch: [3][40/391]\tTime 1.493 (1.453)\tData 0.009 (0.015)\tLoss 1.0806 (1.0005)\tPrec@1 60.938 (66.521)\tPrec@5 98.438 (97.428)\n",
      "TRAINING - Epoch: [3][50/391]\tTime 1.318 (1.447)\tData 0.004 (0.013)\tLoss 0.9306 (0.9929)\tPrec@1 69.531 (66.682)\tPrec@5 98.438 (97.534)\n",
      "TRAINING - Epoch: [3][60/391]\tTime 1.443 (1.443)\tData 0.009 (0.011)\tLoss 0.9435 (0.9861)\tPrec@1 64.844 (66.906)\tPrec@5 96.875 (97.528)\n",
      "TRAINING - Epoch: [3][70/391]\tTime 1.362 (1.439)\tData 0.009 (0.010)\tLoss 0.8011 (0.9842)\tPrec@1 75.000 (66.978)\tPrec@5 98.438 (97.447)\n",
      "TRAINING - Epoch: [3][80/391]\tTime 1.462 (1.438)\tData 0.002 (0.010)\tLoss 0.9261 (0.9840)\tPrec@1 67.969 (66.927)\tPrec@5 96.875 (97.405)\n",
      "TRAINING - Epoch: [3][90/391]\tTime 1.297 (1.434)\tData 0.002 (0.009)\tLoss 0.9930 (0.9902)\tPrec@1 64.062 (66.604)\tPrec@5 95.312 (97.296)\n",
      "TRAINING - Epoch: [3][100/391]\tTime 1.355 (1.432)\tData 0.009 (0.009)\tLoss 0.9975 (0.9860)\tPrec@1 64.062 (66.793)\tPrec@5 97.656 (97.370)\n",
      "TRAINING - Epoch: [3][110/391]\tTime 1.717 (1.434)\tData 0.009 (0.008)\tLoss 0.9943 (0.9866)\tPrec@1 66.406 (66.829)\tPrec@5 96.875 (97.347)\n",
      "TRAINING - Epoch: [3][120/391]\tTime 1.323 (1.435)\tData 0.009 (0.008)\tLoss 1.2528 (0.9903)\tPrec@1 61.719 (66.723)\tPrec@5 96.094 (97.262)\n",
      "TRAINING - Epoch: [3][130/391]\tTime 1.553 (1.434)\tData 0.009 (0.008)\tLoss 1.0038 (0.9871)\tPrec@1 65.625 (66.889)\tPrec@5 94.531 (97.286)\n",
      "TRAINING - Epoch: [3][140/391]\tTime 1.327 (1.434)\tData 0.002 (0.008)\tLoss 0.9180 (0.9846)\tPrec@1 69.531 (66.988)\tPrec@5 97.656 (97.291)\n",
      "TRAINING - Epoch: [3][150/391]\tTime 1.362 (1.437)\tData 0.002 (0.007)\tLoss 0.8974 (0.9824)\tPrec@1 70.312 (67.063)\tPrec@5 97.656 (97.315)\n",
      "TRAINING - Epoch: [3][160/391]\tTime 1.546 (1.435)\tData 0.011 (0.007)\tLoss 0.9522 (0.9794)\tPrec@1 67.969 (67.105)\tPrec@5 99.219 (97.346)\n",
      "TRAINING - Epoch: [3][170/391]\tTime 1.316 (1.432)\tData 0.002 (0.007)\tLoss 1.0318 (0.9751)\tPrec@1 69.531 (67.265)\tPrec@5 99.219 (97.368)\n",
      "TRAINING - Epoch: [3][180/391]\tTime 1.463 (1.434)\tData 0.002 (0.007)\tLoss 0.8629 (0.9742)\tPrec@1 72.656 (67.252)\tPrec@5 96.875 (97.358)\n",
      "TRAINING - Epoch: [3][190/391]\tTime 1.392 (1.433)\tData 0.009 (0.007)\tLoss 1.0613 (0.9747)\tPrec@1 67.969 (67.290)\tPrec@5 96.875 (97.325)\n",
      "TRAINING - Epoch: [3][200/391]\tTime 1.418 (1.434)\tData 0.002 (0.007)\tLoss 0.9597 (0.9749)\tPrec@1 64.062 (67.289)\tPrec@5 98.438 (97.373)\n",
      "TRAINING - Epoch: [3][210/391]\tTime 1.434 (1.434)\tData 0.009 (0.007)\tLoss 0.9424 (0.9766)\tPrec@1 69.531 (67.258)\tPrec@5 98.438 (97.353)\n",
      "TRAINING - Epoch: [3][220/391]\tTime 1.455 (1.434)\tData 0.002 (0.007)\tLoss 0.8870 (0.9771)\tPrec@1 70.312 (67.223)\tPrec@5 99.219 (97.345)\n",
      "TRAINING - Epoch: [3][230/391]\tTime 1.486 (1.434)\tData 0.002 (0.007)\tLoss 0.8946 (0.9773)\tPrec@1 66.406 (67.225)\tPrec@5 98.438 (97.335)\n",
      "TRAINING - Epoch: [3][240/391]\tTime 1.450 (1.434)\tData 0.002 (0.006)\tLoss 1.0926 (0.9750)\tPrec@1 64.062 (67.327)\tPrec@5 95.312 (97.352)\n",
      "TRAINING - Epoch: [3][250/391]\tTime 1.392 (1.433)\tData 0.002 (0.006)\tLoss 0.9183 (0.9740)\tPrec@1 68.750 (67.368)\tPrec@5 97.656 (97.376)\n",
      "TRAINING - Epoch: [3][260/391]\tTime 1.396 (1.431)\tData 0.002 (0.006)\tLoss 1.0306 (0.9734)\tPrec@1 64.844 (67.382)\tPrec@5 95.312 (97.360)\n",
      "TRAINING - Epoch: [3][270/391]\tTime 1.416 (1.431)\tData 0.002 (0.006)\tLoss 0.8864 (0.9723)\tPrec@1 75.000 (67.447)\tPrec@5 97.656 (97.359)\n",
      "TRAINING - Epoch: [3][280/391]\tTime 1.384 (1.431)\tData 0.002 (0.006)\tLoss 0.9822 (0.9726)\tPrec@1 65.625 (67.452)\tPrec@5 98.438 (97.359)\n",
      "TRAINING - Epoch: [3][290/391]\tTime 1.504 (1.431)\tData 0.004 (0.006)\tLoss 0.9947 (0.9717)\tPrec@1 72.656 (67.496)\tPrec@5 97.656 (97.364)\n",
      "TRAINING - Epoch: [3][300/391]\tTime 1.411 (1.431)\tData 0.002 (0.006)\tLoss 0.8552 (0.9696)\tPrec@1 72.656 (67.556)\tPrec@5 98.438 (97.394)\n",
      "TRAINING - Epoch: [3][310/391]\tTime 1.295 (1.432)\tData 0.002 (0.006)\tLoss 0.8734 (0.9681)\tPrec@1 75.000 (67.615)\tPrec@5 96.875 (97.410)\n",
      "TRAINING - Epoch: [3][320/391]\tTime 1.636 (1.432)\tData 0.002 (0.006)\tLoss 0.8576 (0.9676)\tPrec@1 71.094 (67.647)\tPrec@5 99.219 (97.415)\n",
      "TRAINING - Epoch: [3][330/391]\tTime 1.499 (1.433)\tData 0.002 (0.006)\tLoss 0.8296 (0.9648)\tPrec@1 74.219 (67.801)\tPrec@5 99.219 (97.432)\n",
      "TRAINING - Epoch: [3][340/391]\tTime 1.331 (1.433)\tData 0.009 (0.006)\tLoss 0.8562 (0.9639)\tPrec@1 68.750 (67.811)\tPrec@5 97.656 (97.445)\n",
      "TRAINING - Epoch: [3][350/391]\tTime 1.429 (1.433)\tData 0.002 (0.006)\tLoss 1.0808 (0.9626)\tPrec@1 64.844 (67.862)\tPrec@5 96.875 (97.456)\n",
      "TRAINING - Epoch: [3][360/391]\tTime 1.290 (1.432)\tData 0.002 (0.006)\tLoss 0.8818 (0.9597)\tPrec@1 71.875 (67.964)\tPrec@5 97.656 (97.472)\n",
      "TRAINING - Epoch: [3][370/391]\tTime 1.311 (1.433)\tData 0.002 (0.006)\tLoss 0.8318 (0.9588)\tPrec@1 67.969 (67.958)\tPrec@5 99.219 (97.498)\n",
      "TRAINING - Epoch: [3][380/391]\tTime 1.307 (1.432)\tData 0.002 (0.006)\tLoss 1.0423 (0.9582)\tPrec@1 65.625 (67.991)\tPrec@5 97.656 (97.515)\n",
      "TRAINING - Epoch: [3][390/391]\tTime 1.192 (1.430)\tData 0.002 (0.006)\tLoss 1.0229 (0.9571)\tPrec@1 70.000 (68.058)\tPrec@5 93.750 (97.508)\n",
      "EVALUATING - Epoch: [3][0/79]\tTime 1.310 (1.310)\tData 0.348 (0.348)\tLoss 0.8566 (0.8566)\tPrec@1 69.531 (69.531)\tPrec@5 98.438 (98.438)\n",
      "EVALUATING - Epoch: [3][10/79]\tTime 0.557 (0.680)\tData 0.002 (0.036)\tLoss 0.8111 (0.9378)\tPrec@1 71.875 (69.957)\tPrec@5 98.438 (98.082)\n",
      "EVALUATING - Epoch: [3][20/79]\tTime 0.608 (0.658)\tData 0.002 (0.021)\tLoss 1.0521 (0.9834)\tPrec@1 63.281 (68.043)\tPrec@5 95.312 (97.545)\n",
      "EVALUATING - Epoch: [3][30/79]\tTime 0.590 (0.653)\tData 0.002 (0.015)\tLoss 0.9023 (0.9681)\tPrec@1 69.531 (69.330)\tPrec@5 98.438 (97.455)\n",
      "EVALUATING - Epoch: [3][40/79]\tTime 0.648 (0.646)\tData 0.009 (0.012)\tLoss 0.8982 (0.9577)\tPrec@1 70.312 (69.493)\tPrec@5 97.656 (97.561)\n",
      "EVALUATING - Epoch: [3][50/79]\tTime 0.574 (0.641)\tData 0.014 (0.011)\tLoss 1.0415 (0.9629)\tPrec@1 68.750 (69.133)\tPrec@5 97.656 (97.641)\n",
      "EVALUATING - Epoch: [3][60/79]\tTime 0.592 (0.639)\tData 0.011 (0.010)\tLoss 1.0473 (0.9598)\tPrec@1 65.625 (68.904)\tPrec@5 96.875 (97.631)\n",
      "EVALUATING - Epoch: [3][70/79]\tTime 0.587 (0.633)\tData 0.002 (0.009)\tLoss 0.9350 (0.9712)\tPrec@1 70.312 (68.673)\tPrec@5 96.875 (97.623)\n",
      "\n",
      " Epoch: 4\tTraining Loss 0.9571 \tTraining Prec@1 68.058 \tTraining Prec@5 97.508 \tValidation Loss 0.9685 \tValidation Prec@1 68.710 \tValidation Prec@5 97.620 \n",
      "\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "TRAINING - Epoch: [4][0/391]\tTime 2.851 (2.851)\tData 0.424 (0.424)\tLoss 0.9354 (0.9354)\tPrec@1 64.844 (64.844)\tPrec@5 99.219 (99.219)\n",
      "TRAINING - Epoch: [4][10/391]\tTime 1.505 (1.592)\tData 0.001 (0.042)\tLoss 1.0658 (0.9109)\tPrec@1 63.281 (68.892)\tPrec@5 98.438 (98.011)\n",
      "TRAINING - Epoch: [4][20/391]\tTime 1.353 (1.505)\tData 0.009 (0.025)\tLoss 0.8955 (0.9260)\tPrec@1 71.875 (68.564)\tPrec@5 100.000 (97.842)\n",
      "TRAINING - Epoch: [4][30/391]\tTime 1.329 (1.461)\tData 0.002 (0.018)\tLoss 0.8655 (0.9194)\tPrec@1 68.750 (68.952)\tPrec@5 98.438 (97.681)\n",
      "TRAINING - Epoch: [4][40/391]\tTime 1.346 (1.454)\tData 0.012 (0.015)\tLoss 0.9101 (0.9024)\tPrec@1 67.969 (69.550)\tPrec@5 96.875 (97.790)\n",
      "TRAINING - Epoch: [4][50/391]\tTime 1.462 (1.447)\tData 0.002 (0.012)\tLoss 0.9192 (0.9006)\tPrec@1 70.312 (69.868)\tPrec@5 98.438 (97.886)\n",
      "TRAINING - Epoch: [4][60/391]\tTime 1.478 (1.441)\tData 0.002 (0.011)\tLoss 0.8310 (0.9000)\tPrec@1 68.750 (69.634)\tPrec@5 98.438 (97.887)\n",
      "TRAINING - Epoch: [4][70/391]\tTime 1.297 (1.441)\tData 0.008 (0.010)\tLoss 0.9716 (0.8940)\tPrec@1 66.406 (69.905)\tPrec@5 97.656 (97.953)\n",
      "TRAINING - Epoch: [4][80/391]\tTime 1.431 (1.441)\tData 0.009 (0.009)\tLoss 0.8184 (0.8955)\tPrec@1 75.781 (69.927)\tPrec@5 99.219 (98.013)\n",
      "TRAINING - Epoch: [4][90/391]\tTime 1.389 (1.438)\tData 0.002 (0.008)\tLoss 0.9769 (0.8911)\tPrec@1 67.969 (70.064)\tPrec@5 93.750 (97.940)\n",
      "TRAINING - Epoch: [4][100/391]\tTime 1.547 (1.439)\tData 0.002 (0.008)\tLoss 0.8873 (0.8907)\tPrec@1 74.219 (70.127)\tPrec@5 96.094 (97.950)\n",
      "TRAINING - Epoch: [4][110/391]\tTime 1.348 (1.439)\tData 0.002 (0.008)\tLoss 0.7664 (0.8865)\tPrec@1 73.438 (70.186)\tPrec@5 99.219 (97.959)\n",
      "TRAINING - Epoch: [4][120/391]\tTime 1.324 (1.437)\tData 0.002 (0.008)\tLoss 0.7291 (0.8847)\tPrec@1 76.562 (70.274)\tPrec@5 96.875 (97.973)\n",
      "TRAINING - Epoch: [4][130/391]\tTime 1.387 (1.436)\tData 0.009 (0.008)\tLoss 0.9273 (0.8845)\tPrec@1 71.875 (70.348)\tPrec@5 97.656 (97.954)\n",
      "TRAINING - Epoch: [4][140/391]\tTime 1.551 (1.435)\tData 0.002 (0.007)\tLoss 0.7503 (0.8808)\tPrec@1 74.219 (70.495)\tPrec@5 99.219 (97.983)\n",
      "TRAINING - Epoch: [4][150/391]\tTime 1.385 (1.435)\tData 0.004 (0.007)\tLoss 0.6706 (0.8788)\tPrec@1 76.562 (70.644)\tPrec@5 100.000 (97.998)\n",
      "TRAINING - Epoch: [4][160/391]\tTime 1.444 (1.435)\tData 0.002 (0.007)\tLoss 0.8103 (0.8760)\tPrec@1 71.094 (70.759)\tPrec@5 99.219 (98.030)\n",
      "TRAINING - Epoch: [4][170/391]\tTime 1.438 (1.436)\tData 0.009 (0.007)\tLoss 0.8725 (0.8793)\tPrec@1 72.656 (70.696)\tPrec@5 97.656 (98.035)\n",
      "TRAINING - Epoch: [4][180/391]\tTime 1.317 (1.434)\tData 0.002 (0.007)\tLoss 1.0993 (0.8809)\tPrec@1 64.062 (70.757)\tPrec@5 95.312 (97.989)\n",
      "TRAINING - Epoch: [4][190/391]\tTime 1.391 (1.433)\tData 0.009 (0.006)\tLoss 0.9191 (0.8786)\tPrec@1 66.406 (70.787)\tPrec@5 100.000 (98.033)\n",
      "TRAINING - Epoch: [4][200/391]\tTime 1.509 (1.431)\tData 0.002 (0.006)\tLoss 0.8525 (0.8760)\tPrec@1 72.656 (70.896)\tPrec@5 98.438 (98.049)\n",
      "TRAINING - Epoch: [4][210/391]\tTime 1.389 (1.429)\tData 0.002 (0.006)\tLoss 0.8629 (0.8736)\tPrec@1 72.656 (71.023)\tPrec@5 99.219 (98.052)\n",
      "TRAINING - Epoch: [4][220/391]\tTime 1.364 (1.426)\tData 0.009 (0.006)\tLoss 0.7187 (0.8707)\tPrec@1 75.781 (71.126)\tPrec@5 98.438 (98.073)\n",
      "TRAINING - Epoch: [4][230/391]\tTime 1.441 (1.423)\tData 0.002 (0.006)\tLoss 0.8886 (0.8707)\tPrec@1 71.094 (71.117)\tPrec@5 98.438 (98.079)\n",
      "TRAINING - Epoch: [4][240/391]\tTime 1.317 (1.421)\tData 0.002 (0.006)\tLoss 0.7785 (0.8696)\tPrec@1 74.219 (71.168)\tPrec@5 100.000 (98.081)\n",
      "TRAINING - Epoch: [4][250/391]\tTime 1.342 (1.419)\tData 0.002 (0.006)\tLoss 0.7316 (0.8696)\tPrec@1 75.000 (71.231)\tPrec@5 97.656 (98.083)\n",
      "TRAINING - Epoch: [4][260/391]\tTime 1.369 (1.417)\tData 0.002 (0.006)\tLoss 0.8541 (0.8688)\tPrec@1 71.094 (71.249)\tPrec@5 97.656 (98.093)\n",
      "TRAINING - Epoch: [4][270/391]\tTime 1.308 (1.417)\tData 0.002 (0.006)\tLoss 0.8820 (0.8682)\tPrec@1 76.562 (71.281)\tPrec@5 99.219 (98.080)\n",
      "TRAINING - Epoch: [4][280/391]\tTime 1.340 (1.415)\tData 0.002 (0.006)\tLoss 0.8006 (0.8652)\tPrec@1 71.875 (71.350)\tPrec@5 99.219 (98.093)\n",
      "TRAINING - Epoch: [4][290/391]\tTime 1.561 (1.415)\tData 0.009 (0.006)\tLoss 0.9167 (0.8648)\tPrec@1 69.531 (71.408)\tPrec@5 96.094 (98.086)\n",
      "TRAINING - Epoch: [4][300/391]\tTime 1.429 (1.413)\tData 0.009 (0.006)\tLoss 0.8772 (0.8628)\tPrec@1 72.656 (71.504)\tPrec@5 98.438 (98.095)\n",
      "TRAINING - Epoch: [4][310/391]\tTime 1.303 (1.412)\tData 0.009 (0.006)\tLoss 0.8625 (0.8634)\tPrec@1 68.750 (71.528)\tPrec@5 98.438 (98.088)\n",
      "TRAINING - Epoch: [4][320/391]\tTime 1.358 (1.410)\tData 0.002 (0.006)\tLoss 0.7595 (0.8625)\tPrec@1 76.562 (71.632)\tPrec@5 98.438 (98.085)\n",
      "TRAINING - Epoch: [4][330/391]\tTime 1.269 (1.410)\tData 0.002 (0.006)\tLoss 0.9270 (0.8616)\tPrec@1 74.219 (71.722)\tPrec@5 96.094 (98.074)\n",
      "TRAINING - Epoch: [4][340/391]\tTime 1.280 (1.409)\tData 0.002 (0.006)\tLoss 0.7566 (0.8607)\tPrec@1 75.000 (71.756)\tPrec@5 99.219 (98.080)\n",
      "TRAINING - Epoch: [4][350/391]\tTime 1.355 (1.408)\tData 0.002 (0.006)\tLoss 0.6754 (0.8601)\tPrec@1 78.125 (71.788)\tPrec@5 99.219 (98.081)\n",
      "TRAINING - Epoch: [4][360/391]\tTime 1.262 (1.406)\tData 0.009 (0.006)\tLoss 0.9189 (0.8589)\tPrec@1 69.531 (71.845)\tPrec@5 96.875 (98.089)\n",
      "TRAINING - Epoch: [4][370/391]\tTime 1.352 (1.406)\tData 0.002 (0.006)\tLoss 0.7956 (0.8581)\tPrec@1 72.656 (71.890)\tPrec@5 99.219 (98.082)\n",
      "TRAINING - Epoch: [4][380/391]\tTime 1.283 (1.404)\tData 0.002 (0.005)\tLoss 0.8570 (0.8558)\tPrec@1 70.312 (71.984)\tPrec@5 99.219 (98.091)\n",
      "TRAINING - Epoch: [4][390/391]\tTime 0.883 (1.402)\tData 0.002 (0.005)\tLoss 0.9880 (0.8549)\tPrec@1 73.750 (72.016)\tPrec@5 100.000 (98.074)\n",
      "EVALUATING - Epoch: [4][0/79]\tTime 1.201 (1.201)\tData 0.353 (0.353)\tLoss 0.8812 (0.8812)\tPrec@1 68.750 (68.750)\tPrec@5 99.219 (99.219)\n",
      "EVALUATING - Epoch: [4][10/79]\tTime 0.600 (0.687)\tData 0.006 (0.037)\tLoss 0.7647 (0.8943)\tPrec@1 73.438 (71.804)\tPrec@5 98.438 (98.722)\n",
      "EVALUATING - Epoch: [4][20/79]\tTime 0.599 (0.655)\tData 0.002 (0.020)\tLoss 0.8667 (0.9136)\tPrec@1 73.438 (71.615)\tPrec@5 98.438 (98.326)\n",
      "EVALUATING - Epoch: [4][30/79]\tTime 0.582 (0.632)\tData 0.006 (0.015)\tLoss 0.7525 (0.9153)\tPrec@1 76.562 (71.522)\tPrec@5 100.000 (98.236)\n",
      "EVALUATING - Epoch: [4][40/79]\tTime 0.565 (0.628)\tData 0.002 (0.013)\tLoss 0.8561 (0.8985)\tPrec@1 73.438 (72.180)\tPrec@5 99.219 (98.285)\n",
      "EVALUATING - Epoch: [4][50/79]\tTime 0.600 (0.627)\tData 0.002 (0.011)\tLoss 1.0305 (0.9071)\tPrec@1 69.531 (72.151)\tPrec@5 97.656 (98.346)\n",
      "EVALUATING - Epoch: [4][60/79]\tTime 0.665 (0.625)\tData 0.007 (0.010)\tLoss 0.8860 (0.9001)\tPrec@1 71.875 (72.310)\tPrec@5 97.656 (98.386)\n",
      "EVALUATING - Epoch: [4][70/79]\tTime 0.535 (0.619)\tData 0.002 (0.009)\tLoss 0.8860 (0.9064)\tPrec@1 71.875 (72.106)\tPrec@5 99.219 (98.283)\n",
      "\n",
      " Epoch: 5\tTraining Loss 0.8549 \tTraining Prec@1 72.016 \tTraining Prec@5 98.074 \tValidation Loss 0.9009 \tValidation Prec@1 72.290 \tValidation Prec@5 98.320 \n",
      "\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "TRAINING - Epoch: [5][0/391]\tTime 2.399 (2.399)\tData 0.480 (0.480)\tLoss 0.7706 (0.7706)\tPrec@1 75.000 (75.000)\tPrec@5 98.438 (98.438)\n",
      "TRAINING - Epoch: [5][10/391]\tTime 1.291 (1.448)\tData 0.009 (0.047)\tLoss 0.8152 (0.7996)\tPrec@1 76.562 (74.503)\tPrec@5 97.656 (98.722)\n",
      "TRAINING - Epoch: [5][20/391]\tTime 1.430 (1.411)\tData 0.002 (0.027)\tLoss 0.9474 (0.8033)\tPrec@1 67.188 (74.628)\tPrec@5 99.219 (98.549)\n",
      "TRAINING - Epoch: [5][30/391]\tTime 1.702 (1.419)\tData 0.002 (0.019)\tLoss 0.8051 (0.8003)\tPrec@1 74.219 (74.672)\tPrec@5 97.656 (98.337)\n",
      "TRAINING - Epoch: [5][40/391]\tTime 1.277 (1.411)\tData 0.002 (0.015)\tLoss 0.7113 (0.8089)\tPrec@1 78.906 (74.295)\tPrec@5 100.000 (98.380)\n",
      "TRAINING - Epoch: [5][50/391]\tTime 1.298 (1.402)\tData 0.002 (0.013)\tLoss 0.9091 (0.8048)\tPrec@1 71.875 (74.387)\tPrec@5 95.312 (98.330)\n",
      "TRAINING - Epoch: [5][60/391]\tTime 1.420 (1.395)\tData 0.002 (0.012)\tLoss 0.7154 (0.8098)\tPrec@1 75.781 (74.142)\tPrec@5 98.438 (98.309)\n",
      "TRAINING - Epoch: [5][70/391]\tTime 1.304 (1.391)\tData 0.002 (0.010)\tLoss 1.0157 (0.8146)\tPrec@1 66.406 (73.900)\tPrec@5 97.656 (98.261)\n",
      "TRAINING - Epoch: [5][80/391]\tTime 1.322 (1.390)\tData 0.009 (0.010)\tLoss 0.8954 (0.8184)\tPrec@1 74.219 (73.727)\tPrec@5 96.875 (98.196)\n",
      "TRAINING - Epoch: [5][90/391]\tTime 1.381 (1.385)\tData 0.002 (0.009)\tLoss 0.8582 (0.8187)\tPrec@1 76.562 (73.772)\tPrec@5 99.219 (98.240)\n",
      "TRAINING - Epoch: [5][100/391]\tTime 1.298 (1.384)\tData 0.002 (0.009)\tLoss 0.7730 (0.8156)\tPrec@1 71.094 (73.817)\tPrec@5 98.438 (98.260)\n",
      "TRAINING - Epoch: [5][110/391]\tTime 1.478 (1.383)\tData 0.003 (0.008)\tLoss 0.9126 (0.8138)\tPrec@1 67.969 (73.775)\tPrec@5 99.219 (98.297)\n",
      "TRAINING - Epoch: [5][120/391]\tTime 1.502 (1.385)\tData 0.002 (0.008)\tLoss 0.8863 (0.8120)\tPrec@1 68.750 (73.883)\tPrec@5 95.312 (98.244)\n",
      "TRAINING - Epoch: [5][130/391]\tTime 1.283 (1.383)\tData 0.002 (0.008)\tLoss 0.7570 (0.8102)\tPrec@1 78.906 (73.879)\tPrec@5 96.875 (98.229)\n",
      "TRAINING - Epoch: [5][140/391]\tTime 1.387 (1.382)\tData 0.002 (0.008)\tLoss 0.8464 (0.8115)\tPrec@1 77.344 (73.925)\tPrec@5 96.094 (98.255)\n",
      "TRAINING - Epoch: [5][150/391]\tTime 1.318 (1.380)\tData 0.002 (0.007)\tLoss 0.9906 (0.8109)\tPrec@1 70.312 (74.027)\tPrec@5 97.656 (98.236)\n",
      "TRAINING - Epoch: [5][160/391]\tTime 1.270 (1.379)\tData 0.009 (0.007)\tLoss 0.6574 (0.8095)\tPrec@1 81.250 (74.131)\tPrec@5 99.219 (98.243)\n",
      "TRAINING - Epoch: [5][170/391]\tTime 1.328 (1.379)\tData 0.009 (0.007)\tLoss 0.6719 (0.8069)\tPrec@1 79.688 (74.155)\tPrec@5 99.219 (98.246)\n",
      "TRAINING - Epoch: [5][180/391]\tTime 1.410 (1.378)\tData 0.002 (0.007)\tLoss 0.6960 (0.8068)\tPrec@1 80.469 (74.145)\tPrec@5 99.219 (98.265)\n",
      "TRAINING - Epoch: [5][190/391]\tTime 1.330 (1.380)\tData 0.002 (0.007)\tLoss 0.7075 (0.8072)\tPrec@1 78.906 (74.153)\tPrec@5 100.000 (98.274)\n",
      "TRAINING - Epoch: [5][200/391]\tTime 1.313 (1.378)\tData 0.002 (0.006)\tLoss 0.7647 (0.8039)\tPrec@1 79.688 (74.293)\tPrec@5 99.219 (98.298)\n",
      "TRAINING - Epoch: [5][210/391]\tTime 1.252 (1.377)\tData 0.002 (0.006)\tLoss 0.8817 (0.8028)\tPrec@1 69.531 (74.389)\tPrec@5 100.000 (98.289)\n",
      "TRAINING - Epoch: [5][220/391]\tTime 1.352 (1.377)\tData 0.002 (0.006)\tLoss 0.7875 (0.8015)\tPrec@1 73.438 (74.410)\tPrec@5 98.438 (98.303)\n",
      "TRAINING - Epoch: [5][230/391]\tTime 1.419 (1.376)\tData 0.009 (0.006)\tLoss 0.6632 (0.8004)\tPrec@1 81.250 (74.479)\tPrec@5 97.656 (98.306)\n",
      "TRAINING - Epoch: [5][240/391]\tTime 1.291 (1.376)\tData 0.002 (0.006)\tLoss 0.7079 (0.8007)\tPrec@1 78.125 (74.475)\tPrec@5 99.219 (98.321)\n",
      "TRAINING - Epoch: [5][250/391]\tTime 1.256 (1.375)\tData 0.009 (0.006)\tLoss 0.6080 (0.7991)\tPrec@1 80.469 (74.533)\tPrec@5 99.219 (98.316)\n",
      "TRAINING - Epoch: [5][260/391]\tTime 1.342 (1.375)\tData 0.002 (0.006)\tLoss 0.7337 (0.7969)\tPrec@1 72.656 (74.554)\tPrec@5 98.438 (98.327)\n",
      "TRAINING - Epoch: [5][270/391]\tTime 1.400 (1.373)\tData 0.002 (0.006)\tLoss 0.7612 (0.7987)\tPrec@1 71.875 (74.516)\tPrec@5 98.438 (98.322)\n",
      "TRAINING - Epoch: [5][280/391]\tTime 1.377 (1.374)\tData 0.009 (0.006)\tLoss 0.8677 (0.7971)\tPrec@1 71.094 (74.589)\tPrec@5 98.438 (98.324)\n",
      "TRAINING - Epoch: [5][290/391]\tTime 1.508 (1.375)\tData 0.003 (0.006)\tLoss 0.9138 (0.7954)\tPrec@1 69.531 (74.734)\tPrec@5 100.000 (98.317)\n",
      "TRAINING - Epoch: [5][300/391]\tTime 1.299 (1.374)\tData 0.002 (0.006)\tLoss 0.7115 (0.7956)\tPrec@1 77.344 (74.756)\tPrec@5 99.219 (98.318)\n",
      "TRAINING - Epoch: [5][310/391]\tTime 1.392 (1.374)\tData 0.002 (0.006)\tLoss 0.8811 (0.7949)\tPrec@1 71.094 (74.794)\tPrec@5 95.312 (98.304)\n",
      "TRAINING - Epoch: [5][320/391]\tTime 1.327 (1.372)\tData 0.002 (0.006)\tLoss 0.8419 (0.7951)\tPrec@1 71.875 (74.805)\tPrec@5 99.219 (98.301)\n",
      "TRAINING - Epoch: [5][330/391]\tTime 1.406 (1.373)\tData 0.002 (0.006)\tLoss 0.8091 (0.7938)\tPrec@1 77.344 (74.870)\tPrec@5 96.094 (98.294)\n",
      "TRAINING - Epoch: [5][340/391]\tTime 1.589 (1.373)\tData 0.009 (0.006)\tLoss 0.8816 (0.7921)\tPrec@1 70.312 (74.940)\tPrec@5 99.219 (98.318)\n",
      "TRAINING - Epoch: [5][350/391]\tTime 1.339 (1.372)\tData 0.010 (0.006)\tLoss 0.6587 (0.7907)\tPrec@1 81.250 (75.007)\tPrec@5 100.000 (98.328)\n",
      "TRAINING - Epoch: [5][360/391]\tTime 1.332 (1.371)\tData 0.002 (0.006)\tLoss 0.5966 (0.7893)\tPrec@1 81.250 (75.043)\tPrec@5 100.000 (98.340)\n",
      "TRAINING - Epoch: [5][370/391]\tTime 1.336 (1.371)\tData 0.002 (0.006)\tLoss 0.7884 (0.7898)\tPrec@1 76.562 (75.036)\tPrec@5 100.000 (98.351)\n",
      "TRAINING - Epoch: [5][380/391]\tTime 1.351 (1.371)\tData 0.002 (0.005)\tLoss 0.8427 (0.7882)\tPrec@1 71.094 (75.066)\tPrec@5 98.438 (98.362)\n",
      "TRAINING - Epoch: [5][390/391]\tTime 0.941 (1.369)\tData 0.002 (0.005)\tLoss 0.5302 (0.7863)\tPrec@1 81.250 (75.126)\tPrec@5 100.000 (98.380)\n",
      "EVALUATING - Epoch: [5][0/79]\tTime 1.177 (1.177)\tData 0.339 (0.339)\tLoss 0.7938 (0.7938)\tPrec@1 73.438 (73.438)\tPrec@5 99.219 (99.219)\n",
      "EVALUATING - Epoch: [5][10/79]\tTime 0.557 (0.665)\tData 0.001 (0.033)\tLoss 0.7358 (0.8501)\tPrec@1 75.000 (72.301)\tPrec@5 97.656 (98.864)\n",
      "EVALUATING - Epoch: [5][20/79]\tTime 0.544 (0.633)\tData 0.002 (0.019)\tLoss 0.8942 (0.8742)\tPrec@1 72.656 (72.545)\tPrec@5 100.000 (98.698)\n",
      "EVALUATING - Epoch: [5][30/79]\tTime 0.683 (0.622)\tData 0.002 (0.014)\tLoss 0.7051 (0.8612)\tPrec@1 76.562 (72.959)\tPrec@5 100.000 (98.513)\n",
      "EVALUATING - Epoch: [5][40/79]\tTime 0.595 (0.621)\tData 0.002 (0.011)\tLoss 0.9266 (0.8550)\tPrec@1 71.094 (73.152)\tPrec@5 99.219 (98.495)\n",
      "EVALUATING - Epoch: [5][50/79]\tTime 0.592 (0.623)\tData 0.002 (0.010)\tLoss 0.9974 (0.8590)\tPrec@1 72.656 (73.269)\tPrec@5 97.656 (98.529)\n",
      "EVALUATING - Epoch: [5][60/79]\tTime 0.592 (0.620)\tData 0.002 (0.009)\tLoss 0.9749 (0.8593)\tPrec@1 71.875 (73.258)\tPrec@5 98.438 (98.630)\n",
      "EVALUATING - Epoch: [5][70/79]\tTime 0.530 (0.615)\tData 0.002 (0.008)\tLoss 0.8276 (0.8682)\tPrec@1 72.656 (73.140)\tPrec@5 99.219 (98.559)\n",
      "\n",
      " Epoch: 6\tTraining Loss 0.7863 \tTraining Prec@1 75.126 \tTraining Prec@5 98.380 \tValidation Loss 0.8642 \tValidation Prec@1 73.180 \tValidation Prec@5 98.590 \n",
      "\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "TRAINING - Epoch: [6][0/391]\tTime 2.425 (2.425)\tData 0.384 (0.384)\tLoss 0.7341 (0.7341)\tPrec@1 78.906 (78.906)\tPrec@5 99.219 (99.219)\n",
      "TRAINING - Epoch: [6][10/391]\tTime 1.337 (1.499)\tData 0.001 (0.038)\tLoss 0.9163 (0.7461)\tPrec@1 71.094 (77.060)\tPrec@5 99.219 (98.864)\n",
      "TRAINING - Epoch: [6][20/391]\tTime 1.395 (1.428)\tData 0.002 (0.021)\tLoss 0.7838 (0.7434)\tPrec@1 75.781 (77.083)\tPrec@5 99.219 (98.661)\n",
      "TRAINING - Epoch: [6][30/391]\tTime 1.312 (1.411)\tData 0.002 (0.016)\tLoss 0.6631 (0.7610)\tPrec@1 79.688 (75.983)\tPrec@5 98.438 (98.614)\n",
      "TRAINING - Epoch: [6][40/391]\tTime 1.351 (1.400)\tData 0.009 (0.013)\tLoss 0.6358 (0.7619)\tPrec@1 82.031 (75.800)\tPrec@5 100.000 (98.647)\n",
      "TRAINING - Epoch: [6][50/391]\tTime 1.395 (1.402)\tData 0.002 (0.011)\tLoss 0.6720 (0.7601)\tPrec@1 78.906 (75.980)\tPrec@5 99.219 (98.606)\n",
      "TRAINING - Epoch: [6][60/391]\tTime 1.382 (1.397)\tData 0.002 (0.010)\tLoss 0.7630 (0.7532)\tPrec@1 76.562 (76.383)\tPrec@5 98.438 (98.617)\n",
      "TRAINING - Epoch: [6][70/391]\tTime 1.343 (1.392)\tData 0.002 (0.009)\tLoss 0.8653 (0.7510)\tPrec@1 73.438 (76.375)\tPrec@5 97.656 (98.592)\n",
      "TRAINING - Epoch: [6][80/391]\tTime 1.265 (1.388)\tData 0.002 (0.008)\tLoss 0.7985 (0.7476)\tPrec@1 72.656 (76.456)\tPrec@5 97.656 (98.640)\n",
      "TRAINING - Epoch: [6][90/391]\tTime 1.393 (1.391)\tData 0.009 (0.008)\tLoss 0.7074 (0.7455)\tPrec@1 78.125 (76.588)\tPrec@5 99.219 (98.678)\n",
      "TRAINING - Epoch: [6][100/391]\tTime 1.346 (1.392)\tData 0.002 (0.007)\tLoss 0.9870 (0.7449)\tPrec@1 71.094 (76.601)\tPrec@5 97.656 (98.677)\n",
      "TRAINING - Epoch: [6][110/391]\tTime 1.410 (1.391)\tData 0.002 (0.007)\tLoss 0.8420 (0.7441)\tPrec@1 78.906 (76.647)\tPrec@5 98.438 (98.628)\n",
      "TRAINING - Epoch: [6][120/391]\tTime 1.318 (1.389)\tData 0.008 (0.007)\tLoss 0.5888 (0.7431)\tPrec@1 82.031 (76.782)\tPrec@5 99.219 (98.651)\n",
      "TRAINING - Epoch: [6][130/391]\tTime 1.405 (1.391)\tData 0.009 (0.007)\tLoss 0.8490 (0.7407)\tPrec@1 76.562 (76.891)\tPrec@5 96.094 (98.622)\n",
      "TRAINING - Epoch: [6][140/391]\tTime 1.420 (1.393)\tData 0.002 (0.007)\tLoss 0.8089 (0.7385)\tPrec@1 74.219 (76.945)\tPrec@5 99.219 (98.620)\n",
      "TRAINING - Epoch: [6][150/391]\tTime 1.253 (1.388)\tData 0.002 (0.007)\tLoss 0.6779 (0.7352)\tPrec@1 78.906 (77.038)\tPrec@5 100.000 (98.644)\n",
      "TRAINING - Epoch: [6][160/391]\tTime 1.341 (1.386)\tData 0.009 (0.007)\tLoss 0.8011 (0.7364)\tPrec@1 77.344 (77.019)\tPrec@5 98.438 (98.670)\n",
      "TRAINING - Epoch: [6][170/391]\tTime 1.349 (1.387)\tData 0.002 (0.007)\tLoss 0.6772 (0.7403)\tPrec@1 77.344 (76.891)\tPrec@5 98.438 (98.657)\n",
      "TRAINING - Epoch: [6][180/391]\tTime 1.347 (1.386)\tData 0.002 (0.006)\tLoss 0.7478 (0.7401)\tPrec@1 73.438 (76.847)\tPrec@5 100.000 (98.662)\n",
      "TRAINING - Epoch: [6][190/391]\tTime 1.328 (1.384)\tData 0.002 (0.006)\tLoss 0.7523 (0.7391)\tPrec@1 76.562 (76.841)\tPrec@5 99.219 (98.671)\n",
      "TRAINING - Epoch: [6][200/391]\tTime 1.493 (1.387)\tData 0.009 (0.006)\tLoss 0.6642 (0.7410)\tPrec@1 79.688 (76.741)\tPrec@5 99.219 (98.671)\n",
      "TRAINING - Epoch: [6][210/391]\tTime 1.402 (1.387)\tData 0.009 (0.006)\tLoss 0.7581 (0.7417)\tPrec@1 77.344 (76.737)\tPrec@5 100.000 (98.674)\n",
      "TRAINING - Epoch: [6][220/391]\tTime 1.280 (1.387)\tData 0.002 (0.006)\tLoss 0.7999 (0.7409)\tPrec@1 74.219 (76.778)\tPrec@5 98.438 (98.674)\n",
      "TRAINING - Epoch: [6][230/391]\tTime 1.339 (1.386)\tData 0.002 (0.006)\tLoss 0.7207 (0.7391)\tPrec@1 78.125 (76.860)\tPrec@5 98.438 (98.688)\n",
      "TRAINING - Epoch: [6][240/391]\tTime 1.307 (1.386)\tData 0.002 (0.006)\tLoss 0.6837 (0.7391)\tPrec@1 79.688 (76.861)\tPrec@5 99.219 (98.681)\n",
      "TRAINING - Epoch: [6][250/391]\tTime 1.346 (1.386)\tData 0.009 (0.006)\tLoss 0.7178 (0.7384)\tPrec@1 76.562 (76.899)\tPrec@5 100.000 (98.687)\n",
      "TRAINING - Epoch: [6][260/391]\tTime 1.452 (1.386)\tData 0.010 (0.006)\tLoss 0.6450 (0.7381)\tPrec@1 77.344 (76.862)\tPrec@5 100.000 (98.692)\n",
      "TRAINING - Epoch: [6][270/391]\tTime 1.301 (1.387)\tData 0.009 (0.006)\tLoss 0.6433 (0.7381)\tPrec@1 78.125 (76.885)\tPrec@5 99.219 (98.665)\n",
      "TRAINING - Epoch: [6][280/391]\tTime 1.385 (1.386)\tData 0.002 (0.006)\tLoss 0.7626 (0.7385)\tPrec@1 78.125 (76.899)\tPrec@5 96.875 (98.638)\n",
      "TRAINING - Epoch: [6][290/391]\tTime 1.300 (1.385)\tData 0.002 (0.006)\tLoss 0.7318 (0.7390)\tPrec@1 74.219 (76.874)\tPrec@5 99.219 (98.625)\n",
      "TRAINING - Epoch: [6][300/391]\tTime 1.315 (1.384)\tData 0.009 (0.006)\tLoss 0.6553 (0.7385)\tPrec@1 80.469 (76.845)\tPrec@5 100.000 (98.632)\n",
      "TRAINING - Epoch: [6][310/391]\tTime 1.401 (1.385)\tData 0.002 (0.006)\tLoss 0.8068 (0.7389)\tPrec@1 74.219 (76.814)\tPrec@5 99.219 (98.628)\n",
      "TRAINING - Epoch: [6][320/391]\tTime 1.304 (1.384)\tData 0.002 (0.006)\tLoss 0.9137 (0.7394)\tPrec@1 71.875 (76.818)\tPrec@5 99.219 (98.642)\n",
      "TRAINING - Epoch: [6][330/391]\tTime 1.381 (1.385)\tData 0.002 (0.006)\tLoss 0.8975 (0.7397)\tPrec@1 73.438 (76.775)\tPrec@5 95.312 (98.650)\n",
      "TRAINING - Epoch: [6][340/391]\tTime 1.374 (1.385)\tData 0.002 (0.006)\tLoss 0.7844 (0.7390)\tPrec@1 73.438 (76.782)\tPrec@5 99.219 (98.644)\n",
      "TRAINING - Epoch: [6][350/391]\tTime 1.321 (1.385)\tData 0.009 (0.006)\tLoss 0.7048 (0.7395)\tPrec@1 76.562 (76.767)\tPrec@5 98.438 (98.644)\n",
      "TRAINING - Epoch: [6][360/391]\tTime 1.386 (1.384)\tData 0.009 (0.006)\tLoss 0.7457 (0.7389)\tPrec@1 75.781 (76.807)\tPrec@5 99.219 (98.645)\n",
      "TRAINING - Epoch: [6][370/391]\tTime 1.307 (1.384)\tData 0.002 (0.005)\tLoss 0.6461 (0.7370)\tPrec@1 79.688 (76.872)\tPrec@5 100.000 (98.644)\n",
      "TRAINING - Epoch: [6][380/391]\tTime 1.405 (1.384)\tData 0.002 (0.005)\tLoss 0.8674 (0.7366)\tPrec@1 70.312 (76.913)\tPrec@5 98.438 (98.651)\n",
      "TRAINING - Epoch: [6][390/391]\tTime 0.900 (1.382)\tData 0.003 (0.005)\tLoss 0.7628 (0.7361)\tPrec@1 77.500 (76.908)\tPrec@5 98.750 (98.652)\n",
      "EVALUATING - Epoch: [6][0/79]\tTime 1.194 (1.194)\tData 0.433 (0.433)\tLoss 0.7628 (0.7628)\tPrec@1 76.562 (76.562)\tPrec@5 99.219 (99.219)\n",
      "EVALUATING - Epoch: [6][10/79]\tTime 0.581 (0.656)\tData 0.009 (0.045)\tLoss 0.8399 (0.8238)\tPrec@1 76.562 (76.420)\tPrec@5 96.094 (98.438)\n",
      "EVALUATING - Epoch: [6][20/79]\tTime 0.580 (0.641)\tData 0.002 (0.025)\tLoss 0.7656 (0.8368)\tPrec@1 75.781 (75.818)\tPrec@5 97.656 (98.326)\n",
      "EVALUATING - Epoch: [6][30/79]\tTime 0.590 (0.637)\tData 0.002 (0.018)\tLoss 0.7744 (0.8353)\tPrec@1 75.781 (75.504)\tPrec@5 100.000 (98.286)\n",
      "EVALUATING - Epoch: [6][40/79]\tTime 0.568 (0.632)\tData 0.002 (0.014)\tLoss 0.8625 (0.8400)\tPrec@1 75.000 (75.457)\tPrec@5 99.219 (98.285)\n",
      "EVALUATING - Epoch: [6][50/79]\tTime 0.638 (0.626)\tData 0.002 (0.012)\tLoss 0.8698 (0.8449)\tPrec@1 70.312 (75.123)\tPrec@5 98.438 (98.361)\n",
      "EVALUATING - Epoch: [6][60/79]\tTime 0.574 (0.623)\tData 0.002 (0.011)\tLoss 0.8629 (0.8349)\tPrec@1 75.781 (75.179)\tPrec@5 96.875 (98.399)\n",
      "EVALUATING - Epoch: [6][70/79]\tTime 0.619 (0.623)\tData 0.002 (0.010)\tLoss 0.7670 (0.8425)\tPrec@1 77.344 (74.791)\tPrec@5 99.219 (98.415)\n",
      "\n",
      " Epoch: 7\tTraining Loss 0.7361 \tTraining Prec@1 76.908 \tTraining Prec@5 98.652 \tValidation Loss 0.8368 \tValidation Prec@1 74.830 \tValidation Prec@5 98.470 \n",
      "\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "TRAINING - Epoch: [7][0/391]\tTime 2.503 (2.503)\tData 0.538 (0.538)\tLoss 0.7575 (0.7575)\tPrec@1 77.344 (77.344)\tPrec@5 98.438 (98.438)\n",
      "TRAINING - Epoch: [7][10/391]\tTime 1.343 (1.475)\tData 0.001 (0.051)\tLoss 0.6772 (0.6912)\tPrec@1 80.469 (78.267)\tPrec@5 98.438 (98.864)\n",
      "TRAINING - Epoch: [7][20/391]\tTime 1.482 (1.432)\tData 0.002 (0.029)\tLoss 0.8483 (0.7004)\tPrec@1 72.656 (78.423)\tPrec@5 96.875 (98.958)\n",
      "TRAINING - Epoch: [7][30/391]\tTime 1.426 (1.411)\tData 0.002 (0.021)\tLoss 0.9911 (0.6933)\tPrec@1 71.094 (78.831)\tPrec@5 98.438 (99.042)\n",
      "TRAINING - Epoch: [7][40/391]\tTime 1.364 (1.409)\tData 0.003 (0.017)\tLoss 0.8165 (0.6884)\tPrec@1 76.562 (78.944)\tPrec@5 96.875 (99.028)\n",
      "TRAINING - Epoch: [7][50/391]\tTime 1.353 (1.408)\tData 0.002 (0.014)\tLoss 0.6273 (0.6930)\tPrec@1 78.906 (78.722)\tPrec@5 100.000 (98.974)\n",
      "TRAINING - Epoch: [7][60/391]\tTime 1.422 (1.402)\tData 0.002 (0.012)\tLoss 0.9121 (0.7096)\tPrec@1 74.219 (78.279)\tPrec@5 96.875 (98.886)\n",
      "TRAINING - Epoch: [7][70/391]\tTime 1.311 (1.398)\tData 0.002 (0.011)\tLoss 0.8462 (0.7178)\tPrec@1 76.562 (78.103)\tPrec@5 97.656 (98.834)\n",
      "TRAINING - Epoch: [7][80/391]\tTime 1.298 (1.390)\tData 0.002 (0.010)\tLoss 0.6616 (0.7158)\tPrec@1 79.688 (78.183)\tPrec@5 100.000 (98.814)\n",
      "TRAINING - Epoch: [7][90/391]\tTime 1.403 (1.390)\tData 0.002 (0.009)\tLoss 0.7227 (0.7146)\tPrec@1 78.125 (78.262)\tPrec@5 97.656 (98.789)\n",
      "TRAINING - Epoch: [7][100/391]\tTime 1.312 (1.389)\tData 0.002 (0.009)\tLoss 0.7821 (0.7155)\tPrec@1 74.219 (78.241)\tPrec@5 99.219 (98.793)\n",
      "TRAINING - Epoch: [7][110/391]\tTime 1.432 (1.388)\tData 0.002 (0.008)\tLoss 0.7962 (0.7124)\tPrec@1 73.438 (78.301)\tPrec@5 98.438 (98.796)\n",
      "TRAINING - Epoch: [7][120/391]\tTime 1.400 (1.387)\tData 0.002 (0.008)\tLoss 0.5346 (0.7066)\tPrec@1 80.469 (78.512)\tPrec@5 99.219 (98.812)\n",
      "TRAINING - Epoch: [7][130/391]\tTime 1.369 (1.386)\tData 0.011 (0.007)\tLoss 0.6120 (0.7070)\tPrec@1 79.688 (78.507)\tPrec@5 98.438 (98.801)\n",
      "TRAINING - Epoch: [7][140/391]\tTime 1.367 (1.384)\tData 0.002 (0.007)\tLoss 0.8106 (0.7119)\tPrec@1 76.562 (78.313)\tPrec@5 99.219 (98.781)\n",
      "TRAINING - Epoch: [7][150/391]\tTime 1.303 (1.382)\tData 0.009 (0.007)\tLoss 0.7232 (0.7131)\tPrec@1 76.562 (78.270)\tPrec@5 100.000 (98.805)\n",
      "TRAINING - Epoch: [7][160/391]\tTime 1.369 (1.382)\tData 0.002 (0.007)\tLoss 0.6827 (0.7129)\tPrec@1 79.688 (78.266)\tPrec@5 100.000 (98.801)\n",
      "TRAINING - Epoch: [7][170/391]\tTime 1.391 (1.381)\tData 0.002 (0.007)\tLoss 0.6381 (0.7087)\tPrec@1 82.812 (78.417)\tPrec@5 98.438 (98.826)\n",
      "TRAINING - Epoch: [7][180/391]\tTime 1.403 (1.382)\tData 0.009 (0.007)\tLoss 0.7655 (0.7086)\tPrec@1 78.906 (78.431)\tPrec@5 97.656 (98.813)\n",
      "TRAINING - Epoch: [7][190/391]\tTime 1.358 (1.382)\tData 0.002 (0.006)\tLoss 0.7081 (0.7095)\tPrec@1 76.562 (78.362)\tPrec@5 100.000 (98.789)\n",
      "TRAINING - Epoch: [7][200/391]\tTime 1.512 (1.383)\tData 0.002 (0.006)\tLoss 0.6104 (0.7080)\tPrec@1 83.594 (78.378)\tPrec@5 99.219 (98.783)\n",
      "TRAINING - Epoch: [7][210/391]\tTime 1.530 (1.383)\tData 0.009 (0.006)\tLoss 0.6833 (0.7058)\tPrec@1 77.344 (78.436)\tPrec@5 99.219 (98.789)\n",
      "TRAINING - Epoch: [7][220/391]\tTime 1.325 (1.381)\tData 0.009 (0.006)\tLoss 0.8263 (0.7046)\tPrec@1 73.438 (78.443)\tPrec@5 99.219 (98.812)\n",
      "TRAINING - Epoch: [7][230/391]\tTime 1.355 (1.381)\tData 0.002 (0.006)\tLoss 0.5134 (0.7016)\tPrec@1 83.594 (78.582)\tPrec@5 100.000 (98.833)\n",
      "TRAINING - Epoch: [7][240/391]\tTime 1.503 (1.382)\tData 0.009 (0.006)\tLoss 0.8352 (0.7047)\tPrec@1 71.875 (78.472)\tPrec@5 96.875 (98.804)\n",
      "TRAINING - Epoch: [7][250/391]\tTime 1.375 (1.382)\tData 0.002 (0.006)\tLoss 0.6304 (0.7030)\tPrec@1 78.125 (78.511)\tPrec@5 98.438 (98.783)\n",
      "TRAINING - Epoch: [7][260/391]\tTime 1.376 (1.382)\tData 0.002 (0.006)\tLoss 0.6702 (0.7017)\tPrec@1 80.469 (78.568)\tPrec@5 98.438 (98.773)\n",
      "TRAINING - Epoch: [7][270/391]\tTime 1.312 (1.380)\tData 0.002 (0.006)\tLoss 0.8504 (0.7013)\tPrec@1 71.094 (78.606)\tPrec@5 96.094 (98.752)\n",
      "TRAINING - Epoch: [7][280/391]\tTime 1.309 (1.381)\tData 0.009 (0.006)\tLoss 0.7389 (0.7005)\tPrec@1 72.656 (78.600)\tPrec@5 97.656 (98.732)\n",
      "TRAINING - Epoch: [7][290/391]\tTime 1.346 (1.381)\tData 0.009 (0.006)\tLoss 0.7494 (0.7008)\tPrec@1 77.344 (78.606)\tPrec@5 99.219 (98.722)\n",
      "TRAINING - Epoch: [7][300/391]\tTime 1.300 (1.381)\tData 0.009 (0.006)\tLoss 0.7150 (0.7000)\tPrec@1 77.344 (78.660)\tPrec@5 99.219 (98.700)\n",
      "TRAINING - Epoch: [7][310/391]\tTime 1.364 (1.382)\tData 0.009 (0.006)\tLoss 0.6919 (0.6994)\tPrec@1 79.688 (78.640)\tPrec@5 98.438 (98.709)\n",
      "TRAINING - Epoch: [7][320/391]\tTime 1.363 (1.381)\tData 0.002 (0.006)\tLoss 0.7409 (0.6993)\tPrec@1 77.344 (78.617)\tPrec@5 100.000 (98.730)\n",
      "TRAINING - Epoch: [7][330/391]\tTime 1.374 (1.380)\tData 0.002 (0.006)\tLoss 0.6903 (0.6987)\tPrec@1 74.219 (78.654)\tPrec@5 99.219 (98.730)\n",
      "TRAINING - Epoch: [7][340/391]\tTime 1.333 (1.380)\tData 0.002 (0.006)\tLoss 0.6783 (0.6994)\tPrec@1 74.219 (78.588)\tPrec@5 100.000 (98.745)\n",
      "TRAINING - Epoch: [7][350/391]\tTime 1.330 (1.380)\tData 0.014 (0.006)\tLoss 0.6410 (0.6991)\tPrec@1 78.125 (78.597)\tPrec@5 100.000 (98.751)\n",
      "TRAINING - Epoch: [7][360/391]\tTime 1.407 (1.381)\tData 0.009 (0.006)\tLoss 0.5371 (0.6986)\tPrec@1 85.938 (78.625)\tPrec@5 99.219 (98.760)\n",
      "TRAINING - Epoch: [7][370/391]\tTime 1.280 (1.380)\tData 0.002 (0.006)\tLoss 0.6170 (0.6981)\tPrec@1 85.938 (78.637)\tPrec@5 98.438 (98.764)\n",
      "TRAINING - Epoch: [7][380/391]\tTime 1.353 (1.380)\tData 0.002 (0.006)\tLoss 0.6739 (0.6984)\tPrec@1 80.469 (78.615)\tPrec@5 98.438 (98.755)\n",
      "TRAINING - Epoch: [7][390/391]\tTime 0.843 (1.377)\tData 0.002 (0.006)\tLoss 0.6342 (0.6969)\tPrec@1 83.750 (78.674)\tPrec@5 98.750 (98.752)\n",
      "EVALUATING - Epoch: [7][0/79]\tTime 1.237 (1.237)\tData 0.316 (0.316)\tLoss 0.7276 (0.7276)\tPrec@1 77.344 (77.344)\tPrec@5 98.438 (98.438)\n",
      "EVALUATING - Epoch: [7][10/79]\tTime 0.678 (0.721)\tData 0.002 (0.034)\tLoss 0.6908 (0.7607)\tPrec@1 75.781 (77.131)\tPrec@5 98.438 (98.580)\n",
      "EVALUATING - Epoch: [7][20/79]\tTime 0.592 (0.677)\tData 0.002 (0.020)\tLoss 0.7457 (0.7917)\tPrec@1 75.000 (76.079)\tPrec@5 97.656 (98.438)\n",
      "EVALUATING - Epoch: [7][30/79]\tTime 0.599 (0.653)\tData 0.002 (0.014)\tLoss 0.7192 (0.7889)\tPrec@1 77.344 (76.159)\tPrec@5 100.000 (98.488)\n",
      "EVALUATING - Epoch: [7][40/79]\tTime 0.544 (0.643)\tData 0.002 (0.012)\tLoss 0.7881 (0.7778)\tPrec@1 76.562 (76.582)\tPrec@5 99.219 (98.457)\n",
      "EVALUATING - Epoch: [7][50/79]\tTime 0.570 (0.641)\tData 0.002 (0.010)\tLoss 0.8526 (0.7771)\tPrec@1 78.125 (76.823)\tPrec@5 96.094 (98.483)\n",
      "EVALUATING - Epoch: [7][60/79]\tTime 0.592 (0.638)\tData 0.002 (0.009)\tLoss 0.8730 (0.7761)\tPrec@1 71.875 (76.870)\tPrec@5 97.656 (98.591)\n",
      "EVALUATING - Epoch: [7][70/79]\tTime 0.663 (0.633)\tData 0.002 (0.009)\tLoss 0.6939 (0.7804)\tPrec@1 82.812 (76.673)\tPrec@5 99.219 (98.647)\n",
      "\n",
      " Epoch: 8\tTraining Loss 0.6969 \tTraining Prec@1 78.674 \tTraining Prec@5 98.752 \tValidation Loss 0.7776 \tValidation Prec@1 76.680 \tValidation Prec@5 98.640 \n",
      "\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "TRAINING - Epoch: [8][0/391]\tTime 2.395 (2.395)\tData 0.457 (0.457)\tLoss 0.6981 (0.6981)\tPrec@1 78.906 (78.906)\tPrec@5 99.219 (99.219)\n",
      "TRAINING - Epoch: [8][10/391]\tTime 1.427 (1.504)\tData 0.009 (0.046)\tLoss 0.8113 (0.7011)\tPrec@1 74.219 (78.125)\tPrec@5 99.219 (98.509)\n",
      "TRAINING - Epoch: [8][20/391]\tTime 1.377 (1.447)\tData 0.002 (0.026)\tLoss 0.6711 (0.6870)\tPrec@1 80.469 (79.018)\tPrec@5 98.438 (98.475)\n",
      "TRAINING - Epoch: [8][30/391]\tTime 1.378 (1.415)\tData 0.002 (0.019)\tLoss 0.6600 (0.6734)\tPrec@1 78.125 (79.057)\tPrec@5 98.438 (98.614)\n",
      "TRAINING - Epoch: [8][40/391]\tTime 1.587 (1.411)\tData 0.002 (0.016)\tLoss 0.7813 (0.6741)\tPrec@1 73.438 (79.249)\tPrec@5 97.656 (98.628)\n",
      "TRAINING - Epoch: [8][50/391]\tTime 1.336 (1.408)\tData 0.009 (0.013)\tLoss 0.6987 (0.6651)\tPrec@1 79.688 (79.596)\tPrec@5 97.656 (98.759)\n",
      "TRAINING - Epoch: [8][60/391]\tTime 1.299 (1.403)\tData 0.002 (0.012)\tLoss 0.6252 (0.6668)\tPrec@1 79.688 (79.534)\tPrec@5 99.219 (98.758)\n",
      "TRAINING - Epoch: [8][70/391]\tTime 1.378 (1.404)\tData 0.009 (0.011)\tLoss 0.5065 (0.6677)\tPrec@1 85.156 (79.467)\tPrec@5 100.000 (98.834)\n",
      "TRAINING - Epoch: [8][80/391]\tTime 1.294 (1.398)\tData 0.002 (0.010)\tLoss 0.7575 (0.6710)\tPrec@1 77.344 (79.446)\tPrec@5 98.438 (98.852)\n",
      "TRAINING - Epoch: [8][90/391]\tTime 1.348 (1.398)\tData 0.002 (0.010)\tLoss 0.6076 (0.6681)\tPrec@1 82.031 (79.447)\tPrec@5 99.219 (98.884)\n",
      "TRAINING - Epoch: [8][100/391]\tTime 1.383 (1.397)\tData 0.002 (0.009)\tLoss 0.6370 (0.6637)\tPrec@1 79.688 (79.571)\tPrec@5 99.219 (98.902)\n",
      "TRAINING - Epoch: [8][110/391]\tTime 1.419 (1.398)\tData 0.002 (0.009)\tLoss 0.7603 (0.6631)\tPrec@1 78.906 (79.652)\tPrec@5 98.438 (98.909)\n",
      "TRAINING - Epoch: [8][120/391]\tTime 1.365 (1.396)\tData 0.009 (0.008)\tLoss 0.7817 (0.6713)\tPrec@1 77.344 (79.332)\tPrec@5 97.656 (98.864)\n",
      "TRAINING - Epoch: [8][130/391]\tTime 1.356 (1.395)\tData 0.002 (0.008)\tLoss 0.7905 (0.6682)\tPrec@1 75.000 (79.461)\tPrec@5 99.219 (98.879)\n",
      "TRAINING - Epoch: [8][140/391]\tTime 1.420 (1.394)\tData 0.002 (0.008)\tLoss 0.7140 (0.6698)\tPrec@1 78.906 (79.516)\tPrec@5 99.219 (98.903)\n",
      "TRAINING - Epoch: [8][150/391]\tTime 1.256 (1.390)\tData 0.002 (0.007)\tLoss 0.6928 (0.6730)\tPrec@1 77.344 (79.382)\tPrec@5 99.219 (98.898)\n",
      "TRAINING - Epoch: [8][160/391]\tTime 1.367 (1.388)\tData 0.009 (0.007)\tLoss 0.6300 (0.6744)\tPrec@1 82.812 (79.391)\tPrec@5 97.656 (98.850)\n",
      "TRAINING - Epoch: [8][170/391]\tTime 1.296 (1.387)\tData 0.009 (0.007)\tLoss 0.5284 (0.6720)\tPrec@1 84.375 (79.482)\tPrec@5 100.000 (98.853)\n",
      "TRAINING - Epoch: [8][180/391]\tTime 1.364 (1.388)\tData 0.002 (0.007)\tLoss 0.7642 (0.6681)\tPrec@1 78.125 (79.649)\tPrec@5 98.438 (98.882)\n",
      "TRAINING - Epoch: [8][190/391]\tTime 1.522 (1.389)\tData 0.002 (0.007)\tLoss 0.6923 (0.6670)\tPrec@1 79.688 (79.712)\tPrec@5 98.438 (98.896)\n",
      "TRAINING - Epoch: [8][200/391]\tTime 1.441 (1.389)\tData 0.009 (0.007)\tLoss 0.5946 (0.6671)\tPrec@1 81.250 (79.722)\tPrec@5 100.000 (98.896)\n",
      "TRAINING - Epoch: [8][210/391]\tTime 1.260 (1.388)\tData 0.002 (0.007)\tLoss 0.8025 (0.6684)\tPrec@1 75.781 (79.632)\tPrec@5 95.312 (98.900)\n",
      "TRAINING - Epoch: [8][220/391]\tTime 1.523 (1.389)\tData 0.009 (0.007)\tLoss 0.6819 (0.6679)\tPrec@1 75.000 (79.599)\tPrec@5 99.219 (98.911)\n",
      "TRAINING - Epoch: [8][230/391]\tTime 1.321 (1.389)\tData 0.002 (0.007)\tLoss 0.6988 (0.6684)\tPrec@1 78.906 (79.573)\tPrec@5 97.656 (98.911)\n",
      "TRAINING - Epoch: [8][240/391]\tTime 1.356 (1.388)\tData 0.002 (0.006)\tLoss 0.5853 (0.6694)\tPrec@1 84.375 (79.542)\tPrec@5 99.219 (98.911)\n",
      "TRAINING - Epoch: [8][250/391]\tTime 1.388 (1.388)\tData 0.010 (0.006)\tLoss 0.6832 (0.6673)\tPrec@1 78.906 (79.582)\tPrec@5 96.094 (98.907)\n",
      "TRAINING - Epoch: [8][260/391]\tTime 1.442 (1.388)\tData 0.020 (0.006)\tLoss 0.6699 (0.6673)\tPrec@1 78.906 (79.550)\tPrec@5 100.000 (98.925)\n",
      "TRAINING - Epoch: [8][270/391]\tTime 1.413 (1.388)\tData 0.005 (0.006)\tLoss 0.8133 (0.6655)\tPrec@1 75.000 (79.601)\tPrec@5 98.438 (98.939)\n",
      "TRAINING - Epoch: [8][280/391]\tTime 1.375 (1.387)\tData 0.002 (0.006)\tLoss 0.6559 (0.6658)\tPrec@1 78.125 (79.571)\tPrec@5 99.219 (98.949)\n",
      "TRAINING - Epoch: [8][290/391]\tTime 1.310 (1.387)\tData 0.002 (0.006)\tLoss 0.5458 (0.6649)\tPrec@1 84.375 (79.636)\tPrec@5 100.000 (98.940)\n",
      "TRAINING - Epoch: [8][300/391]\tTime 1.339 (1.385)\tData 0.002 (0.006)\tLoss 0.7654 (0.6642)\tPrec@1 77.344 (79.667)\tPrec@5 97.656 (98.923)\n",
      "TRAINING - Epoch: [8][310/391]\tTime 1.272 (1.385)\tData 0.002 (0.006)\tLoss 0.5610 (0.6644)\tPrec@1 81.250 (79.652)\tPrec@5 99.219 (98.927)\n",
      "TRAINING - Epoch: [8][320/391]\tTime 1.391 (1.386)\tData 0.004 (0.006)\tLoss 0.7590 (0.6649)\tPrec@1 76.562 (79.614)\tPrec@5 99.219 (98.907)\n",
      "TRAINING - Epoch: [8][330/391]\tTime 1.295 (1.385)\tData 0.002 (0.006)\tLoss 0.5619 (0.6656)\tPrec@1 78.906 (79.584)\tPrec@5 100.000 (98.917)\n",
      "TRAINING - Epoch: [8][340/391]\tTime 1.319 (1.385)\tData 0.009 (0.006)\tLoss 0.5423 (0.6637)\tPrec@1 83.594 (79.649)\tPrec@5 99.219 (98.907)\n",
      "TRAINING - Epoch: [8][350/391]\tTime 1.304 (1.384)\tData 0.002 (0.006)\tLoss 0.6687 (0.6639)\tPrec@1 80.469 (79.667)\tPrec@5 98.438 (98.889)\n",
      "TRAINING - Epoch: [8][360/391]\tTime 1.285 (1.384)\tData 0.009 (0.006)\tLoss 0.5241 (0.6631)\tPrec@1 85.938 (79.713)\tPrec@5 98.438 (98.892)\n",
      "TRAINING - Epoch: [8][370/391]\tTime 1.298 (1.383)\tData 0.009 (0.006)\tLoss 0.6485 (0.6637)\tPrec@1 82.812 (79.713)\tPrec@5 98.438 (98.892)\n",
      "TRAINING - Epoch: [8][380/391]\tTime 1.290 (1.382)\tData 0.002 (0.006)\tLoss 0.6954 (0.6633)\tPrec@1 79.688 (79.716)\tPrec@5 97.656 (98.891)\n",
      "TRAINING - Epoch: [8][390/391]\tTime 0.942 (1.380)\tData 0.002 (0.006)\tLoss 0.8720 (0.6646)\tPrec@1 71.250 (79.670)\tPrec@5 98.750 (98.888)\n",
      "EVALUATING - Epoch: [8][0/79]\tTime 1.236 (1.236)\tData 0.331 (0.331)\tLoss 0.5332 (0.5332)\tPrec@1 82.031 (82.031)\tPrec@5 100.000 (100.000)\n",
      "EVALUATING - Epoch: [8][10/79]\tTime 0.571 (0.659)\tData 0.001 (0.033)\tLoss 0.6270 (0.6537)\tPrec@1 81.250 (79.972)\tPrec@5 99.219 (99.148)\n",
      "EVALUATING - Epoch: [8][20/79]\tTime 0.570 (0.630)\tData 0.002 (0.019)\tLoss 0.7458 (0.7093)\tPrec@1 76.562 (78.534)\tPrec@5 99.219 (98.958)\n",
      "EVALUATING - Epoch: [8][30/79]\tTime 0.585 (0.624)\tData 0.002 (0.014)\tLoss 0.5821 (0.7107)\tPrec@1 80.469 (78.301)\tPrec@5 99.219 (98.841)\n",
      "EVALUATING - Epoch: [8][40/79]\tTime 0.618 (0.621)\tData 0.002 (0.011)\tLoss 0.7381 (0.7024)\tPrec@1 75.000 (78.773)\tPrec@5 99.219 (98.819)\n",
      "EVALUATING - Epoch: [8][50/79]\tTime 0.583 (0.619)\tData 0.002 (0.010)\tLoss 0.6818 (0.6973)\tPrec@1 83.594 (79.013)\tPrec@5 98.438 (98.958)\n",
      "EVALUATING - Epoch: [8][60/79]\tTime 0.651 (0.616)\tData 0.002 (0.009)\tLoss 0.6573 (0.6942)\tPrec@1 78.906 (79.060)\tPrec@5 99.219 (99.001)\n",
      "EVALUATING - Epoch: [8][70/79]\tTime 0.536 (0.608)\tData 0.002 (0.008)\tLoss 0.6774 (0.6953)\tPrec@1 78.125 (78.972)\tPrec@5 100.000 (98.999)\n",
      "\n",
      " Epoch: 9\tTraining Loss 0.6646 \tTraining Prec@1 79.670 \tTraining Prec@5 98.888 \tValidation Loss 0.6893 \tValidation Prec@1 79.140 \tValidation Prec@5 99.010 \n",
      "\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "TRAINING - Epoch: [9][0/391]\tTime 2.363 (2.363)\tData 0.457 (0.457)\tLoss 0.4922 (0.4922)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "TRAINING - Epoch: [9][10/391]\tTime 1.247 (1.417)\tData 0.001 (0.046)\tLoss 0.6716 (0.6336)\tPrec@1 75.781 (79.545)\tPrec@5 98.438 (98.935)\n",
      "TRAINING - Epoch: [9][20/391]\tTime 1.290 (1.377)\tData 0.009 (0.025)\tLoss 0.8116 (0.6470)\tPrec@1 74.219 (80.022)\tPrec@5 97.656 (98.772)\n",
      "TRAINING - Epoch: [9][30/391]\tTime 1.319 (1.365)\tData 0.002 (0.018)\tLoss 0.7266 (0.6495)\tPrec@1 75.000 (79.965)\tPrec@5 97.656 (98.816)\n",
      "TRAINING - Epoch: [9][40/391]\tTime 1.344 (1.361)\tData 0.002 (0.015)\tLoss 0.6027 (0.6524)\tPrec@1 82.812 (79.916)\tPrec@5 98.438 (98.952)\n",
      "TRAINING - Epoch: [9][50/391]\tTime 1.361 (1.360)\tData 0.002 (0.012)\tLoss 0.5917 (0.6521)\tPrec@1 81.250 (80.086)\tPrec@5 100.000 (99.066)\n",
      "TRAINING - Epoch: [9][60/391]\tTime 1.372 (1.362)\tData 0.002 (0.011)\tLoss 0.5654 (0.6521)\tPrec@1 81.250 (80.097)\tPrec@5 98.438 (99.027)\n",
      "TRAINING - Epoch: [9][70/391]\tTime 1.396 (1.371)\tData 0.002 (0.010)\tLoss 0.5410 (0.6471)\tPrec@1 85.156 (80.260)\tPrec@5 98.438 (98.999)\n",
      "TRAINING - Epoch: [9][80/391]\tTime 1.380 (1.375)\tData 0.002 (0.009)\tLoss 0.6584 (0.6441)\tPrec@1 80.469 (80.285)\tPrec@5 98.438 (99.026)\n",
      "TRAINING - Epoch: [9][90/391]\tTime 1.519 (1.375)\tData 0.003 (0.008)\tLoss 0.7570 (0.6422)\tPrec@1 78.906 (80.374)\tPrec@5 98.438 (99.056)\n",
      "TRAINING - Epoch: [9][100/391]\tTime 1.274 (1.376)\tData 0.003 (0.008)\tLoss 0.7443 (0.6470)\tPrec@1 75.781 (80.167)\tPrec@5 98.438 (98.994)\n",
      "TRAINING - Epoch: [9][110/391]\tTime 1.428 (1.376)\tData 0.009 (0.008)\tLoss 0.6605 (0.6433)\tPrec@1 80.469 (80.328)\tPrec@5 99.219 (98.979)\n",
      "TRAINING - Epoch: [9][120/391]\tTime 1.474 (1.376)\tData 0.009 (0.007)\tLoss 0.5965 (0.6429)\tPrec@1 81.250 (80.223)\tPrec@5 100.000 (99.019)\n",
      "TRAINING - Epoch: [9][130/391]\tTime 1.368 (1.378)\tData 0.002 (0.007)\tLoss 0.7363 (0.6449)\tPrec@1 76.562 (80.135)\tPrec@5 99.219 (99.004)\n",
      "TRAINING - Epoch: [9][140/391]\tTime 1.257 (1.374)\tData 0.009 (0.007)\tLoss 0.5133 (0.6407)\tPrec@1 86.719 (80.330)\tPrec@5 100.000 (99.008)\n",
      "TRAINING - Epoch: [9][150/391]\tTime 1.429 (1.376)\tData 0.002 (0.007)\tLoss 0.6350 (0.6394)\tPrec@1 81.250 (80.427)\tPrec@5 100.000 (99.012)\n",
      "TRAINING - Epoch: [9][160/391]\tTime 1.405 (1.377)\tData 0.002 (0.007)\tLoss 0.5575 (0.6392)\tPrec@1 85.156 (80.406)\tPrec@5 100.000 (99.020)\n",
      "TRAINING - Epoch: [9][170/391]\tTime 1.289 (1.377)\tData 0.002 (0.007)\tLoss 0.6523 (0.6412)\tPrec@1 79.688 (80.327)\tPrec@5 100.000 (99.022)\n",
      "TRAINING - Epoch: [9][180/391]\tTime 1.309 (1.375)\tData 0.002 (0.007)\tLoss 0.5350 (0.6400)\tPrec@1 82.031 (80.348)\tPrec@5 99.219 (99.037)\n",
      "TRAINING - Epoch: [9][190/391]\tTime 1.463 (1.375)\tData 0.002 (0.007)\tLoss 0.7027 (0.6402)\tPrec@1 80.469 (80.342)\tPrec@5 99.219 (99.051)\n",
      "TRAINING - Epoch: [9][200/391]\tTime 1.413 (1.375)\tData 0.009 (0.007)\tLoss 0.5795 (0.6395)\tPrec@1 83.594 (80.445)\tPrec@5 99.219 (99.063)\n",
      "TRAINING - Epoch: [9][210/391]\tTime 1.263 (1.372)\tData 0.002 (0.006)\tLoss 0.6004 (0.6398)\tPrec@1 83.594 (80.480)\tPrec@5 98.438 (99.052)\n",
      "TRAINING - Epoch: [9][220/391]\tTime 1.389 (1.372)\tData 0.002 (0.006)\tLoss 0.5653 (0.6401)\tPrec@1 84.375 (80.522)\tPrec@5 99.219 (99.049)\n",
      "TRAINING - Epoch: [9][230/391]\tTime 1.340 (1.371)\tData 0.002 (0.006)\tLoss 0.6358 (0.6413)\tPrec@1 82.812 (80.492)\tPrec@5 99.219 (99.043)\n",
      "TRAINING - Epoch: [9][240/391]\tTime 1.350 (1.372)\tData 0.002 (0.006)\tLoss 0.5702 (0.6414)\tPrec@1 85.938 (80.543)\tPrec@5 100.000 (99.040)\n",
      "TRAINING - Epoch: [9][250/391]\tTime 1.413 (1.372)\tData 0.009 (0.006)\tLoss 0.6320 (0.6414)\tPrec@1 78.906 (80.581)\tPrec@5 98.438 (99.023)\n",
      "TRAINING - Epoch: [9][260/391]\tTime 1.447 (1.372)\tData 0.002 (0.006)\tLoss 0.6062 (0.6402)\tPrec@1 82.812 (80.594)\tPrec@5 98.438 (99.027)\n",
      "TRAINING - Epoch: [9][270/391]\tTime 1.314 (1.371)\tData 0.002 (0.006)\tLoss 0.5539 (0.6408)\tPrec@1 81.250 (80.610)\tPrec@5 100.000 (99.023)\n",
      "TRAINING - Epoch: [9][280/391]\tTime 1.541 (1.370)\tData 0.002 (0.006)\tLoss 0.6903 (0.6409)\tPrec@1 80.469 (80.630)\tPrec@5 97.656 (99.002)\n",
      "TRAINING - Epoch: [9][290/391]\tTime 1.358 (1.370)\tData 0.002 (0.006)\tLoss 0.4922 (0.6403)\tPrec@1 83.594 (80.633)\tPrec@5 99.219 (98.982)\n",
      "TRAINING - Epoch: [9][300/391]\tTime 1.362 (1.370)\tData 0.002 (0.005)\tLoss 0.6519 (0.6418)\tPrec@1 78.906 (80.578)\tPrec@5 98.438 (98.988)\n",
      "TRAINING - Epoch: [9][310/391]\tTime 1.378 (1.370)\tData 0.002 (0.005)\tLoss 0.8213 (0.6426)\tPrec@1 75.000 (80.549)\tPrec@5 98.438 (98.985)\n",
      "TRAINING - Epoch: [9][320/391]\tTime 1.379 (1.369)\tData 0.002 (0.005)\tLoss 0.6306 (0.6453)\tPrec@1 82.031 (80.474)\tPrec@5 99.219 (98.975)\n",
      "TRAINING - Epoch: [9][330/391]\tTime 1.343 (1.369)\tData 0.002 (0.005)\tLoss 0.6616 (0.6452)\tPrec@1 79.688 (80.483)\tPrec@5 100.000 (98.987)\n",
      "TRAINING - Epoch: [9][340/391]\tTime 1.352 (1.368)\tData 0.002 (0.005)\tLoss 0.7525 (0.6457)\tPrec@1 78.906 (80.460)\tPrec@5 98.438 (98.992)\n",
      "TRAINING - Epoch: [9][350/391]\tTime 1.316 (1.368)\tData 0.005 (0.005)\tLoss 0.7210 (0.6470)\tPrec@1 80.469 (80.424)\tPrec@5 97.656 (98.981)\n",
      "TRAINING - Epoch: [9][360/391]\tTime 1.294 (1.368)\tData 0.011 (0.005)\tLoss 0.5306 (0.6469)\tPrec@1 87.500 (80.410)\tPrec@5 100.000 (98.989)\n",
      "TRAINING - Epoch: [9][370/391]\tTime 1.388 (1.368)\tData 0.009 (0.005)\tLoss 0.5832 (0.6455)\tPrec@1 82.812 (80.433)\tPrec@5 99.219 (98.996)\n",
      "TRAINING - Epoch: [9][380/391]\tTime 1.353 (1.368)\tData 0.002 (0.005)\tLoss 0.7243 (0.6447)\tPrec@1 78.906 (80.458)\tPrec@5 98.438 (98.997)\n",
      "TRAINING - Epoch: [9][390/391]\tTime 0.868 (1.366)\tData 0.002 (0.005)\tLoss 0.5040 (0.6442)\tPrec@1 86.250 (80.492)\tPrec@5 100.000 (98.982)\n",
      "EVALUATING - Epoch: [9][0/79]\tTime 1.183 (1.183)\tData 0.319 (0.319)\tLoss 0.5154 (0.5154)\tPrec@1 83.594 (83.594)\tPrec@5 100.000 (100.000)\n",
      "EVALUATING - Epoch: [9][10/79]\tTime 0.571 (0.653)\tData 0.001 (0.032)\tLoss 0.6318 (0.6282)\tPrec@1 78.906 (80.682)\tPrec@5 98.438 (99.290)\n",
      "EVALUATING - Epoch: [9][20/79]\tTime 0.540 (0.625)\tData 0.002 (0.019)\tLoss 0.6064 (0.6557)\tPrec@1 78.125 (80.357)\tPrec@5 100.000 (98.996)\n",
      "EVALUATING - Epoch: [9][30/79]\tTime 0.586 (0.614)\tData 0.002 (0.014)\tLoss 0.5247 (0.6536)\tPrec@1 82.812 (80.544)\tPrec@5 100.000 (98.866)\n",
      "EVALUATING - Epoch: [9][40/79]\tTime 0.640 (0.608)\tData 0.002 (0.011)\tLoss 0.6003 (0.6470)\tPrec@1 82.031 (80.926)\tPrec@5 99.219 (98.857)\n",
      "EVALUATING - Epoch: [9][50/79]\tTime 0.768 (0.607)\tData 0.009 (0.009)\tLoss 0.6359 (0.6448)\tPrec@1 82.031 (81.036)\tPrec@5 96.875 (98.912)\n",
      "EVALUATING - Epoch: [9][60/79]\tTime 0.648 (0.606)\tData 0.002 (0.008)\tLoss 0.7241 (0.6419)\tPrec@1 80.469 (81.058)\tPrec@5 97.656 (98.963)\n",
      "EVALUATING - Epoch: [9][70/79]\tTime 0.609 (0.608)\tData 0.002 (0.008)\tLoss 0.6206 (0.6446)\tPrec@1 83.594 (81.019)\tPrec@5 100.000 (98.999)\n",
      "\n",
      " Epoch: 10\tTraining Loss 0.6442 \tTraining Prec@1 80.492 \tTraining Prec@5 98.982 \tValidation Loss 0.6453 \tValidation Prec@1 80.950 \tValidation Prec@5 99.000 \n",
      "\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "TRAINING - Epoch: [10][0/391]\tTime 2.505 (2.505)\tData 0.370 (0.370)\tLoss 0.5884 (0.5884)\tPrec@1 82.031 (82.031)\tPrec@5 100.000 (100.000)\n",
      "TRAINING - Epoch: [10][10/391]\tTime 1.271 (1.470)\tData 0.009 (0.038)\tLoss 0.6727 (0.5833)\tPrec@1 78.125 (81.605)\tPrec@5 96.875 (99.077)\n",
      "TRAINING - Epoch: [10][20/391]\tTime 1.333 (1.424)\tData 0.002 (0.023)\tLoss 0.5438 (0.5865)\tPrec@1 85.156 (82.143)\tPrec@5 98.438 (99.033)\n",
      "TRAINING - Epoch: [10][30/391]\tTime 1.336 (1.407)\tData 0.002 (0.016)\tLoss 0.5194 (0.5820)\tPrec@1 86.719 (82.258)\tPrec@5 100.000 (99.143)\n",
      "TRAINING - Epoch: [10][40/391]\tTime 1.271 (1.398)\tData 0.002 (0.013)\tLoss 0.7018 (0.5948)\tPrec@1 78.906 (81.726)\tPrec@5 99.219 (99.181)\n",
      "TRAINING - Epoch: [10][50/391]\tTime 1.389 (1.393)\tData 0.002 (0.011)\tLoss 0.6214 (0.5956)\tPrec@1 85.156 (82.077)\tPrec@5 98.438 (99.127)\n",
      "TRAINING - Epoch: [10][60/391]\tTime 1.363 (1.389)\tData 0.002 (0.010)\tLoss 0.5454 (0.5974)\tPrec@1 86.719 (82.134)\tPrec@5 99.219 (99.168)\n",
      "TRAINING - Epoch: [10][70/391]\tTime 1.298 (1.386)\tData 0.002 (0.009)\tLoss 0.6604 (0.6050)\tPrec@1 78.906 (81.910)\tPrec@5 99.219 (99.197)\n",
      "TRAINING - Epoch: [10][80/391]\tTime 1.294 (1.387)\tData 0.009 (0.008)\tLoss 0.7318 (0.6134)\tPrec@1 78.125 (81.617)\tPrec@5 97.656 (99.190)\n",
      "TRAINING - Epoch: [10][90/391]\tTime 1.330 (1.384)\tData 0.006 (0.008)\tLoss 0.5534 (0.6127)\tPrec@1 82.031 (81.714)\tPrec@5 100.000 (99.202)\n",
      "TRAINING - Epoch: [10][100/391]\tTime 1.282 (1.384)\tData 0.002 (0.007)\tLoss 0.6638 (0.6109)\tPrec@1 77.344 (81.799)\tPrec@5 97.656 (99.141)\n",
      "TRAINING - Epoch: [10][110/391]\tTime 1.399 (1.382)\tData 0.002 (0.007)\tLoss 0.6076 (0.6117)\tPrec@1 82.031 (81.806)\tPrec@5 98.438 (99.099)\n",
      "TRAINING - Epoch: [10][120/391]\tTime 1.450 (1.384)\tData 0.009 (0.007)\tLoss 0.5908 (0.6121)\tPrec@1 83.594 (81.825)\tPrec@5 98.438 (99.083)\n",
      "TRAINING - Epoch: [10][130/391]\tTime 1.357 (1.381)\tData 0.003 (0.007)\tLoss 0.6332 (0.6127)\tPrec@1 78.906 (81.828)\tPrec@5 96.875 (99.082)\n",
      "TRAINING - Epoch: [10][140/391]\tTime 1.323 (1.380)\tData 0.002 (0.007)\tLoss 0.6068 (0.6142)\tPrec@1 81.250 (81.793)\tPrec@5 97.656 (99.058)\n",
      "TRAINING - Epoch: [10][150/391]\tTime 1.533 (1.381)\tData 0.002 (0.006)\tLoss 0.6133 (0.6142)\tPrec@1 82.031 (81.767)\tPrec@5 100.000 (99.089)\n",
      "TRAINING - Epoch: [10][160/391]\tTime 1.337 (1.380)\tData 0.002 (0.006)\tLoss 0.5310 (0.6142)\tPrec@1 85.156 (81.769)\tPrec@5 99.219 (99.093)\n",
      "TRAINING - Epoch: [10][170/391]\tTime 1.437 (1.381)\tData 0.002 (0.006)\tLoss 0.6360 (0.6151)\tPrec@1 83.594 (81.711)\tPrec@5 99.219 (99.086)\n",
      "TRAINING - Epoch: [10][180/391]\tTime 1.634 (1.382)\tData 0.009 (0.006)\tLoss 0.7134 (0.6178)\tPrec@1 78.125 (81.574)\tPrec@5 98.438 (99.085)\n",
      "TRAINING - Epoch: [10][190/391]\tTime 1.335 (1.380)\tData 0.009 (0.006)\tLoss 0.6355 (0.6180)\tPrec@1 84.375 (81.585)\tPrec@5 99.219 (99.080)\n",
      "TRAINING - Epoch: [10][200/391]\tTime 1.300 (1.380)\tData 0.002 (0.006)\tLoss 0.8339 (0.6185)\tPrec@1 73.438 (81.615)\tPrec@5 96.875 (99.075)\n",
      "TRAINING - Epoch: [10][210/391]\tTime 1.464 (1.378)\tData 0.002 (0.006)\tLoss 0.5327 (0.6179)\tPrec@1 87.500 (81.642)\tPrec@5 100.000 (99.082)\n",
      "TRAINING - Epoch: [10][220/391]\tTime 1.268 (1.378)\tData 0.002 (0.006)\tLoss 0.5163 (0.6176)\tPrec@1 85.156 (81.653)\tPrec@5 100.000 (99.081)\n",
      "TRAINING - Epoch: [10][230/391]\tTime 1.456 (1.377)\tData 0.009 (0.006)\tLoss 0.5956 (0.6199)\tPrec@1 77.344 (81.592)\tPrec@5 99.219 (99.063)\n",
      "TRAINING - Epoch: [10][240/391]\tTime 1.478 (1.376)\tData 0.002 (0.006)\tLoss 0.5300 (0.6198)\tPrec@1 84.375 (81.675)\tPrec@5 100.000 (99.060)\n",
      "TRAINING - Epoch: [10][250/391]\tTime 1.329 (1.376)\tData 0.012 (0.006)\tLoss 0.7280 (0.6196)\tPrec@1 79.688 (81.633)\tPrec@5 96.875 (99.044)\n",
      "TRAINING - Epoch: [10][260/391]\tTime 1.344 (1.376)\tData 0.002 (0.006)\tLoss 0.5965 (0.6192)\tPrec@1 78.125 (81.639)\tPrec@5 100.000 (99.045)\n",
      "TRAINING - Epoch: [10][270/391]\tTime 1.618 (1.377)\tData 0.004 (0.006)\tLoss 0.7683 (0.6198)\tPrec@1 77.344 (81.616)\tPrec@5 99.219 (99.026)\n",
      "TRAINING - Epoch: [10][280/391]\tTime 1.309 (1.376)\tData 0.002 (0.006)\tLoss 0.6646 (0.6218)\tPrec@1 78.125 (81.561)\tPrec@5 100.000 (99.027)\n",
      "TRAINING - Epoch: [10][290/391]\tTime 1.357 (1.376)\tData 0.002 (0.006)\tLoss 0.8380 (0.6223)\tPrec@1 75.781 (81.540)\tPrec@5 99.219 (99.020)\n",
      "TRAINING - Epoch: [10][300/391]\tTime 1.331 (1.376)\tData 0.009 (0.006)\tLoss 0.5603 (0.6231)\tPrec@1 82.812 (81.510)\tPrec@5 100.000 (99.006)\n",
      "TRAINING - Epoch: [10][310/391]\tTime 1.311 (1.376)\tData 0.002 (0.006)\tLoss 0.6055 (0.6238)\tPrec@1 80.469 (81.491)\tPrec@5 99.219 (98.988)\n",
      "TRAINING - Epoch: [10][320/391]\tTime 1.369 (1.375)\tData 0.003 (0.006)\tLoss 0.5028 (0.6231)\tPrec@1 85.156 (81.489)\tPrec@5 100.000 (99.005)\n",
      "TRAINING - Epoch: [10][330/391]\tTime 1.289 (1.375)\tData 0.009 (0.006)\tLoss 0.5660 (0.6241)\tPrec@1 81.250 (81.422)\tPrec@5 100.000 (99.013)\n",
      "TRAINING - Epoch: [10][340/391]\tTime 1.384 (1.376)\tData 0.002 (0.006)\tLoss 0.5768 (0.6246)\tPrec@1 82.031 (81.385)\tPrec@5 99.219 (99.015)\n",
      "TRAINING - Epoch: [10][350/391]\tTime 1.368 (1.375)\tData 0.002 (0.005)\tLoss 0.6339 (0.6250)\tPrec@1 81.250 (81.381)\tPrec@5 97.656 (99.010)\n",
      "TRAINING - Epoch: [10][360/391]\tTime 1.341 (1.374)\tData 0.002 (0.005)\tLoss 0.4825 (0.6248)\tPrec@1 84.375 (81.378)\tPrec@5 100.000 (99.020)\n",
      "TRAINING - Epoch: [10][370/391]\tTime 1.522 (1.375)\tData 0.002 (0.005)\tLoss 0.4634 (0.6232)\tPrec@1 85.938 (81.440)\tPrec@5 100.000 (99.029)\n",
      "TRAINING - Epoch: [10][380/391]\tTime 1.290 (1.374)\tData 0.002 (0.005)\tLoss 0.4916 (0.6220)\tPrec@1 85.156 (81.459)\tPrec@5 99.219 (99.038)\n",
      "TRAINING - Epoch: [10][390/391]\tTime 0.869 (1.372)\tData 0.002 (0.005)\tLoss 0.4736 (0.6214)\tPrec@1 85.000 (81.460)\tPrec@5 100.000 (99.044)\n",
      "EVALUATING - Epoch: [10][0/79]\tTime 1.200 (1.200)\tData 0.397 (0.397)\tLoss 0.5408 (0.5408)\tPrec@1 83.594 (83.594)\tPrec@5 100.000 (100.000)\n",
      "EVALUATING - Epoch: [10][10/79]\tTime 0.565 (0.635)\tData 0.001 (0.039)\tLoss 0.6905 (0.6759)\tPrec@1 80.469 (80.327)\tPrec@5 97.656 (99.006)\n",
      "EVALUATING - Epoch: [10][20/79]\tTime 0.620 (0.612)\tData 0.002 (0.022)\tLoss 0.6935 (0.7049)\tPrec@1 83.594 (79.501)\tPrec@5 99.219 (98.884)\n",
      "EVALUATING - Epoch: [10][30/79]\tTime 0.563 (0.599)\tData 0.002 (0.016)\tLoss 0.5664 (0.6906)\tPrec@1 81.250 (80.192)\tPrec@5 100.000 (98.967)\n",
      "EVALUATING - Epoch: [10][40/79]\tTime 0.610 (0.601)\tData 0.002 (0.013)\tLoss 0.7026 (0.6888)\tPrec@1 76.562 (80.412)\tPrec@5 99.219 (98.914)\n",
      "EVALUATING - Epoch: [10][50/79]\tTime 0.618 (0.599)\tData 0.002 (0.011)\tLoss 0.6777 (0.6912)\tPrec@1 79.688 (80.499)\tPrec@5 98.438 (98.943)\n",
      "EVALUATING - Epoch: [10][60/79]\tTime 0.544 (0.596)\tData 0.002 (0.010)\tLoss 0.6567 (0.6852)\tPrec@1 78.906 (80.622)\tPrec@5 99.219 (98.963)\n",
      "EVALUATING - Epoch: [10][70/79]\tTime 0.561 (0.590)\tData 0.002 (0.009)\tLoss 0.6278 (0.6915)\tPrec@1 81.250 (80.062)\tPrec@5 100.000 (98.999)\n",
      "\n",
      " Epoch: 11\tTraining Loss 0.6214 \tTraining Prec@1 81.460 \tTraining Prec@5 99.044 \tValidation Loss 0.6884 \tValidation Prec@1 80.070 \tValidation Prec@5 98.990 \n",
      "\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "TRAINING - Epoch: [11][0/391]\tTime 2.693 (2.693)\tData 0.389 (0.389)\tLoss 0.5534 (0.5534)\tPrec@1 81.250 (81.250)\tPrec@5 98.438 (98.438)\n",
      "TRAINING - Epoch: [11][10/391]\tTime 1.359 (1.505)\tData 0.009 (0.039)\tLoss 0.5347 (0.5959)\tPrec@1 84.375 (82.741)\tPrec@5 100.000 (99.219)\n",
      "TRAINING - Epoch: [11][20/391]\tTime 1.437 (1.421)\tData 0.002 (0.022)\tLoss 0.6365 (0.6264)\tPrec@1 78.125 (81.287)\tPrec@5 100.000 (99.219)\n",
      "TRAINING - Epoch: [11][30/391]\tTime 1.439 (1.417)\tData 0.002 (0.016)\tLoss 0.6237 (0.6278)\tPrec@1 79.688 (80.973)\tPrec@5 99.219 (99.143)\n",
      "TRAINING - Epoch: [11][40/391]\tTime 1.325 (1.405)\tData 0.002 (0.013)\tLoss 0.6982 (0.6290)\tPrec@1 77.344 (80.869)\tPrec@5 99.219 (99.181)\n",
      "TRAINING - Epoch: [11][50/391]\tTime 1.508 (1.401)\tData 0.002 (0.011)\tLoss 0.6035 (0.6272)\tPrec@1 80.469 (80.821)\tPrec@5 100.000 (99.173)\n",
      "TRAINING - Epoch: [11][60/391]\tTime 1.410 (1.401)\tData 0.002 (0.010)\tLoss 0.6656 (0.6195)\tPrec@1 78.125 (81.263)\tPrec@5 100.000 (99.155)\n",
      "TRAINING - Epoch: [11][70/391]\tTime 1.382 (1.398)\tData 0.002 (0.009)\tLoss 0.5031 (0.6148)\tPrec@1 85.156 (81.393)\tPrec@5 100.000 (99.153)\n",
      "TRAINING - Epoch: [11][80/391]\tTime 1.427 (1.396)\tData 0.009 (0.008)\tLoss 0.6925 (0.6148)\tPrec@1 78.906 (81.404)\tPrec@5 97.656 (99.171)\n",
      "TRAINING - Epoch: [11][90/391]\tTime 1.377 (1.393)\tData 0.002 (0.008)\tLoss 0.7177 (0.6159)\tPrec@1 75.000 (81.465)\tPrec@5 99.219 (99.141)\n",
      "TRAINING - Epoch: [11][100/391]\tTime 1.352 (1.390)\tData 0.002 (0.007)\tLoss 0.6226 (0.6158)\tPrec@1 79.688 (81.381)\tPrec@5 99.219 (99.134)\n",
      "TRAINING - Epoch: [11][110/391]\tTime 1.385 (1.388)\tData 0.002 (0.007)\tLoss 0.6767 (0.6093)\tPrec@1 81.250 (81.700)\tPrec@5 98.438 (99.162)\n",
      "TRAINING - Epoch: [11][120/391]\tTime 1.372 (1.386)\tData 0.006 (0.007)\tLoss 0.5850 (0.6076)\tPrec@1 79.688 (81.773)\tPrec@5 100.000 (99.154)\n",
      "TRAINING - Epoch: [11][130/391]\tTime 1.314 (1.383)\tData 0.002 (0.006)\tLoss 0.5910 (0.6096)\tPrec@1 78.906 (81.548)\tPrec@5 98.438 (99.135)\n",
      "TRAINING - Epoch: [11][140/391]\tTime 1.470 (1.384)\tData 0.006 (0.006)\tLoss 0.8034 (0.6104)\tPrec@1 80.469 (81.582)\tPrec@5 97.656 (99.136)\n",
      "TRAINING - Epoch: [11][150/391]\tTime 1.313 (1.383)\tData 0.002 (0.006)\tLoss 0.6695 (0.6111)\tPrec@1 77.344 (81.566)\tPrec@5 100.000 (99.136)\n",
      "TRAINING - Epoch: [11][160/391]\tTime 1.303 (1.383)\tData 0.002 (0.006)\tLoss 0.4328 (0.6092)\tPrec@1 91.406 (81.701)\tPrec@5 99.219 (99.122)\n",
      "TRAINING - Epoch: [11][170/391]\tTime 1.298 (1.381)\tData 0.002 (0.006)\tLoss 0.6940 (0.6098)\tPrec@1 79.688 (81.707)\tPrec@5 98.438 (99.109)\n",
      "TRAINING - Epoch: [11][180/391]\tTime 1.353 (1.382)\tData 0.016 (0.006)\tLoss 0.4591 (0.6078)\tPrec@1 85.938 (81.768)\tPrec@5 100.000 (99.107)\n",
      "TRAINING - Epoch: [11][190/391]\tTime 1.286 (1.384)\tData 0.002 (0.006)\tLoss 0.5286 (0.6088)\tPrec@1 80.469 (81.745)\tPrec@5 100.000 (99.116)\n",
      "TRAINING - Epoch: [11][200/391]\tTime 1.462 (1.383)\tData 0.009 (0.006)\tLoss 0.5718 (0.6091)\tPrec@1 85.938 (81.744)\tPrec@5 99.219 (99.118)\n",
      "TRAINING - Epoch: [11][210/391]\tTime 1.304 (1.383)\tData 0.002 (0.006)\tLoss 0.5132 (0.6103)\tPrec@1 84.375 (81.713)\tPrec@5 100.000 (99.130)\n",
      "TRAINING - Epoch: [11][220/391]\tTime 1.373 (1.382)\tData 0.002 (0.006)\tLoss 0.5716 (0.6102)\tPrec@1 86.719 (81.720)\tPrec@5 100.000 (99.123)\n",
      "TRAINING - Epoch: [11][230/391]\tTime 1.269 (1.380)\tData 0.002 (0.006)\tLoss 0.6179 (0.6088)\tPrec@1 81.250 (81.734)\tPrec@5 99.219 (99.144)\n",
      "TRAINING - Epoch: [11][240/391]\tTime 1.270 (1.379)\tData 0.002 (0.005)\tLoss 0.4603 (0.6076)\tPrec@1 88.281 (81.778)\tPrec@5 99.219 (99.141)\n",
      "TRAINING - Epoch: [11][250/391]\tTime 1.446 (1.379)\tData 0.002 (0.005)\tLoss 0.6566 (0.6089)\tPrec@1 80.469 (81.723)\tPrec@5 100.000 (99.132)\n",
      "TRAINING - Epoch: [11][260/391]\tTime 1.280 (1.378)\tData 0.009 (0.005)\tLoss 0.5987 (0.6073)\tPrec@1 83.594 (81.783)\tPrec@5 98.438 (99.141)\n",
      "TRAINING - Epoch: [11][270/391]\tTime 1.403 (1.376)\tData 0.014 (0.005)\tLoss 0.6852 (0.6092)\tPrec@1 78.906 (81.711)\tPrec@5 98.438 (99.141)\n",
      "TRAINING - Epoch: [11][280/391]\tTime 1.407 (1.376)\tData 0.002 (0.005)\tLoss 0.6606 (0.6097)\tPrec@1 77.344 (81.695)\tPrec@5 99.219 (99.138)\n",
      "TRAINING - Epoch: [11][290/391]\tTime 1.420 (1.375)\tData 0.004 (0.005)\tLoss 0.5805 (0.6097)\tPrec@1 83.594 (81.706)\tPrec@5 100.000 (99.138)\n",
      "TRAINING - Epoch: [11][300/391]\tTime 1.447 (1.375)\tData 0.002 (0.005)\tLoss 0.5802 (0.6104)\tPrec@1 84.375 (81.689)\tPrec@5 99.219 (99.146)\n",
      "TRAINING - Epoch: [11][310/391]\tTime 1.275 (1.373)\tData 0.002 (0.005)\tLoss 0.6033 (0.6102)\tPrec@1 82.031 (81.675)\tPrec@5 98.438 (99.156)\n",
      "TRAINING - Epoch: [11][320/391]\tTime 0.737 (1.364)\tData 0.002 (0.005)\tLoss 0.5884 (0.6108)\tPrec@1 82.031 (81.637)\tPrec@5 99.219 (99.141)\n",
      "TRAINING - Epoch: [11][330/391]\tTime 0.748 (1.346)\tData 0.002 (0.005)\tLoss 0.8178 (0.6117)\tPrec@1 75.781 (81.618)\tPrec@5 99.219 (99.141)\n",
      "TRAINING - Epoch: [11][340/391]\tTime 0.779 (1.329)\tData 0.002 (0.005)\tLoss 0.7458 (0.6121)\tPrec@1 75.781 (81.626)\tPrec@5 100.000 (99.150)\n",
      "TRAINING - Epoch: [11][350/391]\tTime 0.747 (1.313)\tData 0.002 (0.005)\tLoss 0.5070 (0.6120)\tPrec@1 82.812 (81.608)\tPrec@5 100.000 (99.156)\n",
      "TRAINING - Epoch: [11][360/391]\tTime 0.766 (1.298)\tData 0.002 (0.005)\tLoss 0.5807 (0.6115)\tPrec@1 83.594 (81.635)\tPrec@5 99.219 (99.149)\n",
      "TRAINING - Epoch: [11][370/391]\tTime 0.789 (1.284)\tData 0.002 (0.005)\tLoss 0.5912 (0.6103)\tPrec@1 84.375 (81.688)\tPrec@5 99.219 (99.158)\n",
      "TRAINING - Epoch: [11][380/391]\tTime 0.740 (1.271)\tData 0.002 (0.005)\tLoss 0.6061 (0.6092)\tPrec@1 79.688 (81.720)\tPrec@5 99.219 (99.161)\n",
      "TRAINING - Epoch: [11][390/391]\tTime 0.572 (1.257)\tData 0.002 (0.004)\tLoss 0.5988 (0.6106)\tPrec@1 81.250 (81.668)\tPrec@5 98.750 (99.158)\n",
      "EVALUATING - Epoch: [11][0/79]\tTime 0.800 (0.800)\tData 0.281 (0.281)\tLoss 0.5804 (0.5804)\tPrec@1 79.688 (79.688)\tPrec@5 99.219 (99.219)\n",
      "EVALUATING - Epoch: [11][10/79]\tTime 0.364 (0.385)\tData 0.002 (0.028)\tLoss 0.6340 (0.6905)\tPrec@1 80.469 (80.611)\tPrec@5 99.219 (98.864)\n",
      "EVALUATING - Epoch: [11][20/79]\tTime 0.337 (0.362)\tData 0.002 (0.015)\tLoss 0.7219 (0.7203)\tPrec@1 79.688 (79.539)\tPrec@5 100.000 (98.921)\n",
      "EVALUATING - Epoch: [11][30/79]\tTime 0.338 (0.353)\tData 0.002 (0.011)\tLoss 0.5585 (0.7139)\tPrec@1 84.375 (79.940)\tPrec@5 100.000 (98.841)\n",
      "EVALUATING - Epoch: [11][40/79]\tTime 0.379 (0.349)\tData 0.002 (0.009)\tLoss 0.8095 (0.7094)\tPrec@1 76.562 (80.202)\tPrec@5 99.219 (98.819)\n",
      "EVALUATING - Epoch: [11][50/79]\tTime 0.380 (0.349)\tData 0.002 (0.007)\tLoss 0.7924 (0.7066)\tPrec@1 76.562 (80.162)\tPrec@5 97.656 (98.912)\n",
      "EVALUATING - Epoch: [11][60/79]\tTime 0.328 (0.346)\tData 0.002 (0.006)\tLoss 0.6942 (0.7003)\tPrec@1 79.688 (80.174)\tPrec@5 100.000 (98.988)\n",
      "EVALUATING - Epoch: [11][70/79]\tTime 0.372 (0.344)\tData 0.002 (0.006)\tLoss 0.5435 (0.7022)\tPrec@1 85.156 (80.194)\tPrec@5 100.000 (98.977)\n",
      "\n",
      " Epoch: 12\tTraining Loss 0.6106 \tTraining Prec@1 81.668 \tTraining Prec@5 99.158 \tValidation Loss 0.6982 \tValidation Prec@1 80.250 \tValidation Prec@5 99.010 \n",
      "\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "TRAINING - Epoch: [12][0/391]\tTime 1.559 (1.559)\tData 0.328 (0.328)\tLoss 0.5060 (0.5060)\tPrec@1 85.938 (85.938)\tPrec@5 100.000 (100.000)\n",
      "TRAINING - Epoch: [12][10/391]\tTime 0.748 (0.847)\tData 0.001 (0.032)\tLoss 0.7220 (0.6374)\tPrec@1 77.344 (80.824)\tPrec@5 98.438 (99.077)\n",
      "TRAINING - Epoch: [12][20/391]\tTime 0.763 (0.804)\tData 0.002 (0.017)\tLoss 0.6152 (0.6206)\tPrec@1 83.594 (81.250)\tPrec@5 98.438 (99.107)\n",
      "TRAINING - Epoch: [12][30/391]\tTime 0.746 (0.791)\tData 0.002 (0.012)\tLoss 0.5387 (0.6227)\tPrec@1 85.156 (81.452)\tPrec@5 100.000 (99.194)\n",
      "TRAINING - Epoch: [12][40/391]\tTime 0.749 (0.787)\tData 0.002 (0.010)\tLoss 0.5727 (0.6146)\tPrec@1 80.469 (81.460)\tPrec@5 100.000 (99.257)\n",
      "TRAINING - Epoch: [12][50/391]\tTime 0.758 (0.782)\tData 0.002 (0.008)\tLoss 0.5505 (0.6102)\tPrec@1 83.594 (81.618)\tPrec@5 98.438 (99.219)\n",
      "TRAINING - Epoch: [12][60/391]\tTime 0.738 (0.777)\tData 0.002 (0.007)\tLoss 0.6157 (0.6093)\tPrec@1 85.156 (81.814)\tPrec@5 98.438 (99.155)\n",
      "TRAINING - Epoch: [12][70/391]\tTime 0.771 (0.775)\tData 0.002 (0.006)\tLoss 0.7533 (0.6077)\tPrec@1 74.219 (81.811)\tPrec@5 96.875 (99.142)\n",
      "TRAINING - Epoch: [12][80/391]\tTime 0.752 (0.773)\tData 0.002 (0.006)\tLoss 0.5095 (0.5985)\tPrec@1 84.375 (82.157)\tPrec@5 99.219 (99.142)\n",
      "TRAINING - Epoch: [12][90/391]\tTime 0.775 (0.772)\tData 0.002 (0.005)\tLoss 0.5534 (0.5964)\tPrec@1 82.031 (82.126)\tPrec@5 100.000 (99.176)\n",
      "TRAINING - Epoch: [12][100/391]\tTime 0.752 (0.772)\tData 0.002 (0.005)\tLoss 0.5415 (0.5956)\tPrec@1 82.812 (82.132)\tPrec@5 99.219 (99.141)\n",
      "TRAINING - Epoch: [12][110/391]\tTime 1.399 (0.803)\tData 0.002 (0.005)\tLoss 0.5839 (0.5940)\tPrec@1 82.812 (82.186)\tPrec@5 98.438 (99.134)\n",
      "TRAINING - Epoch: [12][120/391]\tTime 0.796 (0.808)\tData 0.002 (0.005)\tLoss 0.6024 (0.5893)\tPrec@1 81.250 (82.315)\tPrec@5 99.219 (99.135)\n",
      "TRAINING - Epoch: [12][130/391]\tTime 0.746 (0.806)\tData 0.002 (0.004)\tLoss 0.6267 (0.5908)\tPrec@1 81.250 (82.264)\tPrec@5 99.219 (99.153)\n",
      "TRAINING - Epoch: [12][140/391]\tTime 0.746 (0.804)\tData 0.002 (0.004)\tLoss 0.4903 (0.5908)\tPrec@1 88.281 (82.275)\tPrec@5 98.438 (99.169)\n",
      "TRAINING - Epoch: [12][150/391]\tTime 0.958 (0.805)\tData 0.002 (0.004)\tLoss 0.7669 (0.5891)\tPrec@1 74.219 (82.316)\tPrec@5 96.094 (99.157)\n",
      "TRAINING - Epoch: [12][160/391]\tTime 0.813 (0.803)\tData 0.002 (0.004)\tLoss 0.6432 (0.5902)\tPrec@1 81.250 (82.284)\tPrec@5 100.000 (99.156)\n",
      "TRAINING - Epoch: [12][170/391]\tTime 0.770 (0.801)\tData 0.002 (0.004)\tLoss 0.5868 (0.5900)\tPrec@1 83.594 (82.264)\tPrec@5 100.000 (99.182)\n",
      "TRAINING - Epoch: [12][180/391]\tTime 0.765 (0.800)\tData 0.002 (0.004)\tLoss 0.6123 (0.5890)\tPrec@1 82.812 (82.320)\tPrec@5 99.219 (99.189)\n",
      "TRAINING - Epoch: [12][190/391]\tTime 0.817 (0.799)\tData 0.002 (0.004)\tLoss 0.7469 (0.5920)\tPrec@1 76.562 (82.232)\tPrec@5 98.438 (99.182)\n",
      "TRAINING - Epoch: [12][200/391]\tTime 0.751 (0.798)\tData 0.002 (0.004)\tLoss 0.5746 (0.5926)\tPrec@1 83.594 (82.179)\tPrec@5 98.438 (99.184)\n",
      "TRAINING - Epoch: [12][210/391]\tTime 0.755 (0.797)\tData 0.002 (0.004)\tLoss 0.6070 (0.5932)\tPrec@1 83.594 (82.209)\tPrec@5 99.219 (99.189)\n",
      "TRAINING - Epoch: [12][220/391]\tTime 0.788 (0.797)\tData 0.002 (0.003)\tLoss 0.6877 (0.5940)\tPrec@1 81.250 (82.176)\tPrec@5 99.219 (99.198)\n",
      "TRAINING - Epoch: [12][230/391]\tTime 0.761 (0.796)\tData 0.002 (0.003)\tLoss 0.4633 (0.5915)\tPrec@1 89.062 (82.261)\tPrec@5 100.000 (99.215)\n",
      "TRAINING - Epoch: [12][240/391]\tTime 0.808 (0.796)\tData 0.002 (0.003)\tLoss 0.6030 (0.5939)\tPrec@1 81.250 (82.167)\tPrec@5 99.219 (99.219)\n",
      "TRAINING - Epoch: [12][250/391]\tTime 0.795 (0.794)\tData 0.002 (0.003)\tLoss 0.6572 (0.5938)\tPrec@1 74.219 (82.159)\tPrec@5 100.000 (99.222)\n",
      "TRAINING - Epoch: [12][260/391]\tTime 0.760 (0.794)\tData 0.002 (0.003)\tLoss 0.6297 (0.5941)\tPrec@1 82.031 (82.184)\tPrec@5 99.219 (99.216)\n",
      "TRAINING - Epoch: [12][270/391]\tTime 0.747 (0.793)\tData 0.002 (0.003)\tLoss 0.6638 (0.5947)\tPrec@1 77.344 (82.175)\tPrec@5 97.656 (99.213)\n",
      "TRAINING - Epoch: [12][280/391]\tTime 0.827 (0.792)\tData 0.002 (0.003)\tLoss 0.5443 (0.5935)\tPrec@1 83.594 (82.209)\tPrec@5 99.219 (99.210)\n",
      "TRAINING - Epoch: [12][290/391]\tTime 0.752 (0.791)\tData 0.002 (0.003)\tLoss 0.6347 (0.5946)\tPrec@1 79.688 (82.176)\tPrec@5 100.000 (99.205)\n",
      "TRAINING - Epoch: [12][300/391]\tTime 0.754 (0.790)\tData 0.002 (0.003)\tLoss 0.6927 (0.5948)\tPrec@1 79.688 (82.174)\tPrec@5 99.219 (99.211)\n",
      "TRAINING - Epoch: [12][310/391]\tTime 0.784 (0.790)\tData 0.002 (0.003)\tLoss 0.5483 (0.5953)\tPrec@1 85.156 (82.162)\tPrec@5 98.438 (99.196)\n",
      "TRAINING - Epoch: [12][320/391]\tTime 0.762 (0.790)\tData 0.003 (0.003)\tLoss 0.6248 (0.5952)\tPrec@1 80.469 (82.160)\tPrec@5 99.219 (99.194)\n",
      "TRAINING - Epoch: [12][330/391]\tTime 0.797 (0.789)\tData 0.002 (0.003)\tLoss 0.5203 (0.5949)\tPrec@1 85.156 (82.175)\tPrec@5 100.000 (99.188)\n",
      "TRAINING - Epoch: [12][340/391]\tTime 0.775 (0.789)\tData 0.002 (0.003)\tLoss 0.6191 (0.5942)\tPrec@1 81.250 (82.192)\tPrec@5 100.000 (99.191)\n",
      "TRAINING - Epoch: [12][350/391]\tTime 0.764 (0.789)\tData 0.002 (0.003)\tLoss 0.5674 (0.5946)\tPrec@1 85.156 (82.178)\tPrec@5 99.219 (99.192)\n",
      "TRAINING - Epoch: [12][360/391]\tTime 0.761 (0.789)\tData 0.002 (0.003)\tLoss 0.5572 (0.5959)\tPrec@1 85.156 (82.185)\tPrec@5 99.219 (99.186)\n",
      "TRAINING - Epoch: [12][370/391]\tTime 0.770 (0.788)\tData 0.002 (0.003)\tLoss 0.5028 (0.5966)\tPrec@1 84.375 (82.151)\tPrec@5 100.000 (99.183)\n",
      "TRAINING - Epoch: [12][380/391]\tTime 0.734 (0.787)\tData 0.002 (0.003)\tLoss 0.6540 (0.5972)\tPrec@1 75.781 (82.119)\tPrec@5 100.000 (99.190)\n",
      "TRAINING - Epoch: [12][390/391]\tTime 0.476 (0.786)\tData 0.002 (0.003)\tLoss 0.6862 (0.5977)\tPrec@1 81.250 (82.130)\tPrec@5 98.750 (99.186)\n",
      "EVALUATING - Epoch: [12][0/79]\tTime 0.866 (0.866)\tData 0.275 (0.275)\tLoss 0.6110 (0.6110)\tPrec@1 79.688 (79.688)\tPrec@5 99.219 (99.219)\n",
      "EVALUATING - Epoch: [12][10/79]\tTime 0.333 (0.393)\tData 0.001 (0.027)\tLoss 0.7248 (0.6910)\tPrec@1 78.906 (79.830)\tPrec@5 99.219 (98.935)\n",
      "EVALUATING - Epoch: [12][20/79]\tTime 0.326 (0.373)\tData 0.002 (0.015)\tLoss 0.6488 (0.7105)\tPrec@1 83.594 (79.762)\tPrec@5 98.438 (98.847)\n",
      "EVALUATING - Epoch: [12][30/79]\tTime 0.328 (0.362)\tData 0.002 (0.011)\tLoss 0.6304 (0.7019)\tPrec@1 80.469 (79.965)\tPrec@5 100.000 (98.589)\n",
      "EVALUATING - Epoch: [12][40/79]\tTime 0.393 (0.361)\tData 0.002 (0.009)\tLoss 0.7590 (0.6998)\tPrec@1 76.562 (79.973)\tPrec@5 98.438 (98.552)\n",
      "EVALUATING - Epoch: [12][50/79]\tTime 0.327 (0.359)\tData 0.002 (0.007)\tLoss 0.7611 (0.7007)\tPrec@1 81.250 (79.963)\tPrec@5 98.438 (98.652)\n",
      "EVALUATING - Epoch: [12][60/79]\tTime 0.359 (0.359)\tData 0.002 (0.006)\tLoss 0.8007 (0.7008)\tPrec@1 78.906 (80.110)\tPrec@5 100.000 (98.732)\n",
      "EVALUATING - Epoch: [12][70/79]\tTime 0.364 (0.357)\tData 0.002 (0.006)\tLoss 0.6241 (0.6973)\tPrec@1 82.812 (80.238)\tPrec@5 99.219 (98.768)\n",
      "\n",
      " Epoch: 13\tTraining Loss 0.5977 \tTraining Prec@1 82.130 \tTraining Prec@5 99.186 \tValidation Loss 0.6947 \tValidation Prec@1 80.300 \tValidation Prec@5 98.800 \n",
      "\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "TRAINING - Epoch: [13][0/391]\tTime 1.598 (1.598)\tData 0.324 (0.324)\tLoss 0.4496 (0.4496)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "TRAINING - Epoch: [13][10/391]\tTime 0.811 (0.859)\tData 0.001 (0.031)\tLoss 0.5692 (0.5601)\tPrec@1 85.938 (82.599)\tPrec@5 100.000 (99.361)\n",
      "TRAINING - Epoch: [13][20/391]\tTime 0.740 (0.813)\tData 0.002 (0.017)\tLoss 0.6200 (0.5719)\tPrec@1 81.250 (82.589)\tPrec@5 100.000 (99.293)\n",
      "TRAINING - Epoch: [13][30/391]\tTime 0.793 (0.800)\tData 0.002 (0.012)\tLoss 0.5206 (0.5786)\tPrec@1 85.938 (82.409)\tPrec@5 99.219 (99.244)\n",
      "TRAINING - Epoch: [13][40/391]\tTime 0.739 (0.792)\tData 0.002 (0.010)\tLoss 0.6365 (0.5783)\tPrec@1 76.562 (82.298)\tPrec@5 100.000 (99.314)\n",
      "TRAINING - Epoch: [13][50/391]\tTime 0.786 (0.790)\tData 0.002 (0.008)\tLoss 0.5557 (0.5840)\tPrec@1 85.938 (82.353)\tPrec@5 97.656 (99.280)\n",
      "TRAINING - Epoch: [13][60/391]\tTime 0.791 (0.787)\tData 0.002 (0.007)\tLoss 0.6360 (0.5923)\tPrec@1 82.031 (82.223)\tPrec@5 98.438 (99.321)\n",
      "TRAINING - Epoch: [13][70/391]\tTime 0.744 (0.785)\tData 0.002 (0.006)\tLoss 0.6010 (0.5948)\tPrec@1 82.031 (82.119)\tPrec@5 97.656 (99.241)\n",
      "TRAINING - Epoch: [13][80/391]\tTime 0.759 (0.783)\tData 0.002 (0.006)\tLoss 0.6398 (0.5917)\tPrec@1 84.375 (82.350)\tPrec@5 97.656 (99.219)\n",
      "TRAINING - Epoch: [13][90/391]\tTime 0.797 (0.782)\tData 0.002 (0.005)\tLoss 0.6440 (0.5898)\tPrec@1 77.344 (82.443)\tPrec@5 100.000 (99.270)\n",
      "TRAINING - Epoch: [13][100/391]\tTime 0.760 (0.781)\tData 0.002 (0.005)\tLoss 0.6339 (0.5869)\tPrec@1 82.031 (82.511)\tPrec@5 97.656 (99.273)\n",
      "TRAINING - Epoch: [13][110/391]\tTime 0.800 (0.780)\tData 0.002 (0.005)\tLoss 0.4905 (0.5875)\tPrec@1 85.938 (82.580)\tPrec@5 99.219 (99.240)\n",
      "TRAINING - Epoch: [13][120/391]\tTime 0.782 (0.780)\tData 0.002 (0.005)\tLoss 0.6016 (0.5889)\tPrec@1 85.156 (82.619)\tPrec@5 98.438 (99.264)\n",
      "TRAINING - Epoch: [13][130/391]\tTime 0.755 (0.780)\tData 0.002 (0.004)\tLoss 0.3962 (0.5870)\tPrec@1 89.844 (82.705)\tPrec@5 99.219 (99.272)\n",
      "TRAINING - Epoch: [13][140/391]\tTime 0.747 (0.780)\tData 0.002 (0.004)\tLoss 0.5724 (0.5871)\tPrec@1 86.719 (82.763)\tPrec@5 99.219 (99.258)\n",
      "TRAINING - Epoch: [13][150/391]\tTime 0.789 (0.779)\tData 0.002 (0.004)\tLoss 0.7468 (0.5903)\tPrec@1 81.250 (82.657)\tPrec@5 97.656 (99.239)\n",
      "TRAINING - Epoch: [13][160/391]\tTime 0.758 (0.779)\tData 0.002 (0.004)\tLoss 0.6893 (0.5912)\tPrec@1 80.469 (82.662)\tPrec@5 98.438 (99.238)\n",
      "TRAINING - Epoch: [13][170/391]\tTime 0.747 (0.778)\tData 0.002 (0.004)\tLoss 0.6725 (0.5871)\tPrec@1 78.125 (82.817)\tPrec@5 100.000 (99.251)\n",
      "TRAINING - Epoch: [13][180/391]\tTime 0.777 (0.777)\tData 0.002 (0.004)\tLoss 0.5111 (0.5853)\tPrec@1 85.938 (82.895)\tPrec@5 100.000 (99.266)\n",
      "TRAINING - Epoch: [13][190/391]\tTime 0.759 (0.777)\tData 0.002 (0.004)\tLoss 0.4989 (0.5852)\tPrec@1 85.156 (82.886)\tPrec@5 100.000 (99.280)\n",
      "TRAINING - Epoch: [13][200/391]\tTime 0.753 (0.777)\tData 0.002 (0.004)\tLoss 0.5041 (0.5847)\tPrec@1 83.594 (82.836)\tPrec@5 99.219 (99.296)\n",
      "TRAINING - Epoch: [13][210/391]\tTime 0.773 (0.777)\tData 0.002 (0.003)\tLoss 0.4642 (0.5854)\tPrec@1 86.719 (82.809)\tPrec@5 100.000 (99.293)\n",
      "TRAINING - Epoch: [13][220/391]\tTime 0.750 (0.777)\tData 0.002 (0.003)\tLoss 0.4678 (0.5847)\tPrec@1 85.938 (82.798)\tPrec@5 99.219 (99.297)\n",
      "TRAINING - Epoch: [13][230/391]\tTime 0.750 (0.777)\tData 0.002 (0.003)\tLoss 0.7001 (0.5846)\tPrec@1 78.125 (82.819)\tPrec@5 98.438 (99.286)\n",
      "TRAINING - Epoch: [13][240/391]\tTime 0.777 (0.777)\tData 0.002 (0.003)\tLoss 0.5434 (0.5839)\tPrec@1 84.375 (82.838)\tPrec@5 100.000 (99.300)\n",
      "TRAINING - Epoch: [13][250/391]\tTime 0.742 (0.776)\tData 0.002 (0.003)\tLoss 0.4524 (0.5857)\tPrec@1 85.938 (82.738)\tPrec@5 100.000 (99.293)\n",
      "TRAINING - Epoch: [13][260/391]\tTime 0.756 (0.776)\tData 0.002 (0.003)\tLoss 0.5172 (0.5867)\tPrec@1 84.375 (82.681)\tPrec@5 100.000 (99.282)\n",
      "TRAINING - Epoch: [13][270/391]\tTime 0.775 (0.776)\tData 0.002 (0.003)\tLoss 0.5666 (0.5884)\tPrec@1 78.906 (82.622)\tPrec@5 99.219 (99.271)\n",
      "TRAINING - Epoch: [13][280/391]\tTime 0.760 (0.775)\tData 0.002 (0.003)\tLoss 0.4723 (0.5876)\tPrec@1 86.719 (82.668)\tPrec@5 100.000 (99.274)\n",
      "TRAINING - Epoch: [13][290/391]\tTime 0.751 (0.775)\tData 0.002 (0.003)\tLoss 0.5785 (0.5890)\tPrec@1 83.594 (82.608)\tPrec@5 99.219 (99.275)\n",
      "TRAINING - Epoch: [13][300/391]\tTime 0.787 (0.775)\tData 0.002 (0.003)\tLoss 0.5283 (0.5888)\tPrec@1 84.375 (82.587)\tPrec@5 100.000 (99.273)\n",
      "TRAINING - Epoch: [13][310/391]\tTime 0.759 (0.775)\tData 0.002 (0.003)\tLoss 0.5832 (0.5896)\tPrec@1 82.812 (82.531)\tPrec@5 100.000 (99.282)\n",
      "TRAINING - Epoch: [13][320/391]\tTime 0.763 (0.775)\tData 0.002 (0.003)\tLoss 0.5571 (0.5889)\tPrec@1 83.594 (82.525)\tPrec@5 99.219 (99.284)\n",
      "TRAINING - Epoch: [13][330/391]\tTime 0.745 (0.774)\tData 0.002 (0.003)\tLoss 0.4722 (0.5892)\tPrec@1 91.406 (82.551)\tPrec@5 99.219 (99.275)\n",
      "TRAINING - Epoch: [13][340/391]\tTime 0.819 (0.774)\tData 0.002 (0.003)\tLoss 0.7342 (0.5897)\tPrec@1 78.906 (82.558)\tPrec@5 98.438 (99.255)\n",
      "TRAINING - Epoch: [13][350/391]\tTime 0.789 (0.774)\tData 0.002 (0.003)\tLoss 0.5511 (0.5902)\tPrec@1 84.375 (82.550)\tPrec@5 100.000 (99.261)\n",
      "TRAINING - Epoch: [13][360/391]\tTime 0.751 (0.774)\tData 0.002 (0.003)\tLoss 0.6968 (0.5893)\tPrec@1 78.906 (82.590)\tPrec@5 97.656 (99.256)\n",
      "TRAINING - Epoch: [13][370/391]\tTime 0.788 (0.774)\tData 0.002 (0.003)\tLoss 0.5300 (0.5889)\tPrec@1 83.594 (82.596)\tPrec@5 100.000 (99.255)\n",
      "TRAINING - Epoch: [13][380/391]\tTime 0.878 (0.774)\tData 0.002 (0.003)\tLoss 0.4599 (0.5891)\tPrec@1 88.281 (82.571)\tPrec@5 100.000 (99.243)\n",
      "TRAINING - Epoch: [13][390/391]\tTime 0.549 (0.773)\tData 0.002 (0.003)\tLoss 0.6275 (0.5892)\tPrec@1 77.500 (82.562)\tPrec@5 98.750 (99.232)\n",
      "EVALUATING - Epoch: [13][0/79]\tTime 0.852 (0.852)\tData 0.336 (0.336)\tLoss 0.5530 (0.5530)\tPrec@1 84.375 (84.375)\tPrec@5 98.438 (98.438)\n",
      "EVALUATING - Epoch: [13][10/79]\tTime 0.328 (0.397)\tData 0.001 (0.032)\tLoss 0.5660 (0.6512)\tPrec@1 83.594 (81.747)\tPrec@5 99.219 (99.361)\n",
      "EVALUATING - Epoch: [13][20/79]\tTime 0.333 (0.375)\tData 0.002 (0.018)\tLoss 0.6487 (0.6758)\tPrec@1 80.469 (81.027)\tPrec@5 100.000 (99.293)\n",
      "EVALUATING - Epoch: [13][30/79]\tTime 0.373 (0.364)\tData 0.002 (0.013)\tLoss 0.5989 (0.6660)\tPrec@1 80.469 (81.149)\tPrec@5 100.000 (99.017)\n",
      "EVALUATING - Epoch: [13][40/79]\tTime 0.353 (0.360)\tData 0.002 (0.010)\tLoss 0.6645 (0.6578)\tPrec@1 77.344 (81.383)\tPrec@5 99.219 (99.028)\n",
      "EVALUATING - Epoch: [13][50/79]\tTime 0.329 (0.357)\tData 0.002 (0.008)\tLoss 0.8075 (0.6562)\tPrec@1 78.125 (81.373)\tPrec@5 98.438 (99.096)\n",
      "EVALUATING - Epoch: [13][60/79]\tTime 0.376 (0.357)\tData 0.002 (0.007)\tLoss 0.6607 (0.6543)\tPrec@1 80.469 (81.276)\tPrec@5 100.000 (99.180)\n",
      "EVALUATING - Epoch: [13][70/79]\tTime 0.325 (0.354)\tData 0.002 (0.007)\tLoss 0.7347 (0.6603)\tPrec@1 81.250 (81.096)\tPrec@5 100.000 (99.131)\n",
      "\n",
      " Epoch: 14\tTraining Loss 0.5892 \tTraining Prec@1 82.562 \tTraining Prec@5 99.232 \tValidation Loss 0.6612 \tValidation Prec@1 81.090 \tValidation Prec@5 99.160 \n",
      "\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "TRAINING - Epoch: [14][0/391]\tTime 1.605 (1.605)\tData 0.329 (0.329)\tLoss 0.6176 (0.6176)\tPrec@1 82.812 (82.812)\tPrec@5 99.219 (99.219)\n",
      "TRAINING - Epoch: [14][10/391]\tTime 0.901 (0.871)\tData 0.001 (0.032)\tLoss 0.6238 (0.5418)\tPrec@1 82.031 (84.162)\tPrec@5 100.000 (99.574)\n",
      "TRAINING - Epoch: [14][20/391]\tTime 0.789 (0.836)\tData 0.002 (0.017)\tLoss 0.5183 (0.5452)\tPrec@1 83.594 (84.003)\tPrec@5 100.000 (99.665)\n",
      "TRAINING - Epoch: [14][30/391]\tTime 0.746 (0.815)\tData 0.002 (0.012)\tLoss 0.6015 (0.5450)\tPrec@1 78.125 (84.022)\tPrec@5 98.438 (99.521)\n",
      "TRAINING - Epoch: [14][40/391]\tTime 0.813 (0.815)\tData 0.002 (0.010)\tLoss 0.4466 (0.5496)\tPrec@1 85.156 (83.670)\tPrec@5 100.000 (99.466)\n",
      "TRAINING - Epoch: [14][50/391]\tTime 0.754 (0.818)\tData 0.002 (0.008)\tLoss 0.4497 (0.5471)\tPrec@1 87.500 (83.701)\tPrec@5 99.219 (99.418)\n",
      "TRAINING - Epoch: [14][60/391]\tTime 0.784 (0.812)\tData 0.002 (0.007)\tLoss 0.6919 (0.5480)\tPrec@1 75.781 (83.671)\tPrec@5 97.656 (99.436)\n",
      "TRAINING - Epoch: [14][70/391]\tTime 0.783 (0.807)\tData 0.003 (0.007)\tLoss 0.5811 (0.5471)\tPrec@1 82.812 (83.803)\tPrec@5 98.438 (99.428)\n",
      "TRAINING - Epoch: [14][80/391]\tTime 0.751 (0.807)\tData 0.002 (0.006)\tLoss 0.5144 (0.5465)\tPrec@1 84.375 (83.912)\tPrec@5 97.656 (99.431)\n",
      "TRAINING - Epoch: [14][90/391]\tTime 0.781 (0.804)\tData 0.002 (0.006)\tLoss 0.5279 (0.5472)\tPrec@1 81.250 (83.765)\tPrec@5 100.000 (99.425)\n",
      "TRAINING - Epoch: [14][100/391]\tTime 0.799 (0.803)\tData 0.002 (0.005)\tLoss 0.5460 (0.5462)\tPrec@1 82.031 (83.818)\tPrec@5 100.000 (99.420)\n",
      "TRAINING - Epoch: [14][110/391]\tTime 0.753 (0.800)\tData 0.002 (0.005)\tLoss 0.6702 (0.5498)\tPrec@1 80.469 (83.805)\tPrec@5 98.438 (99.374)\n",
      "TRAINING - Epoch: [14][120/391]\tTime 0.755 (0.798)\tData 0.002 (0.005)\tLoss 0.4846 (0.5505)\tPrec@1 85.156 (83.755)\tPrec@5 100.000 (99.374)\n",
      "TRAINING - Epoch: [14][130/391]\tTime 0.768 (0.797)\tData 0.002 (0.004)\tLoss 0.5621 (0.5516)\tPrec@1 84.375 (83.785)\tPrec@5 99.219 (99.368)\n",
      "TRAINING - Epoch: [14][140/391]\tTime 0.761 (0.795)\tData 0.002 (0.004)\tLoss 0.6193 (0.5553)\tPrec@1 84.375 (83.666)\tPrec@5 99.219 (99.324)\n",
      "TRAINING - Epoch: [14][150/391]\tTime 0.759 (0.795)\tData 0.002 (0.004)\tLoss 0.5316 (0.5547)\tPrec@1 87.500 (83.796)\tPrec@5 98.438 (99.317)\n",
      "TRAINING - Epoch: [14][160/391]\tTime 0.896 (0.795)\tData 0.002 (0.004)\tLoss 0.5522 (0.5556)\tPrec@1 84.375 (83.754)\tPrec@5 100.000 (99.330)\n",
      "TRAINING - Epoch: [14][170/391]\tTime 0.753 (0.794)\tData 0.002 (0.004)\tLoss 0.5109 (0.5551)\tPrec@1 86.719 (83.840)\tPrec@5 100.000 (99.328)\n",
      "TRAINING - Epoch: [14][180/391]\tTime 0.775 (0.794)\tData 0.002 (0.004)\tLoss 0.6266 (0.5578)\tPrec@1 79.688 (83.723)\tPrec@5 99.219 (99.335)\n",
      "TRAINING - Epoch: [14][190/391]\tTime 0.759 (0.793)\tData 0.002 (0.004)\tLoss 0.6214 (0.5604)\tPrec@1 79.688 (83.680)\tPrec@5 100.000 (99.337)\n",
      "TRAINING - Epoch: [14][200/391]\tTime 0.783 (0.793)\tData 0.002 (0.004)\tLoss 0.5347 (0.5615)\tPrec@1 85.156 (83.617)\tPrec@5 98.438 (99.320)\n",
      "TRAINING - Epoch: [14][210/391]\tTime 0.743 (0.791)\tData 0.002 (0.003)\tLoss 0.6853 (0.5644)\tPrec@1 78.906 (83.490)\tPrec@5 98.438 (99.304)\n",
      "TRAINING - Epoch: [14][220/391]\tTime 0.740 (0.791)\tData 0.002 (0.003)\tLoss 0.6435 (0.5656)\tPrec@1 81.250 (83.456)\tPrec@5 100.000 (99.314)\n",
      "TRAINING - Epoch: [14][230/391]\tTime 0.776 (0.790)\tData 0.002 (0.003)\tLoss 0.6700 (0.5671)\tPrec@1 80.469 (83.418)\tPrec@5 98.438 (99.310)\n",
      "TRAINING - Epoch: [14][240/391]\tTime 0.750 (0.790)\tData 0.002 (0.003)\tLoss 0.6248 (0.5691)\tPrec@1 80.469 (83.334)\tPrec@5 99.219 (99.300)\n",
      "TRAINING - Epoch: [14][250/391]\tTime 0.740 (0.789)\tData 0.002 (0.003)\tLoss 0.6393 (0.5691)\tPrec@1 81.250 (83.335)\tPrec@5 98.438 (99.284)\n",
      "TRAINING - Epoch: [14][260/391]\tTime 0.854 (0.789)\tData 0.002 (0.003)\tLoss 0.6655 (0.5681)\tPrec@1 77.344 (83.354)\tPrec@5 100.000 (99.288)\n",
      "TRAINING - Epoch: [14][270/391]\tTime 0.762 (0.789)\tData 0.002 (0.003)\tLoss 0.6190 (0.5681)\tPrec@1 81.250 (83.352)\tPrec@5 99.219 (99.279)\n",
      "TRAINING - Epoch: [14][280/391]\tTime 0.743 (0.789)\tData 0.002 (0.003)\tLoss 0.5425 (0.5685)\tPrec@1 84.375 (83.327)\tPrec@5 99.219 (99.285)\n",
      "TRAINING - Epoch: [14][290/391]\tTime 0.825 (0.788)\tData 0.002 (0.003)\tLoss 0.6399 (0.5702)\tPrec@1 83.594 (83.301)\tPrec@5 99.219 (99.262)\n",
      "TRAINING - Epoch: [14][300/391]\tTime 0.767 (0.788)\tData 0.002 (0.003)\tLoss 0.7022 (0.5699)\tPrec@1 80.469 (83.319)\tPrec@5 98.438 (99.252)\n",
      "TRAINING - Epoch: [14][310/391]\tTime 0.784 (0.788)\tData 0.002 (0.003)\tLoss 0.5381 (0.5708)\tPrec@1 83.594 (83.272)\tPrec@5 100.000 (99.246)\n",
      "TRAINING - Epoch: [14][320/391]\tTime 0.795 (0.788)\tData 0.002 (0.003)\tLoss 0.7790 (0.5706)\tPrec@1 75.781 (83.285)\tPrec@5 98.438 (99.250)\n",
      "TRAINING - Epoch: [14][330/391]\tTime 0.812 (0.788)\tData 0.002 (0.003)\tLoss 0.4376 (0.5717)\tPrec@1 87.500 (83.256)\tPrec@5 100.000 (99.264)\n",
      "TRAINING - Epoch: [14][340/391]\tTime 0.740 (0.787)\tData 0.002 (0.003)\tLoss 0.5571 (0.5719)\tPrec@1 84.375 (83.266)\tPrec@5 99.219 (99.267)\n",
      "TRAINING - Epoch: [14][350/391]\tTime 0.737 (0.787)\tData 0.002 (0.003)\tLoss 0.6782 (0.5709)\tPrec@1 79.688 (83.307)\tPrec@5 99.219 (99.272)\n",
      "TRAINING - Epoch: [14][360/391]\tTime 0.773 (0.787)\tData 0.002 (0.003)\tLoss 0.7053 (0.5706)\tPrec@1 80.469 (83.334)\tPrec@5 99.219 (99.273)\n",
      "TRAINING - Epoch: [14][370/391]\tTime 0.793 (0.787)\tData 0.002 (0.003)\tLoss 0.4675 (0.5703)\tPrec@1 86.719 (83.360)\tPrec@5 100.000 (99.276)\n",
      "TRAINING - Epoch: [14][380/391]\tTime 0.737 (0.786)\tData 0.002 (0.003)\tLoss 0.5335 (0.5703)\tPrec@1 82.031 (83.354)\tPrec@5 99.219 (99.270)\n",
      "TRAINING - Epoch: [14][390/391]\tTime 0.493 (0.785)\tData 0.002 (0.003)\tLoss 0.5587 (0.5704)\tPrec@1 85.000 (83.346)\tPrec@5 98.750 (99.264)\n",
      "EVALUATING - Epoch: [14][0/79]\tTime 0.829 (0.829)\tData 0.307 (0.307)\tLoss 0.6483 (0.6483)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "EVALUATING - Epoch: [14][10/79]\tTime 0.337 (0.391)\tData 0.001 (0.030)\tLoss 0.5103 (0.6478)\tPrec@1 83.594 (80.753)\tPrec@5 100.000 (99.219)\n",
      "EVALUATING - Epoch: [14][20/79]\tTime 0.326 (0.372)\tData 0.002 (0.016)\tLoss 0.6117 (0.6791)\tPrec@1 79.688 (80.060)\tPrec@5 99.219 (99.070)\n",
      "EVALUATING - Epoch: [14][30/79]\tTime 0.329 (0.367)\tData 0.002 (0.012)\tLoss 0.5435 (0.6731)\tPrec@1 86.719 (80.645)\tPrec@5 100.000 (99.017)\n",
      "EVALUATING - Epoch: [14][40/79]\tTime 0.378 (0.363)\tData 0.002 (0.009)\tLoss 0.7200 (0.6687)\tPrec@1 78.125 (80.774)\tPrec@5 98.438 (99.009)\n",
      "EVALUATING - Epoch: [14][50/79]\tTime 0.327 (0.359)\tData 0.002 (0.008)\tLoss 0.6430 (0.6675)\tPrec@1 82.031 (80.928)\tPrec@5 98.438 (99.035)\n",
      "EVALUATING - Epoch: [14][60/79]\tTime 0.363 (0.356)\tData 0.002 (0.007)\tLoss 0.7041 (0.6647)\tPrec@1 78.906 (81.045)\tPrec@5 99.219 (99.039)\n",
      "EVALUATING - Epoch: [14][70/79]\tTime 0.394 (0.354)\tData 0.002 (0.006)\tLoss 0.5805 (0.6641)\tPrec@1 85.156 (81.206)\tPrec@5 100.000 (99.021)\n",
      "\n",
      " Epoch: 15\tTraining Loss 0.5704 \tTraining Prec@1 83.346 \tTraining Prec@5 99.264 \tValidation Loss 0.6615 \tValidation Prec@1 81.200 \tValidation Prec@5 99.060 \n",
      "\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "TRAINING - Epoch: [15][0/391]\tTime 1.563 (1.563)\tData 0.335 (0.335)\tLoss 0.4389 (0.4389)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "TRAINING - Epoch: [15][10/391]\tTime 0.761 (0.871)\tData 0.002 (0.033)\tLoss 0.4404 (0.5583)\tPrec@1 87.500 (83.452)\tPrec@5 100.000 (99.219)\n",
      "TRAINING - Epoch: [15][20/391]\tTime 0.783 (0.832)\tData 0.002 (0.018)\tLoss 0.4119 (0.5385)\tPrec@1 88.281 (84.152)\tPrec@5 100.000 (99.405)\n",
      "TRAINING - Epoch: [15][30/391]\tTime 0.744 (0.813)\tData 0.002 (0.013)\tLoss 0.4596 (0.5444)\tPrec@1 85.156 (83.770)\tPrec@5 100.000 (99.345)\n",
      "TRAINING - Epoch: [15][40/391]\tTime 0.752 (0.806)\tData 0.002 (0.010)\tLoss 0.4606 (0.5412)\tPrec@1 85.938 (83.861)\tPrec@5 99.219 (99.390)\n",
      "TRAINING - Epoch: [15][50/391]\tTime 0.784 (0.800)\tData 0.002 (0.009)\tLoss 0.5500 (0.5462)\tPrec@1 85.938 (83.747)\tPrec@5 99.219 (99.403)\n",
      "TRAINING - Epoch: [15][60/391]\tTime 0.819 (0.797)\tData 0.002 (0.007)\tLoss 0.5601 (0.5488)\tPrec@1 86.719 (83.773)\tPrec@5 99.219 (99.360)\n",
      "TRAINING - Epoch: [15][70/391]\tTime 0.736 (0.793)\tData 0.002 (0.007)\tLoss 0.4851 (0.5510)\tPrec@1 85.938 (83.715)\tPrec@5 99.219 (99.362)\n",
      "TRAINING - Epoch: [15][80/391]\tTime 0.781 (0.791)\tData 0.002 (0.006)\tLoss 0.5559 (0.5453)\tPrec@1 85.938 (83.893)\tPrec@5 97.656 (99.354)\n",
      "TRAINING - Epoch: [15][90/391]\tTime 0.744 (0.791)\tData 0.002 (0.006)\tLoss 0.6550 (0.5433)\tPrec@1 78.125 (83.971)\tPrec@5 100.000 (99.399)\n",
      "TRAINING - Epoch: [15][100/391]\tTime 0.755 (0.790)\tData 0.002 (0.005)\tLoss 0.6476 (0.5490)\tPrec@1 82.031 (83.795)\tPrec@5 98.438 (99.366)\n",
      "TRAINING - Epoch: [15][110/391]\tTime 0.803 (0.789)\tData 0.002 (0.005)\tLoss 0.3990 (0.5488)\tPrec@1 91.406 (83.826)\tPrec@5 99.219 (99.352)\n",
      "TRAINING - Epoch: [15][120/391]\tTime 0.763 (0.787)\tData 0.002 (0.005)\tLoss 0.6277 (0.5499)\tPrec@1 82.812 (83.820)\tPrec@5 100.000 (99.329)\n",
      "TRAINING - Epoch: [15][130/391]\tTime 0.748 (0.788)\tData 0.002 (0.004)\tLoss 0.4670 (0.5482)\tPrec@1 86.719 (83.898)\tPrec@5 99.219 (99.332)\n",
      "TRAINING - Epoch: [15][140/391]\tTime 0.760 (0.788)\tData 0.002 (0.004)\tLoss 0.4691 (0.5500)\tPrec@1 87.500 (83.876)\tPrec@5 100.000 (99.313)\n",
      "TRAINING - Epoch: [15][150/391]\tTime 0.791 (0.787)\tData 0.002 (0.004)\tLoss 0.6311 (0.5485)\tPrec@1 82.031 (83.956)\tPrec@5 99.219 (99.327)\n",
      "TRAINING - Epoch: [15][160/391]\tTime 0.752 (0.786)\tData 0.002 (0.004)\tLoss 0.5542 (0.5515)\tPrec@1 85.156 (83.904)\tPrec@5 98.438 (99.287)\n",
      "TRAINING - Epoch: [15][170/391]\tTime 0.759 (0.786)\tData 0.002 (0.004)\tLoss 0.5662 (0.5523)\tPrec@1 81.250 (83.877)\tPrec@5 99.219 (99.283)\n",
      "TRAINING - Epoch: [15][180/391]\tTime 0.767 (0.786)\tData 0.002 (0.004)\tLoss 0.5129 (0.5518)\tPrec@1 85.156 (83.844)\tPrec@5 99.219 (99.296)\n",
      "TRAINING - Epoch: [15][190/391]\tTime 0.762 (0.785)\tData 0.002 (0.004)\tLoss 0.5292 (0.5501)\tPrec@1 85.156 (83.917)\tPrec@5 100.000 (99.313)\n",
      "TRAINING - Epoch: [15][200/391]\tTime 0.748 (0.784)\tData 0.002 (0.004)\tLoss 0.6187 (0.5507)\tPrec@1 80.469 (83.874)\tPrec@5 100.000 (99.316)\n",
      "TRAINING - Epoch: [15][210/391]\tTime 0.771 (0.784)\tData 0.002 (0.004)\tLoss 0.4453 (0.5511)\tPrec@1 88.281 (83.883)\tPrec@5 99.219 (99.315)\n",
      "TRAINING - Epoch: [15][220/391]\tTime 0.741 (0.784)\tData 0.002 (0.003)\tLoss 0.6634 (0.5508)\tPrec@1 82.031 (83.898)\tPrec@5 97.656 (99.289)\n",
      "TRAINING - Epoch: [15][230/391]\tTime 0.810 (0.784)\tData 0.003 (0.003)\tLoss 0.4920 (0.5520)\tPrec@1 85.938 (83.854)\tPrec@5 99.219 (99.280)\n",
      "TRAINING - Epoch: [15][240/391]\tTime 0.809 (0.784)\tData 0.002 (0.003)\tLoss 0.5672 (0.5540)\tPrec@1 84.375 (83.769)\tPrec@5 100.000 (99.293)\n",
      "TRAINING - Epoch: [15][250/391]\tTime 0.752 (0.783)\tData 0.002 (0.003)\tLoss 0.3908 (0.5541)\tPrec@1 90.625 (83.759)\tPrec@5 99.219 (99.290)\n",
      "TRAINING - Epoch: [15][260/391]\tTime 0.740 (0.783)\tData 0.002 (0.003)\tLoss 0.6671 (0.5559)\tPrec@1 76.562 (83.731)\tPrec@5 99.219 (99.291)\n",
      "TRAINING - Epoch: [15][270/391]\tTime 0.826 (0.783)\tData 0.002 (0.003)\tLoss 0.6487 (0.5576)\tPrec@1 81.250 (83.674)\tPrec@5 99.219 (99.276)\n",
      "TRAINING - Epoch: [15][280/391]\tTime 0.739 (0.783)\tData 0.002 (0.003)\tLoss 0.6733 (0.5585)\tPrec@1 80.469 (83.644)\tPrec@5 99.219 (99.277)\n",
      "TRAINING - Epoch: [15][290/391]\tTime 0.767 (0.783)\tData 0.002 (0.003)\tLoss 0.3520 (0.5574)\tPrec@1 92.969 (83.715)\tPrec@5 100.000 (99.289)\n",
      "TRAINING - Epoch: [15][300/391]\tTime 0.764 (0.783)\tData 0.002 (0.003)\tLoss 0.5241 (0.5559)\tPrec@1 82.812 (83.775)\tPrec@5 99.219 (99.286)\n",
      "TRAINING - Epoch: [15][310/391]\tTime 0.783 (0.783)\tData 0.002 (0.003)\tLoss 0.5416 (0.5560)\tPrec@1 84.375 (83.747)\tPrec@5 100.000 (99.294)\n",
      "TRAINING - Epoch: [15][320/391]\tTime 0.765 (0.783)\tData 0.002 (0.003)\tLoss 0.5301 (0.5567)\tPrec@1 83.594 (83.701)\tPrec@5 100.000 (99.297)\n",
      "TRAINING - Epoch: [15][330/391]\tTime 0.746 (0.783)\tData 0.002 (0.003)\tLoss 0.6838 (0.5564)\tPrec@1 79.688 (83.714)\tPrec@5 99.219 (99.294)\n",
      "TRAINING - Epoch: [15][340/391]\tTime 0.741 (0.784)\tData 0.002 (0.003)\tLoss 0.6727 (0.5574)\tPrec@1 81.250 (83.690)\tPrec@5 98.438 (99.290)\n",
      "TRAINING - Epoch: [15][350/391]\tTime 0.788 (0.784)\tData 0.002 (0.003)\tLoss 0.5222 (0.5581)\tPrec@1 84.375 (83.669)\tPrec@5 97.656 (99.268)\n",
      "TRAINING - Epoch: [15][360/391]\tTime 0.750 (0.784)\tData 0.002 (0.003)\tLoss 0.7926 (0.5581)\tPrec@1 76.562 (83.663)\tPrec@5 100.000 (99.279)\n",
      "TRAINING - Epoch: [15][370/391]\tTime 0.749 (0.784)\tData 0.002 (0.003)\tLoss 0.6821 (0.5589)\tPrec@1 78.125 (83.646)\tPrec@5 97.656 (99.269)\n",
      "TRAINING - Epoch: [15][380/391]\tTime 0.779 (0.784)\tData 0.002 (0.003)\tLoss 0.8029 (0.5597)\tPrec@1 74.219 (83.618)\tPrec@5 99.219 (99.268)\n",
      "TRAINING - Epoch: [15][390/391]\tTime 0.503 (0.782)\tData 0.002 (0.003)\tLoss 0.5334 (0.5599)\tPrec@1 85.000 (83.620)\tPrec@5 97.500 (99.260)\n",
      "EVALUATING - Epoch: [15][0/79]\tTime 0.856 (0.856)\tData 0.341 (0.341)\tLoss 0.5498 (0.5498)\tPrec@1 82.031 (82.031)\tPrec@5 99.219 (99.219)\n",
      "EVALUATING - Epoch: [15][10/79]\tTime 0.334 (0.388)\tData 0.001 (0.034)\tLoss 0.6458 (0.6335)\tPrec@1 80.469 (82.670)\tPrec@5 100.000 (99.432)\n",
      "EVALUATING - Epoch: [15][20/79]\tTime 0.416 (0.371)\tData 0.002 (0.018)\tLoss 0.5873 (0.6609)\tPrec@1 83.594 (82.143)\tPrec@5 98.438 (98.921)\n",
      "EVALUATING - Epoch: [15][30/79]\tTime 0.357 (0.365)\tData 0.002 (0.013)\tLoss 0.5535 (0.6539)\tPrec@1 81.250 (81.628)\tPrec@5 100.000 (98.916)\n",
      "EVALUATING - Epoch: [15][40/79]\tTime 0.347 (0.361)\tData 0.002 (0.010)\tLoss 0.7568 (0.6450)\tPrec@1 77.344 (81.745)\tPrec@5 98.438 (98.952)\n",
      "EVALUATING - Epoch: [15][50/79]\tTime 0.341 (0.360)\tData 0.002 (0.009)\tLoss 0.7173 (0.6430)\tPrec@1 78.125 (81.817)\tPrec@5 96.875 (99.020)\n",
      "EVALUATING - Epoch: [15][60/79]\tTime 0.342 (0.358)\tData 0.002 (0.008)\tLoss 0.6184 (0.6409)\tPrec@1 81.250 (81.839)\tPrec@5 99.219 (99.052)\n",
      "EVALUATING - Epoch: [15][70/79]\tTime 0.326 (0.355)\tData 0.002 (0.007)\tLoss 0.4622 (0.6449)\tPrec@1 87.500 (81.789)\tPrec@5 100.000 (98.988)\n",
      "\n",
      " Epoch: 16\tTraining Loss 0.5599 \tTraining Prec@1 83.620 \tTraining Prec@5 99.260 \tValidation Loss 0.6447 \tValidation Prec@1 81.870 \tValidation Prec@5 98.990 \n",
      "\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "TRAINING - Epoch: [16][0/391]\tTime 1.618 (1.618)\tData 0.322 (0.322)\tLoss 0.4413 (0.4413)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (100.000)\n",
      "TRAINING - Epoch: [16][10/391]\tTime 0.780 (0.859)\tData 0.001 (0.032)\tLoss 0.5234 (0.5919)\tPrec@1 84.375 (82.884)\tPrec@5 99.219 (99.219)\n",
      "TRAINING - Epoch: [16][20/391]\tTime 0.769 (0.823)\tData 0.002 (0.017)\tLoss 0.5030 (0.5804)\tPrec@1 85.938 (83.110)\tPrec@5 100.000 (99.330)\n",
      "TRAINING - Epoch: [16][30/391]\tTime 0.763 (0.811)\tData 0.002 (0.012)\tLoss 0.5141 (0.5598)\tPrec@1 85.156 (83.795)\tPrec@5 99.219 (99.395)\n",
      "TRAINING - Epoch: [16][40/391]\tTime 0.787 (0.803)\tData 0.002 (0.010)\tLoss 0.5912 (0.5642)\tPrec@1 84.375 (83.632)\tPrec@5 99.219 (99.276)\n",
      "TRAINING - Epoch: [16][50/391]\tTime 0.737 (0.797)\tData 0.002 (0.008)\tLoss 0.4979 (0.5656)\tPrec@1 87.500 (83.502)\tPrec@5 99.219 (99.280)\n",
      "TRAINING - Epoch: [16][60/391]\tTime 0.782 (0.797)\tData 0.002 (0.007)\tLoss 0.5542 (0.5611)\tPrec@1 82.812 (83.747)\tPrec@5 100.000 (99.308)\n",
      "TRAINING - Epoch: [16][70/391]\tTime 0.809 (0.795)\tData 0.004 (0.006)\tLoss 0.5431 (0.5556)\tPrec@1 82.812 (83.913)\tPrec@5 99.219 (99.219)\n",
      "TRAINING - Epoch: [16][80/391]\tTime 0.797 (0.793)\tData 0.002 (0.006)\tLoss 0.5025 (0.5541)\tPrec@1 84.375 (83.922)\tPrec@5 97.656 (99.190)\n",
      "TRAINING - Epoch: [16][90/391]\tTime 0.814 (0.792)\tData 0.002 (0.005)\tLoss 0.4645 (0.5535)\tPrec@1 88.281 (83.937)\tPrec@5 99.219 (99.184)\n",
      "TRAINING - Epoch: [16][100/391]\tTime 0.787 (0.791)\tData 0.002 (0.005)\tLoss 0.4931 (0.5549)\tPrec@1 83.594 (83.957)\tPrec@5 100.000 (99.157)\n",
      "TRAINING - Epoch: [16][110/391]\tTime 0.766 (0.790)\tData 0.002 (0.005)\tLoss 0.5734 (0.5533)\tPrec@1 83.594 (84.016)\tPrec@5 100.000 (99.177)\n",
      "TRAINING - Epoch: [16][120/391]\tTime 0.743 (0.790)\tData 0.002 (0.005)\tLoss 0.6381 (0.5513)\tPrec@1 80.469 (84.084)\tPrec@5 99.219 (99.186)\n",
      "TRAINING - Epoch: [16][130/391]\tTime 0.766 (0.789)\tData 0.002 (0.004)\tLoss 0.6149 (0.5563)\tPrec@1 80.469 (83.892)\tPrec@5 100.000 (99.195)\n",
      "TRAINING - Epoch: [16][140/391]\tTime 0.801 (0.789)\tData 0.002 (0.004)\tLoss 0.6916 (0.5585)\tPrec@1 79.688 (83.754)\tPrec@5 100.000 (99.169)\n",
      "TRAINING - Epoch: [16][150/391]\tTime 0.756 (0.789)\tData 0.002 (0.004)\tLoss 0.4610 (0.5586)\tPrec@1 88.281 (83.759)\tPrec@5 100.000 (99.183)\n",
      "TRAINING - Epoch: [16][160/391]\tTime 0.761 (0.788)\tData 0.002 (0.004)\tLoss 0.5679 (0.5591)\tPrec@1 85.156 (83.827)\tPrec@5 99.219 (99.185)\n",
      "TRAINING - Epoch: [16][170/391]\tTime 0.768 (0.788)\tData 0.002 (0.004)\tLoss 0.6106 (0.5580)\tPrec@1 82.031 (83.909)\tPrec@5 99.219 (99.200)\n",
      "TRAINING - Epoch: [16][180/391]\tTime 0.768 (0.788)\tData 0.002 (0.004)\tLoss 0.4340 (0.5562)\tPrec@1 87.500 (84.012)\tPrec@5 100.000 (99.219)\n",
      "TRAINING - Epoch: [16][190/391]\tTime 0.756 (0.788)\tData 0.002 (0.004)\tLoss 0.5551 (0.5561)\tPrec@1 85.156 (84.019)\tPrec@5 100.000 (99.227)\n",
      "TRAINING - Epoch: [16][200/391]\tTime 0.758 (0.787)\tData 0.002 (0.004)\tLoss 0.5282 (0.5539)\tPrec@1 83.594 (84.095)\tPrec@5 100.000 (99.234)\n",
      "TRAINING - Epoch: [16][210/391]\tTime 0.809 (0.787)\tData 0.002 (0.003)\tLoss 0.4291 (0.5525)\tPrec@1 88.281 (84.145)\tPrec@5 99.219 (99.241)\n",
      "TRAINING - Epoch: [16][220/391]\tTime 0.804 (0.787)\tData 0.002 (0.003)\tLoss 0.5655 (0.5526)\tPrec@1 81.250 (84.135)\tPrec@5 100.000 (99.254)\n",
      "TRAINING - Epoch: [16][230/391]\tTime 0.766 (0.786)\tData 0.002 (0.003)\tLoss 0.5799 (0.5520)\tPrec@1 81.250 (84.125)\tPrec@5 100.000 (99.253)\n",
      "TRAINING - Epoch: [16][240/391]\tTime 0.771 (0.786)\tData 0.002 (0.003)\tLoss 0.3968 (0.5491)\tPrec@1 89.844 (84.203)\tPrec@5 100.000 (99.277)\n",
      "TRAINING - Epoch: [16][250/391]\tTime 0.746 (0.786)\tData 0.002 (0.003)\tLoss 0.6442 (0.5513)\tPrec@1 81.250 (84.120)\tPrec@5 100.000 (99.284)\n",
      "TRAINING - Epoch: [16][260/391]\tTime 0.788 (0.786)\tData 0.002 (0.003)\tLoss 0.4685 (0.5513)\tPrec@1 88.281 (84.121)\tPrec@5 98.438 (99.294)\n",
      "TRAINING - Epoch: [16][270/391]\tTime 0.787 (0.785)\tData 0.002 (0.003)\tLoss 0.4821 (0.5520)\tPrec@1 85.156 (84.090)\tPrec@5 100.000 (99.288)\n",
      "TRAINING - Epoch: [16][280/391]\tTime 0.762 (0.785)\tData 0.002 (0.003)\tLoss 0.4468 (0.5512)\tPrec@1 87.500 (84.122)\tPrec@5 99.219 (99.297)\n",
      "TRAINING - Epoch: [16][290/391]\tTime 0.754 (0.784)\tData 0.002 (0.003)\tLoss 0.6191 (0.5530)\tPrec@1 82.812 (84.085)\tPrec@5 98.438 (99.294)\n",
      "TRAINING - Epoch: [16][300/391]\tTime 0.767 (0.785)\tData 0.002 (0.003)\tLoss 0.6919 (0.5542)\tPrec@1 78.125 (84.030)\tPrec@5 96.875 (99.291)\n",
      "TRAINING - Epoch: [16][310/391]\tTime 0.760 (0.784)\tData 0.002 (0.003)\tLoss 0.4344 (0.5551)\tPrec@1 87.500 (83.961)\tPrec@5 99.219 (99.279)\n",
      "TRAINING - Epoch: [16][320/391]\tTime 0.748 (0.784)\tData 0.002 (0.003)\tLoss 0.6225 (0.5549)\tPrec@1 76.562 (83.956)\tPrec@5 98.438 (99.272)\n",
      "TRAINING - Epoch: [16][330/391]\tTime 0.771 (0.784)\tData 0.002 (0.003)\tLoss 0.5450 (0.5535)\tPrec@1 82.812 (84.007)\tPrec@5 99.219 (99.275)\n",
      "TRAINING - Epoch: [16][340/391]\tTime 0.770 (0.784)\tData 0.002 (0.003)\tLoss 0.5122 (0.5523)\tPrec@1 85.156 (84.031)\tPrec@5 100.000 (99.283)\n",
      "TRAINING - Epoch: [16][350/391]\tTime 0.747 (0.784)\tData 0.002 (0.003)\tLoss 0.6961 (0.5531)\tPrec@1 77.344 (83.968)\tPrec@5 100.000 (99.292)\n",
      "TRAINING - Epoch: [16][360/391]\tTime 0.757 (0.784)\tData 0.002 (0.003)\tLoss 0.5286 (0.5529)\tPrec@1 85.938 (83.949)\tPrec@5 100.000 (99.297)\n",
      "TRAINING - Epoch: [16][370/391]\tTime 0.812 (0.783)\tData 0.002 (0.003)\tLoss 0.5869 (0.5525)\tPrec@1 87.500 (83.977)\tPrec@5 99.219 (99.303)\n",
      "TRAINING - Epoch: [16][380/391]\tTime 0.748 (0.783)\tData 0.002 (0.003)\tLoss 0.5475 (0.5532)\tPrec@1 81.250 (83.932)\tPrec@5 99.219 (99.295)\n",
      "TRAINING - Epoch: [16][390/391]\tTime 0.513 (0.782)\tData 0.002 (0.003)\tLoss 0.4700 (0.5529)\tPrec@1 88.750 (83.950)\tPrec@5 100.000 (99.290)\n",
      "EVALUATING - Epoch: [16][0/79]\tTime 0.876 (0.876)\tData 0.295 (0.295)\tLoss 0.4870 (0.4870)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "EVALUATING - Epoch: [16][10/79]\tTime 0.328 (0.401)\tData 0.001 (0.029)\tLoss 0.6805 (0.6356)\tPrec@1 81.250 (82.102)\tPrec@5 100.000 (99.148)\n",
      "EVALUATING - Epoch: [16][20/79]\tTime 0.340 (0.373)\tData 0.002 (0.016)\tLoss 0.5705 (0.6553)\tPrec@1 84.375 (81.324)\tPrec@5 99.219 (98.958)\n",
      "EVALUATING - Epoch: [16][30/79]\tTime 0.344 (0.366)\tData 0.002 (0.011)\tLoss 0.5557 (0.6522)\tPrec@1 84.375 (81.401)\tPrec@5 100.000 (99.068)\n",
      "EVALUATING - Epoch: [16][40/79]\tTime 0.329 (0.362)\tData 0.002 (0.009)\tLoss 0.7409 (0.6572)\tPrec@1 76.562 (81.174)\tPrec@5 99.219 (98.990)\n",
      "EVALUATING - Epoch: [16][50/79]\tTime 0.341 (0.359)\tData 0.002 (0.008)\tLoss 0.6547 (0.6560)\tPrec@1 84.375 (81.112)\tPrec@5 97.656 (99.050)\n",
      "EVALUATING - Epoch: [16][60/79]\tTime 0.329 (0.357)\tData 0.002 (0.007)\tLoss 0.6592 (0.6525)\tPrec@1 81.250 (81.122)\tPrec@5 100.000 (99.116)\n",
      "EVALUATING - Epoch: [16][70/79]\tTime 0.329 (0.355)\tData 0.002 (0.006)\tLoss 0.5525 (0.6515)\tPrec@1 82.812 (81.030)\tPrec@5 100.000 (99.109)\n",
      "\n",
      " Epoch: 17\tTraining Loss 0.5529 \tTraining Prec@1 83.950 \tTraining Prec@5 99.290 \tValidation Loss 0.6499 \tValidation Prec@1 81.130 \tValidation Prec@5 99.090 \n",
      "\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "TRAINING - Epoch: [17][0/391]\tTime 1.554 (1.554)\tData 0.245 (0.245)\tLoss 0.5019 (0.5019)\tPrec@1 83.594 (83.594)\tPrec@5 100.000 (100.000)\n",
      "TRAINING - Epoch: [17][10/391]\tTime 0.761 (0.855)\tData 0.001 (0.024)\tLoss 0.7258 (0.5656)\tPrec@1 75.781 (82.955)\tPrec@5 100.000 (99.645)\n",
      "TRAINING - Epoch: [17][20/391]\tTime 0.755 (0.820)\tData 0.002 (0.014)\tLoss 0.5692 (0.5529)\tPrec@1 82.031 (83.408)\tPrec@5 100.000 (99.628)\n",
      "TRAINING - Epoch: [17][30/391]\tTime 0.807 (0.809)\tData 0.002 (0.010)\tLoss 0.5658 (0.5349)\tPrec@1 80.469 (84.224)\tPrec@5 98.438 (99.521)\n",
      "TRAINING - Epoch: [17][40/391]\tTime 0.742 (0.801)\tData 0.002 (0.008)\tLoss 0.5716 (0.5330)\tPrec@1 85.938 (84.375)\tPrec@5 100.000 (99.524)\n",
      "TRAINING - Epoch: [17][50/391]\tTime 0.739 (0.797)\tData 0.002 (0.007)\tLoss 0.5133 (0.5294)\tPrec@1 81.250 (84.482)\tPrec@5 100.000 (99.540)\n",
      "TRAINING - Epoch: [17][60/391]\tTime 0.787 (0.794)\tData 0.002 (0.006)\tLoss 0.6887 (0.5330)\tPrec@1 79.688 (84.452)\tPrec@5 99.219 (99.475)\n",
      "TRAINING - Epoch: [17][70/391]\tTime 0.747 (0.791)\tData 0.002 (0.005)\tLoss 0.6266 (0.5366)\tPrec@1 81.250 (84.331)\tPrec@5 98.438 (99.439)\n",
      "TRAINING - Epoch: [17][80/391]\tTime 0.751 (0.789)\tData 0.002 (0.005)\tLoss 0.5251 (0.5337)\tPrec@1 85.938 (84.471)\tPrec@5 100.000 (99.460)\n",
      "TRAINING - Epoch: [17][90/391]\tTime 0.791 (0.787)\tData 0.002 (0.005)\tLoss 0.4740 (0.5312)\tPrec@1 86.719 (84.624)\tPrec@5 100.000 (99.485)\n",
      "TRAINING - Epoch: [17][100/391]\tTime 0.758 (0.787)\tData 0.002 (0.004)\tLoss 0.5538 (0.5303)\tPrec@1 83.594 (84.661)\tPrec@5 99.219 (99.489)\n",
      "TRAINING - Epoch: [17][110/391]\tTime 0.746 (0.787)\tData 0.002 (0.004)\tLoss 0.4624 (0.5291)\tPrec@1 87.500 (84.720)\tPrec@5 100.000 (99.514)\n",
      "TRAINING - Epoch: [17][120/391]\tTime 0.789 (0.788)\tData 0.002 (0.004)\tLoss 0.5389 (0.5278)\tPrec@1 85.156 (84.788)\tPrec@5 100.000 (99.490)\n",
      "TRAINING - Epoch: [17][130/391]\tTime 0.810 (0.787)\tData 0.002 (0.004)\tLoss 0.4972 (0.5279)\tPrec@1 89.062 (84.840)\tPrec@5 100.000 (99.481)\n",
      "TRAINING - Epoch: [17][140/391]\tTime 0.745 (0.786)\tData 0.002 (0.004)\tLoss 0.5875 (0.5298)\tPrec@1 80.469 (84.746)\tPrec@5 99.219 (99.451)\n",
      "TRAINING - Epoch: [17][150/391]\tTime 0.764 (0.786)\tData 0.002 (0.004)\tLoss 0.5492 (0.5336)\tPrec@1 86.719 (84.597)\tPrec@5 98.438 (99.436)\n",
      "TRAINING - Epoch: [17][160/391]\tTime 0.837 (0.785)\tData 0.002 (0.003)\tLoss 0.5472 (0.5352)\tPrec@1 84.375 (84.530)\tPrec@5 99.219 (99.437)\n",
      "TRAINING - Epoch: [17][170/391]\tTime 0.755 (0.784)\tData 0.002 (0.003)\tLoss 0.5460 (0.5379)\tPrec@1 85.938 (84.393)\tPrec@5 100.000 (99.452)\n",
      "TRAINING - Epoch: [17][180/391]\tTime 0.787 (0.784)\tData 0.002 (0.003)\tLoss 0.4870 (0.5406)\tPrec@1 89.062 (84.297)\tPrec@5 98.438 (99.435)\n",
      "TRAINING - Epoch: [17][190/391]\tTime 0.779 (0.783)\tData 0.002 (0.003)\tLoss 0.5306 (0.5400)\tPrec@1 84.375 (84.265)\tPrec@5 98.438 (99.444)\n",
      "TRAINING - Epoch: [17][200/391]\tTime 0.762 (0.783)\tData 0.002 (0.003)\tLoss 0.4491 (0.5399)\tPrec@1 86.719 (84.262)\tPrec@5 100.000 (99.417)\n",
      "TRAINING - Epoch: [17][210/391]\tTime 0.750 (0.782)\tData 0.002 (0.003)\tLoss 0.6176 (0.5408)\tPrec@1 84.375 (84.271)\tPrec@5 100.000 (99.415)\n",
      "TRAINING - Epoch: [17][220/391]\tTime 0.767 (0.781)\tData 0.002 (0.003)\tLoss 0.5268 (0.5402)\tPrec@1 84.375 (84.269)\tPrec@5 98.438 (99.417)\n",
      "TRAINING - Epoch: [17][230/391]\tTime 0.754 (0.781)\tData 0.003 (0.003)\tLoss 0.6127 (0.5408)\tPrec@1 82.812 (84.267)\tPrec@5 99.219 (99.401)\n",
      "TRAINING - Epoch: [17][240/391]\tTime 0.746 (0.781)\tData 0.002 (0.003)\tLoss 0.4759 (0.5399)\tPrec@1 84.375 (84.281)\tPrec@5 100.000 (99.400)\n",
      "TRAINING - Epoch: [17][250/391]\tTime 0.755 (0.780)\tData 0.002 (0.003)\tLoss 0.5412 (0.5380)\tPrec@1 85.156 (84.366)\tPrec@5 98.438 (99.406)\n",
      "TRAINING - Epoch: [17][260/391]\tTime 0.761 (0.780)\tData 0.002 (0.003)\tLoss 0.5149 (0.5388)\tPrec@1 85.938 (84.345)\tPrec@5 100.000 (99.404)\n",
      "TRAINING - Epoch: [17][270/391]\tTime 0.831 (0.780)\tData 0.002 (0.003)\tLoss 0.7548 (0.5405)\tPrec@1 77.344 (84.277)\tPrec@5 98.438 (99.397)\n",
      "TRAINING - Epoch: [17][280/391]\tTime 0.774 (0.780)\tData 0.002 (0.003)\tLoss 0.6590 (0.5403)\tPrec@1 78.906 (84.294)\tPrec@5 98.438 (99.402)\n",
      "TRAINING - Epoch: [17][290/391]\tTime 0.763 (0.780)\tData 0.002 (0.003)\tLoss 0.5443 (0.5415)\tPrec@1 82.031 (84.254)\tPrec@5 100.000 (99.391)\n",
      "TRAINING - Epoch: [17][300/391]\tTime 0.742 (0.779)\tData 0.002 (0.003)\tLoss 0.5255 (0.5439)\tPrec@1 85.156 (84.178)\tPrec@5 100.000 (99.372)\n",
      "TRAINING - Epoch: [17][310/391]\tTime 0.803 (0.779)\tData 0.002 (0.003)\tLoss 0.4959 (0.5447)\tPrec@1 87.500 (84.149)\tPrec@5 98.438 (99.369)\n",
      "TRAINING - Epoch: [17][320/391]\tTime 0.754 (0.779)\tData 0.002 (0.003)\tLoss 0.6487 (0.5464)\tPrec@1 79.688 (84.105)\tPrec@5 100.000 (99.370)\n",
      "TRAINING - Epoch: [17][330/391]\tTime 0.785 (0.778)\tData 0.002 (0.003)\tLoss 0.4985 (0.5468)\tPrec@1 84.375 (84.080)\tPrec@5 99.219 (99.353)\n",
      "TRAINING - Epoch: [17][340/391]\tTime 0.748 (0.778)\tData 0.002 (0.003)\tLoss 0.6206 (0.5488)\tPrec@1 78.906 (84.036)\tPrec@5 100.000 (99.342)\n",
      "TRAINING - Epoch: [17][350/391]\tTime 0.762 (0.778)\tData 0.002 (0.003)\tLoss 0.4341 (0.5482)\tPrec@1 89.844 (84.057)\tPrec@5 100.000 (99.348)\n",
      "TRAINING - Epoch: [17][360/391]\tTime 0.797 (0.778)\tData 0.002 (0.003)\tLoss 0.5099 (0.5480)\tPrec@1 89.062 (84.094)\tPrec@5 98.438 (99.342)\n",
      "TRAINING - Epoch: [17][370/391]\tTime 0.741 (0.778)\tData 0.002 (0.003)\tLoss 0.5625 (0.5469)\tPrec@1 86.719 (84.135)\tPrec@5 99.219 (99.347)\n",
      "TRAINING - Epoch: [17][380/391]\tTime 0.733 (0.778)\tData 0.002 (0.003)\tLoss 0.6274 (0.5478)\tPrec@1 80.469 (84.076)\tPrec@5 99.219 (99.354)\n",
      "TRAINING - Epoch: [17][390/391]\tTime 0.539 (0.777)\tData 0.002 (0.003)\tLoss 0.4072 (0.5472)\tPrec@1 90.000 (84.112)\tPrec@5 100.000 (99.352)\n",
      "EVALUATING - Epoch: [17][0/79]\tTime 0.811 (0.811)\tData 0.265 (0.265)\tLoss 0.5067 (0.5067)\tPrec@1 83.594 (83.594)\tPrec@5 99.219 (99.219)\n",
      "EVALUATING - Epoch: [17][10/79]\tTime 0.329 (0.381)\tData 0.001 (0.026)\tLoss 0.5673 (0.5800)\tPrec@1 81.250 (83.665)\tPrec@5 98.438 (99.148)\n",
      "EVALUATING - Epoch: [17][20/79]\tTime 0.328 (0.361)\tData 0.002 (0.014)\tLoss 0.5542 (0.6038)\tPrec@1 84.375 (83.519)\tPrec@5 100.000 (99.144)\n",
      "EVALUATING - Epoch: [17][30/79]\tTime 0.340 (0.354)\tData 0.002 (0.010)\tLoss 0.4823 (0.6034)\tPrec@1 86.719 (83.569)\tPrec@5 100.000 (99.017)\n",
      "EVALUATING - Epoch: [17][40/79]\tTime 0.328 (0.350)\tData 0.002 (0.008)\tLoss 0.8712 (0.6094)\tPrec@1 73.438 (83.232)\tPrec@5 98.438 (99.009)\n",
      "EVALUATING - Epoch: [17][50/79]\tTime 0.330 (0.349)\tData 0.002 (0.007)\tLoss 0.7250 (0.6103)\tPrec@1 80.469 (83.272)\tPrec@5 99.219 (99.081)\n",
      "EVALUATING - Epoch: [17][60/79]\tTime 0.325 (0.348)\tData 0.002 (0.006)\tLoss 0.6016 (0.6082)\tPrec@1 80.469 (83.184)\tPrec@5 99.219 (99.155)\n",
      "EVALUATING - Epoch: [17][70/79]\tTime 0.321 (0.345)\tData 0.002 (0.006)\tLoss 0.5584 (0.6075)\tPrec@1 85.156 (83.154)\tPrec@5 100.000 (99.186)\n",
      "\n",
      " Epoch: 18\tTraining Loss 0.5472 \tTraining Prec@1 84.112 \tTraining Prec@5 99.352 \tValidation Loss 0.6050 \tValidation Prec@1 83.210 \tValidation Prec@5 99.230 \n",
      "\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "TRAINING - Epoch: [18][0/391]\tTime 1.539 (1.539)\tData 0.287 (0.287)\tLoss 0.4171 (0.4171)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "TRAINING - Epoch: [18][10/391]\tTime 0.822 (0.840)\tData 0.002 (0.028)\tLoss 0.4593 (0.5331)\tPrec@1 88.281 (84.375)\tPrec@5 99.219 (99.716)\n",
      "TRAINING - Epoch: [18][20/391]\tTime 0.762 (0.807)\tData 0.002 (0.015)\tLoss 0.5351 (0.5320)\tPrec@1 84.375 (84.301)\tPrec@5 100.000 (99.516)\n",
      "TRAINING - Epoch: [18][30/391]\tTime 0.746 (0.792)\tData 0.002 (0.011)\tLoss 0.5491 (0.5329)\tPrec@1 86.719 (84.677)\tPrec@5 100.000 (99.546)\n",
      "TRAINING - Epoch: [18][40/391]\tTime 0.761 (0.785)\tData 0.002 (0.009)\tLoss 0.5911 (0.5275)\tPrec@1 80.469 (84.870)\tPrec@5 99.219 (99.524)\n",
      "TRAINING - Epoch: [18][50/391]\tTime 0.791 (0.778)\tData 0.002 (0.007)\tLoss 0.4910 (0.5244)\tPrec@1 89.062 (85.049)\tPrec@5 99.219 (99.525)\n",
      "TRAINING - Epoch: [18][60/391]\tTime 0.755 (0.774)\tData 0.002 (0.007)\tLoss 0.5454 (0.5240)\tPrec@1 83.594 (85.105)\tPrec@5 100.000 (99.462)\n",
      "TRAINING - Epoch: [18][70/391]\tTime 0.739 (0.772)\tData 0.002 (0.006)\tLoss 0.5045 (0.5278)\tPrec@1 83.594 (84.969)\tPrec@5 99.219 (99.384)\n",
      "TRAINING - Epoch: [18][80/391]\tTime 0.753 (0.771)\tData 0.002 (0.005)\tLoss 0.4817 (0.5307)\tPrec@1 87.500 (84.886)\tPrec@5 100.000 (99.421)\n",
      "TRAINING - Epoch: [18][90/391]\tTime 0.771 (0.770)\tData 0.002 (0.005)\tLoss 0.4983 (0.5307)\tPrec@1 85.938 (84.839)\tPrec@5 100.000 (99.408)\n",
      "TRAINING - Epoch: [18][100/391]\tTime 0.802 (0.771)\tData 0.002 (0.005)\tLoss 0.4178 (0.5282)\tPrec@1 90.625 (84.924)\tPrec@5 100.000 (99.420)\n",
      "TRAINING - Epoch: [18][110/391]\tTime 0.742 (0.772)\tData 0.002 (0.004)\tLoss 0.4807 (0.5283)\tPrec@1 89.062 (84.924)\tPrec@5 99.219 (99.409)\n",
      "TRAINING - Epoch: [18][120/391]\tTime 0.780 (0.772)\tData 0.002 (0.004)\tLoss 0.4971 (0.5259)\tPrec@1 83.594 (85.001)\tPrec@5 100.000 (99.400)\n",
      "TRAINING - Epoch: [18][130/391]\tTime 0.842 (0.772)\tData 0.002 (0.004)\tLoss 0.6253 (0.5297)\tPrec@1 82.812 (84.918)\tPrec@5 98.438 (99.380)\n",
      "TRAINING - Epoch: [18][140/391]\tTime 0.754 (0.772)\tData 0.002 (0.004)\tLoss 0.3580 (0.5277)\tPrec@1 92.188 (84.968)\tPrec@5 100.000 (99.379)\n",
      "TRAINING - Epoch: [18][150/391]\tTime 0.758 (0.773)\tData 0.002 (0.004)\tLoss 0.5683 (0.5276)\tPrec@1 87.500 (84.965)\tPrec@5 99.219 (99.384)\n",
      "TRAINING - Epoch: [18][160/391]\tTime 0.773 (0.774)\tData 0.002 (0.004)\tLoss 0.4432 (0.5267)\tPrec@1 87.500 (84.967)\tPrec@5 100.000 (99.384)\n",
      "TRAINING - Epoch: [18][170/391]\tTime 0.752 (0.774)\tData 0.002 (0.004)\tLoss 0.3976 (0.5244)\tPrec@1 90.625 (84.992)\tPrec@5 100.000 (99.392)\n",
      "TRAINING - Epoch: [18][180/391]\tTime 0.767 (0.775)\tData 0.002 (0.003)\tLoss 0.6822 (0.5268)\tPrec@1 78.906 (84.979)\tPrec@5 99.219 (99.387)\n",
      "TRAINING - Epoch: [18][190/391]\tTime 0.818 (0.775)\tData 0.002 (0.003)\tLoss 0.5151 (0.5276)\tPrec@1 83.594 (84.952)\tPrec@5 99.219 (99.370)\n",
      "TRAINING - Epoch: [18][200/391]\tTime 0.757 (0.775)\tData 0.002 (0.003)\tLoss 0.5139 (0.5295)\tPrec@1 82.031 (84.861)\tPrec@5 100.000 (99.366)\n",
      "TRAINING - Epoch: [18][210/391]\tTime 0.743 (0.775)\tData 0.002 (0.003)\tLoss 0.6140 (0.5298)\tPrec@1 82.031 (84.853)\tPrec@5 98.438 (99.374)\n",
      "TRAINING - Epoch: [18][220/391]\tTime 0.800 (0.775)\tData 0.002 (0.003)\tLoss 0.6098 (0.5323)\tPrec@1 83.594 (84.810)\tPrec@5 97.656 (99.353)\n",
      "TRAINING - Epoch: [18][230/391]\tTime 0.756 (0.774)\tData 0.002 (0.003)\tLoss 0.6659 (0.5334)\tPrec@1 80.469 (84.788)\tPrec@5 99.219 (99.341)\n",
      "TRAINING - Epoch: [18][240/391]\tTime 0.750 (0.775)\tData 0.002 (0.003)\tLoss 0.4819 (0.5316)\tPrec@1 83.594 (84.819)\tPrec@5 100.000 (99.358)\n",
      "TRAINING - Epoch: [18][250/391]\tTime 0.786 (0.775)\tData 0.002 (0.003)\tLoss 0.5715 (0.5332)\tPrec@1 77.344 (84.749)\tPrec@5 99.219 (99.346)\n",
      "TRAINING - Epoch: [18][260/391]\tTime 0.752 (0.774)\tData 0.002 (0.003)\tLoss 0.5881 (0.5340)\tPrec@1 83.594 (84.707)\tPrec@5 100.000 (99.359)\n",
      "TRAINING - Epoch: [18][270/391]\tTime 0.766 (0.774)\tData 0.002 (0.003)\tLoss 0.4940 (0.5339)\tPrec@1 81.250 (84.724)\tPrec@5 100.000 (99.366)\n",
      "TRAINING - Epoch: [18][280/391]\tTime 0.769 (0.774)\tData 0.002 (0.003)\tLoss 0.6004 (0.5330)\tPrec@1 85.156 (84.764)\tPrec@5 99.219 (99.377)\n",
      "TRAINING - Epoch: [18][290/391]\tTime 0.752 (0.774)\tData 0.002 (0.003)\tLoss 0.4682 (0.5335)\tPrec@1 89.844 (84.821)\tPrec@5 100.000 (99.358)\n",
      "TRAINING - Epoch: [18][300/391]\tTime 0.743 (0.774)\tData 0.002 (0.003)\tLoss 0.5045 (0.5343)\tPrec@1 87.500 (84.827)\tPrec@5 100.000 (99.354)\n",
      "TRAINING - Epoch: [18][310/391]\tTime 0.779 (0.774)\tData 0.002 (0.003)\tLoss 0.6014 (0.5345)\tPrec@1 84.375 (84.787)\tPrec@5 100.000 (99.364)\n",
      "TRAINING - Epoch: [18][320/391]\tTime 0.786 (0.774)\tData 0.002 (0.003)\tLoss 0.3906 (0.5347)\tPrec@1 91.406 (84.760)\tPrec@5 100.000 (99.362)\n",
      "TRAINING - Epoch: [18][330/391]\tTime 0.746 (0.774)\tData 0.002 (0.003)\tLoss 0.6288 (0.5362)\tPrec@1 80.469 (84.684)\tPrec@5 100.000 (99.363)\n",
      "TRAINING - Epoch: [18][340/391]\tTime 0.779 (0.774)\tData 0.002 (0.003)\tLoss 0.5340 (0.5368)\tPrec@1 83.594 (84.664)\tPrec@5 100.000 (99.356)\n",
      "TRAINING - Epoch: [18][350/391]\tTime 0.747 (0.773)\tData 0.002 (0.003)\tLoss 0.6583 (0.5362)\tPrec@1 83.594 (84.669)\tPrec@5 98.438 (99.355)\n",
      "TRAINING - Epoch: [18][360/391]\tTime 0.769 (0.774)\tData 0.002 (0.003)\tLoss 0.5309 (0.5364)\tPrec@1 86.719 (84.684)\tPrec@5 98.438 (99.349)\n",
      "TRAINING - Epoch: [18][370/391]\tTime 0.778 (0.774)\tData 0.002 (0.003)\tLoss 0.5736 (0.5367)\tPrec@1 82.812 (84.670)\tPrec@5 100.000 (99.345)\n",
      "TRAINING - Epoch: [18][380/391]\tTime 0.789 (0.773)\tData 0.002 (0.003)\tLoss 0.5457 (0.5369)\tPrec@1 85.156 (84.666)\tPrec@5 99.219 (99.342)\n",
      "TRAINING - Epoch: [18][390/391]\tTime 0.544 (0.772)\tData 0.003 (0.003)\tLoss 0.6745 (0.5371)\tPrec@1 77.500 (84.670)\tPrec@5 98.750 (99.338)\n",
      "EVALUATING - Epoch: [18][0/79]\tTime 0.843 (0.843)\tData 0.345 (0.345)\tLoss 0.4964 (0.4964)\tPrec@1 82.812 (82.812)\tPrec@5 99.219 (99.219)\n",
      "EVALUATING - Epoch: [18][10/79]\tTime 0.324 (0.390)\tData 0.001 (0.033)\tLoss 0.5311 (0.5938)\tPrec@1 82.812 (83.310)\tPrec@5 100.000 (99.503)\n",
      "EVALUATING - Epoch: [18][20/79]\tTime 0.350 (0.363)\tData 0.002 (0.018)\tLoss 0.5278 (0.6117)\tPrec@1 83.594 (82.999)\tPrec@5 100.000 (99.330)\n",
      "EVALUATING - Epoch: [18][30/79]\tTime 0.367 (0.359)\tData 0.002 (0.013)\tLoss 0.4930 (0.6123)\tPrec@1 82.812 (82.989)\tPrec@5 100.000 (99.320)\n",
      "EVALUATING - Epoch: [18][40/79]\tTime 0.338 (0.357)\tData 0.002 (0.010)\tLoss 0.6990 (0.6080)\tPrec@1 78.125 (83.117)\tPrec@5 99.219 (99.238)\n",
      "EVALUATING - Epoch: [18][50/79]\tTime 0.355 (0.353)\tData 0.002 (0.008)\tLoss 0.7358 (0.6069)\tPrec@1 81.250 (83.364)\tPrec@5 99.219 (99.295)\n",
      "EVALUATING - Epoch: [18][60/79]\tTime 0.339 (0.351)\tData 0.002 (0.007)\tLoss 0.6472 (0.6089)\tPrec@1 83.594 (83.338)\tPrec@5 100.000 (99.334)\n",
      "EVALUATING - Epoch: [18][70/79]\tTime 0.326 (0.350)\tData 0.002 (0.007)\tLoss 0.5922 (0.6100)\tPrec@1 81.250 (83.242)\tPrec@5 100.000 (99.351)\n",
      "\n",
      " Epoch: 19\tTraining Loss 0.5371 \tTraining Prec@1 84.670 \tTraining Prec@5 99.338 \tValidation Loss 0.6093 \tValidation Prec@1 83.270 \tValidation Prec@5 99.370 \n",
      "\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "TRAINING - Epoch: [19][0/391]\tTime 1.577 (1.577)\tData 0.303 (0.303)\tLoss 0.4902 (0.4902)\tPrec@1 84.375 (84.375)\tPrec@5 100.000 (100.000)\n",
      "TRAINING - Epoch: [19][10/391]\tTime 0.748 (0.854)\tData 0.001 (0.029)\tLoss 0.4319 (0.5124)\tPrec@1 87.500 (84.872)\tPrec@5 100.000 (99.503)\n",
      "TRAINING - Epoch: [19][20/391]\tTime 0.801 (0.813)\tData 0.002 (0.016)\tLoss 0.4922 (0.5221)\tPrec@1 88.281 (85.156)\tPrec@5 100.000 (99.516)\n",
      "TRAINING - Epoch: [19][30/391]\tTime 0.768 (0.794)\tData 0.002 (0.012)\tLoss 0.4878 (0.5212)\tPrec@1 87.500 (84.803)\tPrec@5 100.000 (99.496)\n",
      "TRAINING - Epoch: [19][40/391]\tTime 0.755 (0.785)\tData 0.002 (0.009)\tLoss 0.5342 (0.5159)\tPrec@1 83.594 (84.947)\tPrec@5 99.219 (99.524)\n",
      "TRAINING - Epoch: [19][50/391]\tTime 0.775 (0.783)\tData 0.002 (0.008)\tLoss 0.5176 (0.5177)\tPrec@1 85.938 (84.988)\tPrec@5 100.000 (99.494)\n",
      "TRAINING - Epoch: [19][60/391]\tTime 0.757 (0.781)\tData 0.002 (0.007)\tLoss 0.5665 (0.5234)\tPrec@1 83.594 (84.746)\tPrec@5 99.219 (99.475)\n",
      "TRAINING - Epoch: [19][70/391]\tTime 0.792 (0.780)\tData 0.002 (0.006)\tLoss 0.5343 (0.5267)\tPrec@1 85.156 (84.628)\tPrec@5 98.438 (99.450)\n",
      "TRAINING - Epoch: [19][80/391]\tTime 0.851 (0.781)\tData 0.002 (0.006)\tLoss 0.5909 (0.5267)\tPrec@1 82.031 (84.655)\tPrec@5 98.438 (99.421)\n",
      "TRAINING - Epoch: [19][90/391]\tTime 0.762 (0.780)\tData 0.002 (0.005)\tLoss 0.4901 (0.5247)\tPrec@1 88.281 (84.796)\tPrec@5 99.219 (99.399)\n",
      "TRAINING - Epoch: [19][100/391]\tTime 0.747 (0.780)\tData 0.002 (0.005)\tLoss 0.4445 (0.5227)\tPrec@1 88.281 (84.769)\tPrec@5 100.000 (99.420)\n",
      "TRAINING - Epoch: [19][110/391]\tTime 0.822 (0.780)\tData 0.002 (0.005)\tLoss 0.5261 (0.5277)\tPrec@1 83.594 (84.671)\tPrec@5 99.219 (99.409)\n",
      "TRAINING - Epoch: [19][120/391]\tTime 0.745 (0.780)\tData 0.002 (0.004)\tLoss 0.6084 (0.5305)\tPrec@1 79.688 (84.607)\tPrec@5 97.656 (99.387)\n",
      "TRAINING - Epoch: [19][130/391]\tTime 0.737 (0.780)\tData 0.002 (0.004)\tLoss 0.5289 (0.5327)\tPrec@1 85.938 (84.560)\tPrec@5 99.219 (99.392)\n",
      "TRAINING - Epoch: [19][140/391]\tTime 0.800 (0.779)\tData 0.002 (0.004)\tLoss 0.5397 (0.5321)\tPrec@1 82.031 (84.569)\tPrec@5 99.219 (99.374)\n",
      "TRAINING - Epoch: [19][150/391]\tTime 0.760 (0.778)\tData 0.002 (0.004)\tLoss 0.6333 (0.5324)\tPrec@1 79.688 (84.572)\tPrec@5 98.438 (99.364)\n",
      "TRAINING - Epoch: [19][160/391]\tTime 0.743 (0.779)\tData 0.002 (0.004)\tLoss 0.5758 (0.5323)\tPrec@1 83.594 (84.501)\tPrec@5 98.438 (99.355)\n",
      "TRAINING - Epoch: [19][170/391]\tTime 0.787 (0.778)\tData 0.002 (0.004)\tLoss 0.4520 (0.5323)\tPrec@1 88.281 (84.466)\tPrec@5 100.000 (99.379)\n",
      "TRAINING - Epoch: [19][180/391]\tTime 0.753 (0.777)\tData 0.002 (0.004)\tLoss 0.3965 (0.5319)\tPrec@1 89.844 (84.530)\tPrec@5 100.000 (99.383)\n",
      "TRAINING - Epoch: [19][190/391]\tTime 0.752 (0.777)\tData 0.002 (0.004)\tLoss 0.5411 (0.5311)\tPrec@1 85.156 (84.571)\tPrec@5 98.438 (99.395)\n",
      "TRAINING - Epoch: [19][200/391]\tTime 0.821 (0.777)\tData 0.002 (0.003)\tLoss 0.4760 (0.5311)\tPrec@1 84.375 (84.612)\tPrec@5 100.000 (99.398)\n",
      "TRAINING - Epoch: [19][210/391]\tTime 0.776 (0.777)\tData 0.002 (0.003)\tLoss 0.4508 (0.5302)\tPrec@1 89.062 (84.649)\tPrec@5 99.219 (99.415)\n",
      "TRAINING - Epoch: [19][220/391]\tTime 0.752 (0.777)\tData 0.002 (0.003)\tLoss 0.5636 (0.5296)\tPrec@1 83.594 (84.633)\tPrec@5 100.000 (99.420)\n",
      "TRAINING - Epoch: [19][230/391]\tTime 0.824 (0.777)\tData 0.002 (0.003)\tLoss 0.4502 (0.5300)\tPrec@1 88.281 (84.642)\tPrec@5 100.000 (99.422)\n",
      "TRAINING - Epoch: [19][240/391]\tTime 0.754 (0.777)\tData 0.002 (0.003)\tLoss 0.5899 (0.5319)\tPrec@1 82.031 (84.599)\tPrec@5 99.219 (99.436)\n",
      "TRAINING - Epoch: [19][250/391]\tTime 0.757 (0.777)\tData 0.002 (0.003)\tLoss 0.5009 (0.5328)\tPrec@1 86.719 (84.584)\tPrec@5 99.219 (99.434)\n",
      "TRAINING - Epoch: [19][260/391]\tTime 0.799 (0.777)\tData 0.002 (0.003)\tLoss 0.5991 (0.5336)\tPrec@1 81.250 (84.549)\tPrec@5 100.000 (99.428)\n",
      "TRAINING - Epoch: [19][270/391]\tTime 0.805 (0.777)\tData 0.002 (0.003)\tLoss 0.6829 (0.5361)\tPrec@1 83.594 (84.482)\tPrec@5 98.438 (99.421)\n",
      "TRAINING - Epoch: [19][280/391]\tTime 0.746 (0.777)\tData 0.002 (0.003)\tLoss 0.5441 (0.5369)\tPrec@1 83.594 (84.458)\tPrec@5 97.656 (99.411)\n",
      "TRAINING - Epoch: [19][290/391]\tTime 0.777 (0.777)\tData 0.002 (0.003)\tLoss 0.5526 (0.5382)\tPrec@1 85.156 (84.434)\tPrec@5 98.438 (99.404)\n",
      "TRAINING - Epoch: [19][300/391]\tTime 0.752 (0.777)\tData 0.002 (0.003)\tLoss 0.4645 (0.5386)\tPrec@1 89.844 (84.419)\tPrec@5 99.219 (99.403)\n",
      "TRAINING - Epoch: [19][310/391]\tTime 0.756 (0.777)\tData 0.002 (0.003)\tLoss 0.4687 (0.5376)\tPrec@1 88.281 (84.430)\tPrec@5 100.000 (99.407)\n",
      "TRAINING - Epoch: [19][320/391]\tTime 0.795 (0.776)\tData 0.002 (0.003)\tLoss 0.5310 (0.5376)\tPrec@1 81.250 (84.412)\tPrec@5 100.000 (99.413)\n",
      "TRAINING - Epoch: [19][330/391]\tTime 0.761 (0.776)\tData 0.002 (0.003)\tLoss 0.3731 (0.5365)\tPrec@1 91.406 (84.453)\tPrec@5 100.000 (99.417)\n",
      "TRAINING - Epoch: [19][340/391]\tTime 0.738 (0.776)\tData 0.002 (0.003)\tLoss 0.5481 (0.5366)\tPrec@1 82.031 (84.446)\tPrec@5 100.000 (99.430)\n",
      "TRAINING - Epoch: [19][350/391]\tTime 0.839 (0.776)\tData 0.002 (0.003)\tLoss 0.4385 (0.5372)\tPrec@1 90.625 (84.435)\tPrec@5 100.000 (99.432)\n",
      "TRAINING - Epoch: [19][360/391]\tTime 0.799 (0.776)\tData 0.002 (0.003)\tLoss 0.7995 (0.5377)\tPrec@1 76.562 (84.431)\tPrec@5 99.219 (99.431)\n",
      "TRAINING - Epoch: [19][370/391]\tTime 0.749 (0.776)\tData 0.002 (0.003)\tLoss 0.4841 (0.5389)\tPrec@1 90.625 (84.381)\tPrec@5 99.219 (99.431)\n",
      "TRAINING - Epoch: [19][380/391]\tTime 0.768 (0.776)\tData 0.002 (0.003)\tLoss 0.5756 (0.5397)\tPrec@1 79.688 (84.342)\tPrec@5 100.000 (99.426)\n",
      "TRAINING - Epoch: [19][390/391]\tTime 0.482 (0.775)\tData 0.002 (0.003)\tLoss 0.3227 (0.5390)\tPrec@1 96.250 (84.368)\tPrec@5 100.000 (99.432)\n",
      "EVALUATING - Epoch: [19][0/79]\tTime 0.861 (0.861)\tData 0.288 (0.288)\tLoss 0.4858 (0.4858)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "EVALUATING - Epoch: [19][10/79]\tTime 0.359 (0.386)\tData 0.001 (0.028)\tLoss 0.5230 (0.6030)\tPrec@1 84.375 (82.955)\tPrec@5 98.438 (99.148)\n",
      "EVALUATING - Epoch: [19][20/79]\tTime 0.325 (0.361)\tData 0.002 (0.015)\tLoss 0.5403 (0.6230)\tPrec@1 82.812 (82.701)\tPrec@5 100.000 (98.958)\n",
      "EVALUATING - Epoch: [19][30/79]\tTime 0.346 (0.355)\tData 0.002 (0.011)\tLoss 0.5124 (0.6142)\tPrec@1 85.938 (82.812)\tPrec@5 100.000 (99.093)\n",
      "EVALUATING - Epoch: [19][40/79]\tTime 0.327 (0.350)\tData 0.002 (0.009)\tLoss 0.8091 (0.6178)\tPrec@1 75.000 (82.793)\tPrec@5 98.438 (99.104)\n",
      "EVALUATING - Epoch: [19][50/79]\tTime 0.325 (0.346)\tData 0.002 (0.007)\tLoss 0.6971 (0.6134)\tPrec@1 79.688 (83.042)\tPrec@5 98.438 (99.188)\n",
      "EVALUATING - Epoch: [19][60/79]\tTime 0.329 (0.345)\tData 0.002 (0.007)\tLoss 0.7072 (0.6057)\tPrec@1 78.906 (83.120)\tPrec@5 100.000 (99.283)\n",
      "EVALUATING - Epoch: [19][70/79]\tTime 0.322 (0.343)\tData 0.002 (0.006)\tLoss 0.5562 (0.6059)\tPrec@1 85.938 (83.011)\tPrec@5 100.000 (99.285)\n",
      "\n",
      " Epoch: 20\tTraining Loss 0.5390 \tTraining Prec@1 84.368 \tTraining Prec@5 99.432 \tValidation Loss 0.6030 \tValidation Prec@1 82.910 \tValidation Prec@5 99.300 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# optimizer = OptimRegime(model.parameters(), regime)\n",
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                            lr=1e-1,\n",
    "                            weight_decay=1e-4,\n",
    "                            momentum=0.9)\n",
    "logging.info('training regime: %s', regime)\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    # train for one epoch\n",
    "    train_loss, train_prec1, train_prec5 = train(\n",
    "        train_loader, model, criterion, epoch, optimizer)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    val_loss, val_prec1, val_prec5 = validate(\n",
    "        val_loader, model, criterion, epoch)\n",
    "\n",
    "    # remember best prec@1 and save checkpoint\n",
    "    is_best = val_prec1 > best_prec1\n",
    "    best_prec1 = max(val_prec1, best_prec1)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'model': \"RESNET\",\n",
    "        # 'config': args.model_config,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec1': best_prec1,\n",
    "        'regime': regime\n",
    "    }, is_best, path=save_path)\n",
    "    logging.info('\\n Epoch: {0}\\t'\n",
    "                    'Training Loss {train_loss:.4f} \\t'\n",
    "                    'Training Prec@1 {train_prec1:.3f} \\t'\n",
    "                    'Training Prec@5 {train_prec5:.3f} \\t'\n",
    "                    'Validation Loss {val_loss:.4f} \\t'\n",
    "                    'Validation Prec@1 {val_prec1:.3f} \\t'\n",
    "                    'Validation Prec@5 {val_prec5:.3f} \\n'\n",
    "                    .format(epoch + 1, train_loss=train_loss, val_loss=val_loss,\n",
    "                            train_prec1=train_prec1, val_prec1=val_prec1,\n",
    "                            train_prec5=train_prec5, val_prec5=val_prec5))\n",
    "\n",
    "    results.add(epoch=epoch + 1, train_loss=train_loss, val_loss=val_loss,\n",
    "                train_error1=100 - train_prec1, val_error1=100 - val_prec1,\n",
    "                train_error5=100 - train_prec5, val_error5=100 - val_prec5)\n",
    "    results.plot(x='epoch', y=['train_loss', 'val_loss'],\n",
    "                    legend=['training', 'validation'],\n",
    "                    title='Loss', ylabel='loss')\n",
    "    results.plot(x='epoch', y=['train_error1', 'val_error1'],\n",
    "                    legend=['training', 'validation'],\n",
    "                    title='Error@1', ylabel='error %')\n",
    "    results.plot(x='epoch', y=['train_error5', 'val_error5'],\n",
    "                    legend=['training', 'validation'],\n",
    "                    title='Error@5', ylabel='error %')\n",
    "    results.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 82.99%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test dataset\n",
    "# Define transforms for data normalization\n",
    "# import torch\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "# ])\n",
    "\n",
    "# # Load the CIFAR10 test dataset (assuming it's already downloaded)\n",
    "# testset = torchvision.datasets.CIFAR10(root='./cifar10', train=False, download=False, transform=transform)\n",
    "\n",
    "# # Create data loader\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print('Accuracy of the model on the test images: {}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "ResNet_cifar10                                --\n",
       "QConv2d: 1-1                                448\n",
       "    QuantMeasure: 2-1                      --\n",
       "RangeBN: 1-2                                32\n",
       "    QuantMeasure: 2-2                      --\n",
       "ReLU: 1-3                                   --\n",
       "Sequential: 1-4                             --\n",
       "    BasicBlock: 2-3                        --\n",
       "        QConv2d: 3-1                      2,304\n",
       "        RangeBN: 3-2                      32\n",
       "        ReLU: 3-3                         --\n",
       "        QConv2d: 3-4                      2,304\n",
       "        RangeBN: 3-5                      32\n",
       "        FloatFunctional: 3-6              --\n",
       "    BasicBlock: 2-4                        --\n",
       "        QConv2d: 3-7                      2,304\n",
       "        RangeBN: 3-8                      32\n",
       "        ReLU: 3-9                         --\n",
       "        QConv2d: 3-10                     2,304\n",
       "        RangeBN: 3-11                     32\n",
       "        FloatFunctional: 3-12             --\n",
       "Sequential: 1-5                             --\n",
       "    BasicBlock: 2-5                        --\n",
       "        QConv2d: 3-13                     4,608\n",
       "        RangeBN: 3-14                     64\n",
       "        ReLU: 3-15                        --\n",
       "        QConv2d: 3-16                     9,216\n",
       "        RangeBN: 3-17                     64\n",
       "        Sequential: 3-18                  576\n",
       "        FloatFunctional: 3-19             --\n",
       "    BasicBlock: 2-6                        --\n",
       "        QConv2d: 3-20                     9,216\n",
       "        RangeBN: 3-21                     64\n",
       "        ReLU: 3-22                        --\n",
       "        QConv2d: 3-23                     9,216\n",
       "        RangeBN: 3-24                     64\n",
       "        FloatFunctional: 3-25             --\n",
       "Sequential: 1-6                             --\n",
       "    BasicBlock: 2-7                        --\n",
       "        QConv2d: 3-26                     18,432\n",
       "        RangeBN: 3-27                     128\n",
       "        ReLU: 3-28                        --\n",
       "        QConv2d: 3-29                     36,864\n",
       "        RangeBN: 3-30                     128\n",
       "        Sequential: 3-31                  2,176\n",
       "        FloatFunctional: 3-32             --\n",
       "    BasicBlock: 2-8                        --\n",
       "        QConv2d: 3-33                     36,864\n",
       "        RangeBN: 3-34                     128\n",
       "        ReLU: 3-35                        --\n",
       "        QConv2d: 3-36                     36,864\n",
       "        RangeBN: 3-37                     128\n",
       "        FloatFunctional: 3-38             --\n",
       "AvgPool2d: 1-7                              --\n",
       "QLinear: 1-8                                650\n",
       "    QuantMeasure: 2-9                      --\n",
       "======================================================================\n",
       "Total params: 175,274\n",
       "Trainable params: 175,274\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "      QuantMeasure-1            [-1, 3, 32, 32]               0\n",
      "           QConv2d-2           [-1, 16, 32, 32]             448\n",
      "      QuantMeasure-3           [-1, 16, 32, 32]               0\n",
      "           RangeBN-4           [-1, 16, 32, 32]              32\n",
      "              ReLU-5           [-1, 16, 32, 32]               0\n",
      "      QuantMeasure-6           [-1, 16, 32, 32]               0\n",
      "           QConv2d-7           [-1, 16, 32, 32]           2,304\n",
      "      QuantMeasure-8           [-1, 16, 32, 32]               0\n",
      "           RangeBN-9           [-1, 16, 32, 32]              32\n",
      "             ReLU-10           [-1, 16, 32, 32]               0\n",
      "     QuantMeasure-11           [-1, 16, 32, 32]               0\n",
      "          QConv2d-12           [-1, 16, 32, 32]           2,304\n",
      "     QuantMeasure-13           [-1, 16, 32, 32]               0\n",
      "          RangeBN-14           [-1, 16, 32, 32]              32\n",
      "         Identity-15           [-1, 16, 32, 32]               0\n",
      "       BasicBlock-16           [-1, 16, 32, 32]               0\n",
      "     QuantMeasure-17           [-1, 16, 32, 32]               0\n",
      "          QConv2d-18           [-1, 16, 32, 32]           2,304\n",
      "     QuantMeasure-19           [-1, 16, 32, 32]               0\n",
      "          RangeBN-20           [-1, 16, 32, 32]              32\n",
      "             ReLU-21           [-1, 16, 32, 32]               0\n",
      "     QuantMeasure-22           [-1, 16, 32, 32]               0\n",
      "          QConv2d-23           [-1, 16, 32, 32]           2,304\n",
      "     QuantMeasure-24           [-1, 16, 32, 32]               0\n",
      "          RangeBN-25           [-1, 16, 32, 32]              32\n",
      "         Identity-26           [-1, 16, 32, 32]               0\n",
      "       BasicBlock-27           [-1, 16, 32, 32]               0\n",
      "     QuantMeasure-28           [-1, 16, 32, 32]               0\n",
      "          QConv2d-29           [-1, 32, 16, 16]           4,608\n",
      "     QuantMeasure-30           [-1, 32, 16, 16]               0\n",
      "          RangeBN-31           [-1, 32, 16, 16]              64\n",
      "             ReLU-32           [-1, 32, 16, 16]               0\n",
      "     QuantMeasure-33           [-1, 32, 16, 16]               0\n",
      "          QConv2d-34           [-1, 32, 16, 16]           9,216\n",
      "     QuantMeasure-35           [-1, 32, 16, 16]               0\n",
      "          RangeBN-36           [-1, 32, 16, 16]              64\n",
      "     QuantMeasure-37           [-1, 16, 32, 32]               0\n",
      "          QConv2d-38           [-1, 32, 16, 16]             512\n",
      "     QuantMeasure-39           [-1, 32, 16, 16]               0\n",
      "          RangeBN-40           [-1, 32, 16, 16]              64\n",
      "         Identity-41           [-1, 32, 16, 16]               0\n",
      "       BasicBlock-42           [-1, 32, 16, 16]               0\n",
      "     QuantMeasure-43           [-1, 32, 16, 16]               0\n",
      "          QConv2d-44           [-1, 32, 16, 16]           9,216\n",
      "     QuantMeasure-45           [-1, 32, 16, 16]               0\n",
      "          RangeBN-46           [-1, 32, 16, 16]              64\n",
      "             ReLU-47           [-1, 32, 16, 16]               0\n",
      "     QuantMeasure-48           [-1, 32, 16, 16]               0\n",
      "          QConv2d-49           [-1, 32, 16, 16]           9,216\n",
      "     QuantMeasure-50           [-1, 32, 16, 16]               0\n",
      "          RangeBN-51           [-1, 32, 16, 16]              64\n",
      "         Identity-52           [-1, 32, 16, 16]               0\n",
      "       BasicBlock-53           [-1, 32, 16, 16]               0\n",
      "     QuantMeasure-54           [-1, 32, 16, 16]               0\n",
      "          QConv2d-55             [-1, 64, 8, 8]          18,432\n",
      "     QuantMeasure-56             [-1, 64, 8, 8]               0\n",
      "          RangeBN-57             [-1, 64, 8, 8]             128\n",
      "             ReLU-58             [-1, 64, 8, 8]               0\n",
      "     QuantMeasure-59             [-1, 64, 8, 8]               0\n",
      "          QConv2d-60             [-1, 64, 8, 8]          36,864\n",
      "     QuantMeasure-61             [-1, 64, 8, 8]               0\n",
      "          RangeBN-62             [-1, 64, 8, 8]             128\n",
      "     QuantMeasure-63           [-1, 32, 16, 16]               0\n",
      "          QConv2d-64             [-1, 64, 8, 8]           2,048\n",
      "     QuantMeasure-65             [-1, 64, 8, 8]               0\n",
      "          RangeBN-66             [-1, 64, 8, 8]             128\n",
      "         Identity-67             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-68             [-1, 64, 8, 8]               0\n",
      "     QuantMeasure-69             [-1, 64, 8, 8]               0\n",
      "          QConv2d-70             [-1, 64, 8, 8]          36,864\n",
      "     QuantMeasure-71             [-1, 64, 8, 8]               0\n",
      "          RangeBN-72             [-1, 64, 8, 8]             128\n",
      "             ReLU-73             [-1, 64, 8, 8]               0\n",
      "     QuantMeasure-74             [-1, 64, 8, 8]               0\n",
      "          QConv2d-75             [-1, 64, 8, 8]          36,864\n",
      "     QuantMeasure-76             [-1, 64, 8, 8]               0\n",
      "          RangeBN-77             [-1, 64, 8, 8]             128\n",
      "         Identity-78             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-79             [-1, 64, 8, 8]               0\n",
      "        AvgPool2d-80             [-1, 64, 1, 1]               0\n",
      "     QuantMeasure-81                   [-1, 64]               0\n",
      "          QLinear-82                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 175,274\n",
      "Trainable params: 175,274\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 5.90\n",
      "Params size (MB): 0.67\n",
      "Estimated Total Size (MB): 6.58\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, 32, 32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
