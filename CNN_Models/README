
Latest py_ahir code along with generated Aa code - https://drive.google.com/drive/folders/1otOVTVLwS2-Ss3oX7Wbb820n0qCLlUd0

py_ahir_convolution.ipynb - python code for implementation of AI ML Accelerator
(colab link - https://colab.research.google.com/drive/1P47d1V_sEBoMZYmkSyUxW3K5jeOcJc2p?usp=sharing)

/resnet/models/ 
    - qresnet.py - Code for ResNet 18 architecture
                    [Reference: https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py]
    - quantize.py - Code for implementing forward and backward pass of 
                    Quantized Convolution Neural Network and also 
                    quantizing input data of convolution and weights
                    to 8 bits

/resnet/utils/ - folder contains helper code for finding accuracy and logging 

/resnet/preprocess.py - code for preprocessing the input data

/resnet/resnet_8bit.ipynb - Jupyter Notebook containing code for training and testing ResNet-18 architecture
                    with quantized 8-bit input and weights

/resnet/resnet_original.ipynb - Actual ResNet-18 architecture using float data type 

/lenet/lenet_float.ipynb - LeNet-5 architecture implementation using float datatype

/lenet/lenet_posit.ipynb - LeNet-5 architecture with float arithmetic. The input of each
                            convolution layer is first converted to 8-bit posit
                            and then converted back to 32 bit float. This is to simulate
                            the storage of 32 bit output of convolution to 8 bit posit
                            in the accelerator.

References
    - https://github.com/soon-yau/QNN/tree/master - Tutorial for understanding Quantized Neural Network

TODO

                                        Dataset     Training Accuracy     Testing Accuracy

ResNet-18 with 8-bit quantization       CIFAR-10          88 %                  82%

ResNet-18 with float32 data type        CIFAR-10          92 %                  86%

ResNet-18 with float32 data type        MNIST-Digit       99.5 %                99%

ResNet-18 with 8 bit posit conversion   MNIST-Digit       99 %                  94%
(32 bit float operation)

ResNet-18 with 8bit posit conversion    MNIST-Digit       98%                   93%
(16 bit float operation)

LeNet-5 with 32 bit float               MNIST-Digit       96.48%                96.42%

LeNet-5 with 8 bit posit conversion     MNIST-Digit       88%                   89%
(32 bit float operation)

LeNet-5 with 16 bit float               MNIST-Digit       96.65%                96.37%

LeNet-5 with 8 bit posit conversion     MNIST-Digit       88.7%                 89.37%
(16 bit float operation)


Observations
    - In LeNet-5, conversion from full precision to half precision operations did not change
        accuracy much
