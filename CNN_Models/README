
/resnet/models/ 
    - qresnet.py - Code for ResNet 18 architecture
                    [Reference: https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py]
    - quantize.py - Code for implementing forward and backward pass of 
                    Quantized Convolution Neural Network and also 
                    quantizing input data of convolution and weights
                    to 8 bits

/resnet/utils/ - folder contains helper code for finding accuracy and logging 

/resnet/preprocess.py - code for preprocessing the input data

/resnet/resnet_8bit.ipynb - Jupyter Notebook containing code for training and testing ResNet-18 architecture
                    with quantized 8-bit input and weights

/resnet/resnet_original.ipynb - Actual ResNet-18 architecture using float data type 

/lenet/lenet_float.ipynb - LeNet-5 architecture implementation using float datatype

/lenet/lenet_posit.ipynb - LeNet-5 architecture with float arithmetic. The input of each
                            convolution layer is first converted to 8-bit posit
                            and then converted back to 32 bit float. This is to simulate
                            the storage of 32 bit output of convolution to 8 bit posit
                            in the accelerator.

References
    - https://github.com/soon-yau/QNN/tree/master - Tutorial for understanding Quantized Neural Network

TODO

                                        Dataset     Training Accuracy     Testing Accuracy

ResNet-18 with 8-bit quantization       CIFAR-10          88 %                  82%

ResNet-18 with float data type          CIFAR-10          92 %                  86%

Old LeNet-5 with 8 bit posit(9 hours)   MNIST             94.59%                94.30%  

New LeNet-5 with 8 bit posit(2 hours)   MNIST             86.63%                86.52%

LeNet-5 with 32 bit float(10 min)       MNIST             96.48%                96.42%

Observations
    - Training of each iteration reduced from 2.5 hours to 10 min
    - Number of iterations increased from 3 to 12
    - Previously the time consuming functions were posit conversion(1.5 sec per input) function and 
        back propogation(2 sec per input)
    - In new architecture posit conversion takes 0.2 sec per input and backpropogation 
        takes 0.5 ms per input